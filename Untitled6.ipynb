{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SURIe1UDpLCu"
      },
      "outputs": [],
      "source": [
        "#!pip install openai langchain langchain_community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uG7Azba0ppsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "\n",
        "model = OpenAI(model_name=\"gpt-35-turbo\", temperature=0.5)\n",
        "\n",
        "# Define a prompt template for extracting key terms\n",
        "prompt_template = PromptTemplate(input_variables=[\"text\"], template=\"\"\"\n",
        "Extract the key terms from the following text:\n",
        "{text}\n",
        "\"\"\")\n",
        "\n",
        "# Create a chain for extracting terms\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)\n",
        "\n",
        "def extract_key_terms(text):\n",
        "    result = chain.run({\"text\": text})\n",
        "    return result.strip().split(\", \")\n",
        "\n",
        "\n",
        "text = \" something ......\"\n",
        "\n",
        "# Extract key terms\n",
        "key_terms = extract_key_terms(text)\n",
        "print(key_terms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "c1k1giNHpll5",
        "outputId": "9c83e3bb-faa4-4286-825f-4cd5d52a8345"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-40f56bc016f8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-35-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define a prompt template for extracting key terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6GIPm3mqs91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4rcL6TvDqtA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Replace with your Azure OpenAI endpoint and API key\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"https://<your-resource-name>.openai.azure.com/\"\n",
        "openai.api_version = \"2023-05-15\"\n",
        "openai.api_key = \"<your-api-key>\"\n",
        "\n",
        "# Initialize the model\n",
        "model = AzureOpenAI(\n",
        "    openai_api_base=openai.api_base,\n",
        "    openai_api_key=openai.api_key,\n",
        "    model_name=\"gpt-35-turbo\",\n",
        "    openai_api_version=openai.api_version,\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "# Define a prompt template for extracting key terms\n",
        "prompt_template = PromptTemplate(input_variables=[\"text\"], template=\"\"\"\n",
        "Extract the key terms from the following text:\n",
        "{text}\n",
        "\"\"\")\n",
        "\n",
        "# Create a chain for extracting terms\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)\n",
        "\n",
        "def extract_key_terms(text):\n",
        "    result = chain.run({\"text\": text})\n",
        "    return result.strip().split(\", \")  # Assuming the model returns a comma-separated list of key terms\n",
        "\n",
        "# Example text\n",
        "text = \"Azure OpenAI Service provides access to OpenAI's powerful language models, including GPT-3.5-turbo.\"\n",
        "\n",
        "# Extract key terms\n",
        "key_terms = extract_key_terms(text)\n",
        "print(key_terms)\n"
      ],
      "metadata": {
        "id": "2TBZ8lg8qtbA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ke1qR_oCr14-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZM-a39JRr172"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai\n",
        "\n",
        "# Set up OpenAI API key\n",
        "openai.api_key = \"apikey\"\n",
        "# Define input text\n",
        "text =  \"something\"\n",
        "\n",
        "\n",
        "\n",
        "# Pass preprocessed text through OpenAI GPT for contextual analysis\n",
        "contextual_embeddings = openai.Completion.create(engine= \"text-davinci-002\", prompt=text, max_tokens=50)\n",
        "\n",
        "# Combine RAKE scores with contextual embeddings to rank keywords\n",
        "# Select top-ranked keywords as final extracted keywords\n",
        "final_keywords = [keyword for keyword  in contextual_embeddings.choices[0].text]\n",
        "\n",
        "# Print final extracted keywords\n",
        "print(\"Extracted Keywords:\")\n",
        "for keyword in final_keywords:\n",
        "    print(keyword)"
      ],
      "metadata": {
        "id": "9G5Hq9lRr1_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}