{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9a21b58bb204beba0ebe0333abba1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff1dbaf9c15542e4a909f7ebecf25e51",
              "IPY_MODEL_d01f0f56165944d9b175e8cc76dae4b1",
              "IPY_MODEL_c2a1bb648bb84105b0bff9123b8d0655"
            ],
            "layout": "IPY_MODEL_49cc3e3ec5464e67834028dfc83be975"
          }
        },
        "ff1dbaf9c15542e4a909f7ebecf25e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df8daafbce3453d8bd6cd65f2214912",
            "placeholder": "​",
            "style": "IPY_MODEL_1d1d1bb0e00b408893bc424430fa4ddd",
            "value": "Fetching 5 files: 100%"
          }
        },
        "d01f0f56165944d9b175e8cc76dae4b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34046841112e4a70871f9b2bbaf3a92c",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbe6af47694049debf494950affb916c",
            "value": 5
          }
        },
        "c2a1bb648bb84105b0bff9123b8d0655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ec5195371e4dae903a31dc1f3de1cb",
            "placeholder": "​",
            "style": "IPY_MODEL_b383286bbcba408eba64b4eae0e2d7a9",
            "value": " 5/5 [00:00&lt;00:00, 163.63it/s]"
          }
        },
        "49cc3e3ec5464e67834028dfc83be975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df8daafbce3453d8bd6cd65f2214912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1d1bb0e00b408893bc424430fa4ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34046841112e4a70871f9b2bbaf3a92c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe6af47694049debf494950affb916c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94ec5195371e4dae903a31dc1f3de1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b383286bbcba408eba64b4eae0e2d7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "326558eb36914c74bd9c0e974df138ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b681dfdb4e7541288a71d2e7b3112532",
              "IPY_MODEL_31700e8382b64faebe89fcaf1a08dacc",
              "IPY_MODEL_dd5541393d7e4215bd06c0bb1acf9d41"
            ],
            "layout": "IPY_MODEL_e2fe31a9b8d147439c8ecd93ae6a6eac"
          }
        },
        "b681dfdb4e7541288a71d2e7b3112532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20cf66b2dcae4b939d61716e9111fb3c",
            "placeholder": "​",
            "style": "IPY_MODEL_97cdfee763254f2a9bb4db6887016e6f",
            "value": "Fetching 5 files: 100%"
          }
        },
        "31700e8382b64faebe89fcaf1a08dacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ee682784be42f58f36363a555e9f0f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ef36c3cb2744678fc365165041e057",
            "value": 5
          }
        },
        "dd5541393d7e4215bd06c0bb1acf9d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded0b637c7174eb68bd6e79e0fb3c1b8",
            "placeholder": "​",
            "style": "IPY_MODEL_81951d2647544d4fa23d376b884888e5",
            "value": " 5/5 [00:00&lt;00:00, 221.16it/s]"
          }
        },
        "e2fe31a9b8d147439c8ecd93ae6a6eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cf66b2dcae4b939d61716e9111fb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cdfee763254f2a9bb4db6887016e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36ee682784be42f58f36363a555e9f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ef36c3cb2744678fc365165041e057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ded0b637c7174eb68bd6e79e0fb3c1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81951d2647544d4fa23d376b884888e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28da3384a4664f4cbb7c55c9d865e089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5935e35a35de4b8ebb65e8a1cbd8742d",
              "IPY_MODEL_62936512ce2640d881b6d6290dba0ae6",
              "IPY_MODEL_67b4220a7f59493db5063bdc4b7b325d"
            ],
            "layout": "IPY_MODEL_d235a3b36df74dd2b99013c8f4e78460"
          }
        },
        "5935e35a35de4b8ebb65e8a1cbd8742d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4463e4bf3bb84f78bc2d609849a8e5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_be4a715205434d60a43ee35f704f4a85",
            "value": "Fetching 5 files: 100%"
          }
        },
        "62936512ce2640d881b6d6290dba0ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e616b82a8e394088b84f91b609eeea2b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a69b359d1ac4ab884482580ab146e5f",
            "value": 5
          }
        },
        "67b4220a7f59493db5063bdc4b7b325d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cee1a435638f4123b53c5455340ba6bb",
            "placeholder": "​",
            "style": "IPY_MODEL_3fd2cb5066f94f6faedc2abb1200e37a",
            "value": " 5/5 [00:00&lt;00:00, 137.53it/s]"
          }
        },
        "d235a3b36df74dd2b99013c8f4e78460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4463e4bf3bb84f78bc2d609849a8e5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4a715205434d60a43ee35f704f4a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e616b82a8e394088b84f91b609eeea2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a69b359d1ac4ab884482580ab146e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cee1a435638f4123b53c5455340ba6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd2cb5066f94f6faedc2abb1200e37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-pibYO7zN6v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas chromadb langchain-groq fastembed pypdf openai konlpy sentence-transformers"
      ],
      "metadata": {
        "id": "dOi-pEn8zR_p"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install python-docx\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVvUjj233XZW",
        "outputId": "6db8355a-700c-4d14-f8f1-a744bcacba0f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqg_jYL47VyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, SpacyTextSplitter, NLTKTextSplitter, split_text_on_tokens, SentenceTransformersTokenTextSplitter, LatexTextSplitter\n",
        "from langchain.text_splitter import PythonCodeTextSplitter, KonlpyTextSplitter, ElementType\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import torch\n",
        "import re\n",
        "from docx import Document as worddoc\n",
        "\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents.base import Document"
      ],
      "metadata": {
        "id": "wM-2284fzyP_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TextChunker:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_text_from_pdf(self, path = \"/\"):\n",
        "        text = \"\"\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents = loader.load()\n",
        "        for doc in documents:\n",
        "            text = text + \"\\n\" + str(doc.page_content)\n",
        "        return text,documents\n",
        "\n",
        "    def char_count_chunking_with_overlap(self, text, chunk_size=200, chunk_overlap=50, splitter_type = \"CharacterTextSplitter\"):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        # Create documents using the text splitter\n",
        "        docs =[]\n",
        "        chunks = []\n",
        "        if isinstance(text, str):\n",
        "          docs = text_splitter.create_documents([text])\n",
        "          chunks = [doc.page_content for doc in docs]\n",
        "        elif isinstance(text, list):\n",
        "          try:\n",
        "            docs = text_splitter.create_documents(text)\n",
        "            chunks = [doc.page_content for doc in docs]\n",
        "          except:\n",
        "            chunks = text_splitter.split_documents(text)\n",
        "        return chunks\n",
        "\n",
        "    #Nonoverlap\n",
        "    def char_count_chunking_with_nonoveralp(self, text, chunk_size=200, splitter_type = \"CharacterTextSplitter\"):\n",
        "       # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter clas\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
        "        # Create documents using the text splitter\n",
        "        docs =[]\n",
        "        chunks = []\n",
        "        if isinstance(text, str):\n",
        "          docs = text_splitter.create_documents([text])\n",
        "          chunks = [doc.page_content for doc in docs]\n",
        "        elif isinstance(text, list):\n",
        "          try:\n",
        "            docs = text_splitter.create_documents(text)\n",
        "            chunks = [doc.page_content for doc in docs]\n",
        "          except:\n",
        "            chunks = text_splitter.split_documents(text)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    #chunking\n",
        "    def char_count_chunking_with_custom_delimiter(self, text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",splitter_type = \"CharacterTextSplitter\"):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"NLTKTextSplitter\":\n",
        "           text_splitter = NLTKTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SentenceTransformersTokenTextSplitter\":\n",
        "           text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"LatexTextSplitter\":\n",
        "           text_splitter = LatexTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"PythonCodeTextSplitter\":\n",
        "           text_splitter = PythonCodeTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"KonlpyTextSplitter\":\n",
        "           text_splitter = KonlpyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        # Create documents using the text splitter\n",
        "        docs =[]\n",
        "        chunks = []\n",
        "        if isinstance(text, str):\n",
        "          docs = text_splitter.create_documents([text])\n",
        "          chunks = [doc.page_content for doc in docs]\n",
        "        else:\n",
        "          try:\n",
        "            chunks = text_splitter.split_documents(text)\n",
        "          except:\n",
        "            docs = text_splitter.create_documents(text)\n",
        "            chunks = [doc.page_content for doc in docs]\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    #Semantic chunking\n",
        "    def semantic_section_chunking(self, documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\"):\n",
        "        embed_model = FastEmbedEmbeddings(model_name = text_embedding_model_name)\n",
        "        semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=breakpoint_threshold_type)\n",
        "        try:\n",
        "          semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
        "        except:\n",
        "          semantic_chunks = semantic_chunker.create_documents([d for d in markdown_splits])\n",
        "        return semantic_chunks\n",
        "\n",
        "    #Markdown HeaderTextSplitter\n",
        "    def markdown_header_textsplitter(self, text, headers_to_split_on, strip_headers):\n",
        "        from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "        markdown_document = text\n",
        "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on,  strip_headers=strip_headers)\n",
        "        md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "        return md_header_splits\n",
        "\n",
        "    #Markdown TextSplitter\n",
        "    def markdown_textsplitter(self, text):\n",
        "        from langchain.text_splitter import MarkdownTextSplitter\n",
        "        splitter = MarkdownTextSplitter()\n",
        "        split_text = splitter.split_text(text)\n",
        "        return split_text\n",
        "\n",
        "    #ouputs to Doc\n",
        "    #from docx import Document\n",
        "    #import re\n",
        "    def save_as_word_doc(self, split_text, file_path ,heading):\n",
        "        doc = worddoc()\n",
        "        doc.add_heading(heading, level=1)\n",
        "        for i, text_chunk in enumerate(split_text):\n",
        "            text_chunk= re.sub(r'[\\x00-\\x1F]+', '', str(text_chunk))\n",
        "            doc.add_heading(f'Chunk {i + 1}', level=1)\n",
        "            doc.add_paragraph(text_chunk)\n",
        "        # Save the document\n",
        "        doc.save(file_path)\n",
        "        print(\"Output saved as doc successfully\")\n",
        "\n",
        "\n",
        "\n",
        "#Usage\n",
        "pdf_path =r\"/content/drive/MyDrive/chunking/ds (1).pdf\"\n",
        "\n",
        "text_chunker = TextChunker()\n",
        "text, documents  = text_chunker.extract_text_from_pdf(path = pdf_path )"
      ],
      "metadata": {
        "id": "xNCgyIqwzrvA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NE86Jq5Ft0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownTextSplitter\n",
        "\n",
        "split_text = text_chunker.markdown_textsplitter(text)\n",
        "file_name = \"MarkdownTextSplitter_chunks.docx\"\n",
        "file_path = \"/content/drive/MyDrive/chunking/\"\n",
        "heading = \"MarkdownTextSplitter_chunks\"\n",
        "\n",
        "text_chunker.save_as_word_doc(split_text, file_path + file_name ,heading)\n",
        "# for i, text_chunk in enumerate(split_text):\n",
        "#     print(\"########################################################################\")\n",
        "#     print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v3rqsHdhsmwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9222bfb-fb68-463f-9df9-d5479bfa8376"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as doc successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMzU0gw4Dgo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fziuDnyr6GSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SentenceTransformersTokenTextSplitter chunking\n",
        "split_text = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"SentenceTransformersTokenTextSplitter\")\n",
        "\n",
        "file_name = \"SentenceTransformersTokenTextSplitter.docx\"\n",
        "file_path = \"/content/drive/MyDrive/chunking/\"\n",
        "heading = \"SentenceTransformersTokenTextSplitter\"\n",
        "\n",
        "text_chunker.save_as_word_doc(split_text, file_path + file_name ,heading)\n",
        "# for i, text_chunk in enumerate(split_text):\n",
        "#     print(\"########################################################################\")\n",
        "#     print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ1O9Kgp6GVE",
        "outputId": "a4ff8bce-af81-434b-8f46-c574c77273fb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as doc successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6dlGPz95uDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownTextSplitter with SentenceTransformersTokenTextSplitter chunking\n",
        "\n",
        "file_name = \"MarkdownTextSplitter_with_SentenceTransformersTokenTextSplitter_chunking.docx\"\n",
        "file_path = \"/content/drive/MyDrive/chunking/\"\n",
        "heading = \"MarkdownTextSplitter with SentenceTransformersTokenTextSplitter chunking\"\n",
        "\n",
        "markdown_split_text = text_chunker.markdown_textsplitter(text)\n",
        "text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=30)\n",
        "\n",
        "SentenceTransformers_split_text = text_chunker.char_count_chunking_with_custom_delimiter(markdown_split_text, chunk_size=200, chunk_overlap=50, splitter_type = \"SentenceTransformersTokenTextSplitter\")\n",
        "\n",
        "text_chunker.save_as_word_doc(SentenceTransformers_split_text, file_path + file_name ,heading)\n",
        "\n",
        "# for i, text_chunk in enumerate(SentenceTransformers_split_text):\n",
        "#     print(\"########################################################################\")\n",
        "#     print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")"
      ],
      "metadata": {
        "id": "5LKfDcVr5uF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bbc812-540d-475c-f6fa-c49b2c110073"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as doc successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7p7IPmNGF_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownTextSplitter with CharacterTextSplitter chunking\n",
        "\n",
        "file_name = \"MarkdownTextSplitter_with_CharacterTextSplitter_chunking.docx\"\n",
        "file_path = \"/content/drive/MyDrive/chunking/\"\n",
        "heading = \"MarkdownTextSplitter with CharacterTextSplitter chunking\"\n",
        "\n",
        "markdown_split_text = text_chunker.markdown_textsplitter(text)\n",
        "\n",
        "split_text = text_chunker.char_count_chunking_with_overlap( markdown_split_text, chunk_size=200, chunk_overlap=50, splitter_type = \"CharacterTextSplitter\")\n",
        "\n",
        "\n",
        "text_chunker.save_as_word_doc(split_text, file_path + file_name ,heading)\n",
        "\n",
        "# for i, text_chunk in enumerate(SentenceTransformers_split_text):\n",
        "#     print(\"########################################################################\")\n",
        "#     print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNn4lDLtH4vm",
        "outputId": "4f5c4d7c-0da5-4e66-af98-a76be0b88926"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as doc successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9empKJTm2CoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownTextSplitter with Semantic chunking\n",
        "\n",
        "file_name = \"MarkdownTextSplitter_with_Semantic_chunking.docx\"\n",
        "file_path = \"/content/drive/MyDrive/chunking/\"\n",
        "heading = \"MarkdownTextSplitter with Semantic chunking\"\n",
        "\n",
        "markdown_splits = text_chunker.markdown_textsplitter(text)\n",
        "\n",
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( markdown_splits , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "text_chunker.save_as_word_doc(semantic_chunks, file_path + file_name ,heading)\n",
        "\n",
        "# for i, text_chunk in enumerate(semantic_chunks ):\n",
        "#     print(\"########################################################################\")\n",
        "#     print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "326558eb36914c74bd9c0e974df138ed",
            "b681dfdb4e7541288a71d2e7b3112532",
            "31700e8382b64faebe89fcaf1a08dacc",
            "dd5541393d7e4215bd06c0bb1acf9d41",
            "e2fe31a9b8d147439c8ecd93ae6a6eac",
            "20cf66b2dcae4b939d61716e9111fb3c",
            "97cdfee763254f2a9bb4db6887016e6f",
            "36ee682784be42f58f36363a555e9f0f",
            "f3ef36c3cb2744678fc365165041e057",
            "ded0b637c7174eb68bd6e79e0fb3c1b8",
            "81951d2647544d4fa23d376b884888e5"
          ]
        },
        "id": "Hr1b8RetLWOC",
        "outputId": "b8e42d1c-93c9-4c29-a4de-399ab6534c81"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "326558eb36914c74bd9c0e974df138ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as doc successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ns2xDgG9TaU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hrWztMzLWt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownHeaderTextSplitter with CharacterTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "            (\"#\", \"Header 1\"),\n",
        "            (\"##\", \"Header 2\"),\n",
        "            (\"###\", \"Header 3\"),\n",
        "            (\"####\", \"Header 4\"),\n",
        "        ]\n",
        "strip_headers = False\n",
        "\n",
        "md_header_splits = text_chunker.markdown_header_textsplitter(text, headers_to_split_on, strip_headers)\n",
        "\n",
        "\n",
        "md_header_splits\n",
        "\n",
        "#You can use any other chuknikng startegies after this  MarkdownHeaderTextSplitter\n",
        "chunks = text_chunker.char_count_chunking_with_nonoveralp(md_header_splits , chunk_size=200, splitter_type = \"CharacterTextSplitter\")\n",
        "\n",
        "\n",
        "print(\"chunking _with_ _with_custom_delimiter with MarkdownHeaderTextSplitter:\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHDGqkWIz0NT",
        "outputId": "b30b0ecb-13f9-4899-c243-324f6848a0b1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunking _with_ _with_custom_delimiter with MarkdownHeaderTextSplitter:\n",
            "Chunk 1: page_content='Mathematical Foundations of Data Sciences\\nGabriel Peyr´ e\\nCNRS & DMA\\n´Ecole Normale Sup´ erieure\\ngabriel.peyre@ens.fr\\nhttps://mathematical-tours.github.io\\nwww.numerical-tours.com\\nAugust 14, 2019\\n2\\nChapter 1\\nOptimal Transport\\n1.1 Radon Measures\\nMeasures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\\nbelongs to the probability simplex\\nΣndef.={\\na∈Rn\\n+;n∑\\ni=1ai= 1}\\n.\\nA discrete measure with weights aand locations x1,...,xn∈X reads\\nα=n∑\\ni=1aiδxi (1.1)\\nwhereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\\nx. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\\nmeasure if each of the “weights” described in vector ais positive itself.\\nRemark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\\n“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\\nto the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\\nequipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\\nit against continuous functions, denoted f∈C(X).\\nIntegration of f∈C(X) against a discrete measure αcomputes a sum\\n∫\\nXf(x)dα(x) =n∑\\ni=1aif(xi).\\nMore general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\\ndα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\\ndx, which means that\\n∀h∈C(Rd),∫\\nRdh(x)dα(x) =∫\\nRdh(x)ρα(x)dx.\\nAn arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\\nthe fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\\nXf(x)dα(x)∈R.\\nIfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\\nMeasure as thus in some sense “less regular” than functions, but more regular than distributions (which are\\ndual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\\nset of all positive measures on X. The set of probability measures is denoted M1\\n+(X), which means that\\nanyα∈M1\\n+(X) is positive, and that α(X) =∫\\nXdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\\nclasses of measures, beyond histograms, considered in this work.\\n3\\nDiscreted= 1 Discrete d= 2 Density d= 1 Density d= 2\\nFigure 1.1: Schematic display of discrete distributions α=∑n\\ni=1aiδxi(red corresponds to empirical uniform\\ndistribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\\n1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\\nand in 2-D using point clouds (radius equal to ai).\\nOperators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\\nT♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\\npositions of all the points in the support of the measure\\nT♯αdef.=∑\\niaiδT(xi).\\nFor more general measures, for instance for those with a density, the notion of push-forward plays a funda-\\nmental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\\nDeﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\\nα∈M (X)reads\\n∀h∈C(Y),∫\\nYh(y)dβ(y) =∫\\nXh(T(x))dα(x). (1.2)\\nEquivalently, for any measurable set B⊂Y, one has\\nβ(B) =α({x∈X;T(x)∈B}). (1.3)\\nNote thatT♯preserves positivity and total mass, so that if α∈M1\\n+(X)thenT♯α∈M1\\n+(Y).\\nIntuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\\nmeasurable space to another. The more general extension T♯can now “move” an entire probability measure\\nonXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\\na measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\\nnew measure onY) writtenT♯α. Note that such a push-forward T♯:M1\\n+(X)→M1\\n+(Y) is a linear operator\\nbetween measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\\nRemark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\\nwith densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\\ndensities linearly as a change of variables in the integration formula, indeed\\nρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\\nwhereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\\nofT). This implies, denoting y=T(x)\\n|det(T′(x))|=ρα(x)\\nρβ(y).\\n4\\n=PixiT↵T]↵def.=PiT(xi)\\nTT]gdef.=gTgPush-forward of measures Pull-back of functions\\nFigure 1.2: Comparison of push-forward T♯and pull-back T♯.\\nRemark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\\nthe pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\\nmap deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\\nothers, in the sense that\\n∀(α,g)∈M (X)×C(Y),∫\\nYgd(T♯α) =∫\\nX(T♯g)dα.\\nIt is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\\nthe presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\\nregistration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\\nbetween these push-forward and pull-back operators.\\nRemark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\\nbutions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\\n(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\\n+(X) such\\nthatP(X∈A) =α(A) =∫\\nAdα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\\nanother push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\\nvariableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\\nyfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\\nConvergence of random variable. Convergence of random variable (in probability, almost sure, in law),\\nconvergence of measures (strong, weak).\\n1.2 Monge Problem\\nGiven a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\\nbijectionσin the set Perm( n) of permutations of nelements solving\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i). (1.5)\\nOne could naively evaluate the cost function above using all permutations in the set Perm( n). However,\\nthat set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\\n10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\\nalgorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\\n5\\nx1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\\neither matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\\nthe blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\\ndisk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\\n4⩽i⩽7 we haveT(xi) =y1.\\nRemark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\\nSuppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\\ncorners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\\nonly two assignments exist, and they share the same cost.\\nFor discrete measures\\nα=n∑\\ni=1aiδxiandβ=m∑\\nj=1bjδyj (1.6)\\nthe Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\\npush the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\\nmust verify that\\n∀j∈JmK,bj=∑\\ni:T(xi)=yjai (1.7)\\nwhich we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\\nparameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\\nmin\\nT{∑\\nic(xi,T(xi)) ;T♯α=β}\\n. (1.8)\\nSuch a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\\nindicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\\n∑\\ni∈σ−1(j)ai=bj.\\nIn the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\\nconstraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\\noptimal matching problem (1.5) where the cost matrix is\\nCi,jdef.=c(xi,yj).\\nWhenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\\nto another. This happens when their weight vectors are not compatible, which is always the case when the\\ntarget measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\\nan (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\\n6\\nMonge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\\n(X,Y) as ﬁnding a map T:X→Y that minimizes\\nmin\\nT{∫\\nXc(x,T(x))dα(x) ;T♯α=β}\\n(1.9)\\nThe constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\\noperator (1.2).\\n1.3 Kantorovitch Problem\\nThe assignment problem has several limitations in practical settings, also encountered when using the\\nMonge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\\nbe used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\\nuniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\\nalso be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\\n(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\\nset for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\\nconstraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\\nKantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\\nture of transportation, namely the fact that a source point xican only be assigned to another, or transported\\nto one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\\ndispatched across several locations. Kantorovich moves away from the idea that mass transportation should\\nbe “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\\ncommonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\\nusing, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\\n+, where Pi,jdescribes the\\namount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\\nof discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\\nU(a,b)def.={\\nP∈Rn×m\\n+ ;P1m=aand PT1n=b}\\n, (1.10)\\nwhere we used the following matrix-vector notation\\nP1m=\\n∑\\njPi,j  \\ni∈Rnand PT1n=(∑\\niPi,j)\\nj∈Rm.\\nThe set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\\npolytope (the convex hull of a ﬁnite set of matrices).\\nAdditionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\\nasymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\\nU(a,b) if and only if PTis inU(b,a).\\nKantorovich’s optimal transport problem now reads\\nLC(a,b)def.= min\\nP∈U(a,b)⟨C,P⟩def.=∑\\ni,jCi,jPi,j. (1.11)\\nThis is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\\nnot necessarily unique.\\n7\\n↵\\n↵Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\\nindicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\\ncorresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\\nRight: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\\nassociate two arbitrary discrete measures.\\nPermutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\\ning permutation matrix,\\n∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\\n0 otherwise.(1.12)\\nOne can check that in that case\\n⟨C,Pσ⟩=1\\nnn∑\\ni=1Ci,σi,\\nwhich shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\\ncouplings Pare restricted to be exactly permutation matrices:\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i)= min\\nσ∈Perm(n)⟨C,Pσ⟩.\\nNext, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\\npolytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\\n1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\\nmin\\nσ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\\nThe following proposition shows that these problems result in fact in the same optimum, namely that\\none can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\\nmeasures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\\nproblems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\\ncase.\\nProposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\\nsolution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\\nPerm(n)for Problem (1.5) .\\nProof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\\npermutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\\nminimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\\npolyhedron.\\n8\\n⇡↵↵\\n⇡↵↵\\n⇡↵↵\\nDiscrete Semi-discrete Continuous\\nFigure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\\nscenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\\n⇡↵\\n⇡↵\\nFigure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\\ncoupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\\ncouplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\\ndisplay with a black disk at position ( i,j) with radius proportional to Ti,j.\\nKantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\\narbitrary measures by considering couplings π∈M1\\n+(X×Y ) which are joint distributions over the product\\nspace. The discrete case is a special situation where one imposes this product measure to be of the form\\nπ=∑\\ni,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\\nmarginal constraint on joint probability distributions\\nU(α,β)def.={\\nπ∈M1\\n+(X×Y ) ;PX♯π=αandPY♯π=β}\\n. (1.13)\\nHerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\\nFigure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\\nmeasures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\\nα(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\\nThe Kantorovich problem (1.11) is then generalized as\\nLc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y). (1.14)\\nThis is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\\nand continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\\ninvolving discrete and continuous marginals.\\nOn compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\\nweak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\\n9\\n↵↵⇡\\n↵↵⇡\\n↵↵⇡\\n↵↵⇡Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\\nabove (arrows) and couplings below. Inspired by [ ?].\\nis weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\\nto impose moment condition on αandβ.\\nWasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\\nand probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\\nunderstood as a canonical way to lift a ground distance between points to a distance between histogram or\\nmeasures.\\nWe ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\\nis ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\\nto compare. The following proposition states that OT provides a meaningful distance between histograms\\nsupported on these bins.\\nProposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\\ni,j)i,j∈Rn×nwhere D∈Rn×n\\n+\\nis a distance on JnK,i.e.\\n1.D∈Rn×n\\n+ is symmetric;\\n2.Di,j= 0if and only if i=j;\\n3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\\nThen\\nWp(a,b)def.= LDp(a,b)1/p(1.15)\\n(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\\nWp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\\n∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\\nProof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\\nWp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\\nelements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\\na non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\\nTo prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\\ngluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\\nthe explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\\nsolutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\\nand set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\\nSdef.=Pdiag(1/¯b)Q∈Rn×n\\n+.\\n10\\nWe remark that S∈U(a,c) because\\nS1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\\nwhere we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\\nbecause necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\\nThe triangle inequality follows from\\nWp(a,c) =(\\nmin\\nP∈U(a,c)⟨P,Dp⟩)1/p\\n⩽⟨S,Dp⟩1/p\\n=\\n∑\\nikDp\\nik∑\\njPijQjk\\n¯bj\\n1/p\\n⩽\\n∑\\nijk(Dij+Djk)pPijQjk\\n¯bj\\n1/p\\n⩽\\n∑\\nijkDp\\nijPijQjk\\n¯bj\\n1/p\\n+\\n∑\\nijkDp\\njkPijQjk\\n¯bj\\n1/p\\n=\\n∑\\nijDp\\nijPij∑\\nkQjk\\n¯bj\\n1/p\\n+\\n∑\\njkDp\\njkQjk∑\\niPij\\n¯bj\\n1/p\\n=\\n∑\\nijDp\\nijPij\\n1/p\\n+\\n∑\\njkDp\\njkQjk\\n1/p\\n= Wp(a,b) + Wp(b,b).\\nThe ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\\ninD, and the third comes from Minkowski’s inequality.\\nProposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\\nProposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\\nX,i.e.\\n(i)d(x,y) =d(y,x)⩾0;\\n(ii)d(x,y) = 0 if and only if x=y;\\n(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\\nThen\\nWp(α,β)def.=Ldp(α,β)1/p(1.16)\\n(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\\nWp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\\n∀(α,β,γ )∈M1\\n+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\\nProof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\\nbetween (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\\nThe Wasserstein distance Wphas many important properties, the most important one being that it is a\\nweak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\\nspatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\\nare not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\\nwith a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\\nbe ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\\np(δx,δy) =d(x,y). Indeed, it suﬃces\\nto notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\\nWp\\np(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\\ncorresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\\n11\\nDeﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\\n+(X)(denotedαk⇀α ) if and only if\\nfor any continuous function g∈C(X),∫\\nXgdαk→∫\\nXgdα. This notion of weak convergence corresponds to\\nthe convergence in law of random vectors.\\nThis convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\\nconvergence of the moments up to order pfor unbounded metric spaces).\\nNote that there exists alternative distances which also metrize weak convergence. The simplest one are\\nHilbertian norms, deﬁned as\\n||α||2\\nkdef.=Eα⊗α(k) =∫\\nX×Xk(x,y)dα(x)dα(y)\\nfor a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\\ne−||x−y||2\\n2σ2for some choice of bandwidth σ>0.\\nThis convergence should not be confounded with the strong convergence of measures, which is metrized\\nby the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\\nAlgorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\\nas interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\\npivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\\nexists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\\nthe auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\\nwhich is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\\nOT problem.\\n1.4 Duality\\nThe Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\\nnaturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\\nfollowing fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\\nrelationship between the primal and dual problems.\\nProposition 4. One has\\nLC(a,b) = max\\n(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\\nwhere the set of admissible potentials is\\nR(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\\nProof. This result is a direct consequence of the more general result on the strong duality for linear pro-\\ngrams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\\nis a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\\nwith the use of Lagrangian duality. The Lagangian associate to (1.11) reads\\nmin\\nP⩾0max\\n(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\\nFor linear program, one can always exchange the min and the max and get the same value of the linear\\nprogram, and one thus consider\\nmax\\n(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\\nP⩾0⟨C−f1⊤\\nm−1ng⊤,P⟩.\\nWe conclude by remarking that\\nmin\\nP⩾0⟨Q,P⟩={0 if Q⩾0\\n−∞ otherwise\\nso that the constraint reads C−f1⊤\\nm−1ng⊤=C−f⊕g⩾0.\\n12\\nThe primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\\ntransport plan\\nSupp( P)⊂{\\n(i,j)∈JnK×JmK;fi+gj=Ci,j}\\n. (1.20)\\nTo extend this primal-dual construction to arbitrary measures, it is important to realize that measures\\nare naturally paired in duality with continuous functions (a measure can only be accessed through integration\\nagainst continuous functions). The duality is formalized in the following proposition, which boils down to\\nProposition 4 when dealing with discrete measures.\\nProposition 5. One has\\nLc(α,β) = max\\n(f,g)∈R(c)∫\\nXf(x)dα(x) +∫\\nYg(y)dβ(y), (1.21)\\nwhere the set of admissible dual potentials is\\nR(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\\nHere, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\\nThe discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\\n(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\\nand (1.20) is generalized as\\nSupp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\\nNote that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\\ntrivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\\nmachinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\\nLipschitz regular, which enable to replace the constraint by a compact one.\\nBenier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\\nRdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\\nare equivalent.\\nTheorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\\nmeasures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\\nKantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\\nRd→Rd. This means that π= (Id,T)♯µ,i.e.\\n∀h∈C(X×Y ),∫\\nX×Yh(x,y)dπ(x,y) =∫\\nXh(x,T(x))dµ(x). (1.24)\\nFurthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\\nϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\\nrelated to the dual potential fsolving (1.21) asϕ(x) =||x||2\\n2−f(x).\\nProof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\\nthat∫\\ncdπ=Cα,β−2∫\\n⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\\n||x||2dα(x) +∫\\n||y||2dβ(y). Instead of\\nsolving (1.14), one can thus consider the following problem\\nmax\\nπ∈U(α,β)∫\\nX×Y⟨x, y⟩dπ(x,y),\\nwhose dual reads\\nmin\\n(ϕ,ψ){∫\\nXϕdα+∫\\nYψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\\n. (1.25)\\n13\\nThe relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\\n2−f,||·||2\\n2−g). One can replace the\\nconstraint by\\n∀y, ψ (y)⩾ϕ∗(y)def.= sup\\nx⟨x, y⟩−ϕ(x). (1.26)\\nHereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\\nalso ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\\nminimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\\nmin\\nϕ∫\\nXϕdα+∫\\nYϕ∗dβ, (1.27)\\nsee also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\\ntwice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\\nCondition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\\nsuch anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\\ny∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\\ndiﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\\neverywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\\nThis results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\\nand its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\\nof Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\\nproblem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\\nBrenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\\nbe examined under the light that a convex function is the natural generalization of the notion of increasing\\nfunctions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\\nfunctions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\\nNote also that this theorem can be extended in many directions. The condition that αhas a density can\\nbe weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\\nthand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\\nstrictly convex function.\\nFor measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\\nconstant) convex function which solves the following Monge-Amp ˜A¨re-type equation\\ndet(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\\nwhere∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\\nnon-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\\nLaplacian ∆ as a linearization since for smooth maps\\ndet(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\\nThe convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\\nSpecial cases In general, computing OT distances is numerically involved. We review special favorable\\ncases where the resolution of the OT problem is easy.\\nRemark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\\nthe diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\\nthe 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\\ndiscrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\\nbetween two discrete measures αandβis equal to their total variation distance.\\n14\\n↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\\nTop: empirical measures with same number of points (optimal matching). Bottom: generic case. This\\ncorresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\\nyj⩽yj′.\\nRemark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\\nn∑n\\ni=1δxiandβ=1\\nn∑n\\nj=1δyj,\\nand assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\\ny1⩽y2⩽...⩽yn, then one has the simple formula\\nWp(α,β)p=p∑\\ni=1|xi−yi|p, (1.29)\\ni.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\\nαandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\\nmight change whenever some of the values change. That formula is a simple consequence of the more general\\nremark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\\nwith the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\\ndiscrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\\ncircle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\\nof the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\\nRemark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\\n∀x∈R,Cα(x)def.=∫x\\n−∞dα, (1.30)\\nwhich is a function Cα:R→[0,1], and its pseudo-inverse C−1\\nα: [0,1]→R∪{−∞}\\n∀r∈[0,1],C−1\\nα(r) = min\\nx{x∈R∪{−∞} ;Cα(x)⩾r}.\\nThat function is also called the generalized quantile function of α. For anyp⩾1, one has\\nWp(α,β)p=||C−1\\nα−C−1\\nβ||p\\nLp([0,1])=∫1\\n0|C−1\\nα(r)−C−1\\nβ(r)|pdr. (1.31)\\nThis means that through the map α↦→C−1\\nα, the Wasserstein distance is isometric to a linear space equipped\\nwith theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\\nmetric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\\ngeometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\\nin§??. Forp= 1, one even has the simpler formula\\nW1(α,β) =||Cα−Cβ||L1(R)=∫\\nR|Cα(x)−Cβ(x)|dx (1.32)\\n=∫\\nR⏐⏐⏐⏐∫x\\n−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\\n15\\nµ ν (tT+ (1−t)Id)♯µ\\n0 0.5 10.5Cµ\\nCν\\n0 0.5 100.51\\nCµ-1\\nCν-1\\n0 0.5 100.51\\nT\\nT-1\\n0 0.5 100.51\\n(Cα,Cβ) (C−1\\nα,C−1\\nβ) ( T,T−1) (1−t)C−1\\nα+tC−1\\nβ\\nFigure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\\nfunction as detailed in (1.34).\\nwhich shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\\nmapTsuch thatT♯α=βis then deﬁned by\\nT=C−1\\nβ◦Cα. (1.34)\\nFigure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\\ninterpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\\noptimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\\nRemark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\\nthen one can show that the following map\\nT:x↦→mβ+A(x−mα), (1.35)\\nwhere\\nA=Σ−1\\n2α(\\nΣ1\\n2αΣβΣ1\\n2α)1\\n2Σ−1\\n2α=AT,\\nis such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\\nsince\\nρβ(T(x)) = det(2πΣβ)−1\\n2exp(−⟨T(x)−mβ,Σ−1\\nβ(T(x)−mβ)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα, ATΣ−1\\nβA(x−mα)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα,Σ−1\\nα(x−mα)⟩),\\nand sinceTis a linear map we have that\\n|detT′(x)|= detA=(detΣβ\\ndetΣα)1\\n2\\nand we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\\nfunctionψ:x↦→1\\n2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\\nthatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\\n16\\n-4 -2 0 2 4 6-3-2-101234\\nρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\\nmean and variance matrices mα= (−2,0),Σα=1\\n2(\\n1−1\\n2;−1\\n21)\\nandmβ= (3,1),Σβ=(\\n2,1\\n2;1\\n2,1)\\n. The\\narrows originate at random points xtaken on the plane and end at the corresponding mappings of those\\npointsT(x) =mβ+A(x−mα).\\nm\\nFigure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\\n1√\\n2πse−(x−m)2\\n2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\\nWith additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\\ncost of that map is\\nW2\\n2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\\nwhereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\\nB(Σα,Σβ)2def.= tr(\\nΣα+Σβ−2(Σ1/2\\nαΣβΣ1/2\\nα)1/2)\\n, (1.37)\\nwhere Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\\nB2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\\ndiagonals, the Bures metric is the Hellinger distance\\nB(Σα,Σβ) =||√r−√s||2.\\nFor 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\\nΣ), as illustrated in Figure 1.11.\\nFor a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\\n1.5 Sinkhorn\\nThis section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\\nof optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\\nthe original problem. This regularization has several important advantages, but a few stand out particularly:\\nThe minimization of the regularized problen can be solved using a simple alternate minimization scheme;\\nthat scheme translates into iterations that are simple matrix products, making them particularly suited to\\nexecution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\\nand positions of the Diracs.\\n17\\nc\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\\nargminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\\nEntropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\\nH(P)def.=−∑\\ni,jPi,j(log(Pi,j)−1), (1.38)\\nwith an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\\n0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\\nPi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\\nto obtain approximate solutions to the original transport problem (1.11):\\nLε\\nC(a,b)def.= min\\nP∈U(a,b)⟨P,C⟩−εH(P). (1.39)\\nSince the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\\nto regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\\ntransportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\\nsolution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\\nto rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\\nthat, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\\nmore “blurred” traﬃc prediction.\\nFigure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\\ncan thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\\nfrom the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\\ntriangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\\nproblem towards an optimal solution of the original linear program has been studied by [ ?].\\nProposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\\nwith maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\\nPεε→0−→argmin\\nP{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\\nso that in particular\\nLε\\nC(a,b)ε→0−→LC(a,b).\\nOne has\\nPεε→∞−→abT= (aibj)i,j. (1.41)\\nProof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\\nε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\\nsuch that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\\nBy optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\\n0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\\n18\\n⇡\"↵\\n\"↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\\nLeft: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\\nn=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\\nbetweenxiandyj).\\nSince His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\\nP⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\\nH(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\\n0to this program is unique\\nby strict convexity of −H, one has P⋆=P⋆\\n0, and the whole sequence is converging.\\nFormula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\\ntransport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\\ncoupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\\ntwo independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\\nperformed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\\nshows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\\nbecomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\\nturn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\\nfaster statistical convergence (as exposed in §??).\\nDeﬁning the Kullback-Leibler divergence between couplings as\\nKL(P|K)def.=∑\\ni,jPi,jlog(Pi,j\\nKi,j)\\n−Pi,j+Ki,j, (1.43)\\nthe unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\\nCas\\nKi,jdef.=e−Ci,j\\nε\\nIndeed one has that using the deﬁnition above\\nPε= ProjKL\\nU(a,b)(K)def.= argmin\\nP∈U(a,b)KL(P|K). (1.44)\\nRemark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\\nby the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\\nregularized counterpart to (1.14) using\\nLε\\nc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\\nwhere the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\\nKL(π|ξ)def.=∫\\nX×Ylog(dπ\\ndξ(x,y))\\ndπ(x,y)+\\n∫\\nX×Y(dξ(x,y)−dπ(x,y)),(1.46)\\n19\\nand by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\\ndξwith respect to ξ. It is important to\\nrealize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\\nplays no speciﬁc role, only its support matters.\\nFormula (1.45) can be re-factored as a projection problem\\nmin\\nπ∈U(α,β)KL(π|K) (1.47)\\nwhereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\\nεdµ(x)dν(y). This problem is often referred to as the\\n“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\\nAsε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\\ndetails an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\\nthe points of two measures.\\nSinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\\nwhich can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\\nthe sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\\nProposition 7. The solution to (1.39) is unique and has the form\\n∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\\nfor two (unknown) scaling variable (u,v)∈Rn\\n+×Rm\\n+.\\nProof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\\nreads\\nE(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\\nConsidering ﬁrst order conditions, we have\\n∂E(P,f,g)\\n∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\\nwhich results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\\nwhich can be rewritten in the form provided in the proposition using non-negative vectors uandv.\\nThe factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\\nmatrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\\ncorrespond to the mass conservation constraints inherent to U(a,b),\\ndiag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\\nThese two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\\ntimes Kvis\\nu⊙(Kv) =aand v⊙(KTu) =b (1.50)\\nwhere⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\\ncommunity as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\\nthese equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\\nEquation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\\nu(ℓ+1)def.=a\\nKv(ℓ)and v(ℓ+1)def.=b\\nKTu(ℓ+1), (1.51)\\ninitialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\\nvectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\\n20\\n`⇡(`)\"\\n1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\\nε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\\nSinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\\nin term of marginal constraint violation log( ||πℓ\\nε1m−b||1).\\nsolution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\\nso doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\\na justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\\nthe same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\\ndiag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\\noptimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\\nRemark 11 (Relation with iterative projections) .Denoting\\nC1\\nadef.={P;P1m=a}andC2\\nbdef.={\\nP;PT1m=b}\\nthe rows and columns constraints, one has U(a,b) =C1\\na∩C2\\nb. One can use Bregman iterative projections [ ?]\\nP(ℓ+1) def.= ProjKL\\nC1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\\nC2\\nb(P(ℓ+1)). (1.52)\\nSince the setsC1\\naandC2\\nbare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\\nThese iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\\nP(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\\none has\\nP(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\\nand P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\\nIn practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\\nmultiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\\nRemark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\\ngreatly simpliﬁed using Hilbert projective metric on Rn\\n+,∗(positive vectors), deﬁned as\\n∀(u,u′)∈(Rn\\n+,∗)2, dH(u,u′)def.= log max\\ni,i′uiu′\\ni′\\nui′u′\\ni.\\nThis can be shows to be a distance on the projective cone Rn\\n+,∗/∼, where u∼u′means that∃s>0,u=su′\\n(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\\ntriangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\\ndistance on bounded open convex sets [ ?]. The projective cone Rn\\n+,∗/∼is a complete metric space for this\\ndistance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\\ntheorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\\nproved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\\ncone of positive vectors.\\n21\\nTheorem 2. Let K∈Rn×m\\n+,∗, then for (v,v′)∈(Rm\\n+,∗)2\\ndH(Kv,Kv′)⩽λ(K)dH(v,v′)where  \\nλ(K)def.=√\\nη(K)−1√\\nη(K)+1<1\\nη(K)def.= max\\ni,j,k,ℓKi,kKj,ℓ\\nKj,kKi,ℓ.\\nRemark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\\nshow the linear convergence of Sinkhorn’s iterations.\\nTheorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\\ndH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\\nOne also has\\ndH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\\n1−λ(K)\\ndH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\\n1−λ(K)(1.54)\\nwhere we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\\n∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\\nwhere P⋆is the unique solution of (1.39) .\\nProof. One notice that for any ( v,v′)∈(Rm\\n+,∗)2, one has\\ndH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\\nThis shows that\\ndH(u(ℓ+1),u⋆) =dH(a\\nKv(ℓ),a\\nKv⋆)\\n=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\\nwhere we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\\ndH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\\n⩽dH(a\\nKv(ℓ),u(ℓ))\\n+λ(K)dH(u(ℓ),u⋆)\\n=dH(\\na,u(ℓ)⊙(Kv(ℓ)))\\n+λ(K)dH(u(ℓ),u⋆),\\nwhich gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\\nof (1.55) follows from [ ?, Lemma 3]\\nThe bound (1.54) shows that some error measures on the marginal constraints violation, for instance\\n∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\\nFigure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\\ndegrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\\nTheory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\\nconvergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\\nof the scaled coupling matrix.\\n22\\nRegularized Dual and Log-domain Computations The following proposition details the dual problem\\nassociated to (1.39).\\nProposition 8. One has\\nLε\\nC(a,b) = max\\nf∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\\nThe optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\\n(u,v) = (ef/ε,eg/ε). (1.57)\\nProof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\\nand dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\\nLagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\\ndual function equals\\nf,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\\nThe entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\\n⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\\n=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\\ntherefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\\nare those displayed in (1.56).\\nRemark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\\nproblem (1.56) reads\\nsup\\nf,g∈C(X)×C(Y)∫\\nXf(x)dα(x) +∫\\nYg(x)dβ(x)−ε∫\\nX×Ye−c(x,y)+f(x)+g(y)\\nε dα(x)dβ(y)\\nThis corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\\nis retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\\npotentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\\nusec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\\nconvergence of Sinkhorn iterations, see [ ?] for more details.\\nRemark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\\nunconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\\nupdate alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\\ncan easily notice that, writing Q(f,g) for the objective of (1.56) that\\n∇|fQ(f,g) =a−ef/ε⊙(\\nKeg/ε)\\n, (1.59)\\n∇|gQ(f,g) =b−eg/ε⊙(\\nKTef/ε)\\n. (1.60)\\nBlock coordinate ascent can therefore be implemented in a closed form by applying successively the following\\nupdates, starting from any arbitrary g(0), forl⩾0,\\nf(ℓ+1)=εloga−εlog(\\nKeg(ℓ)/ε)\\n, (1.61)\\ng(ℓ+1)=εlogb−εlog(\\nKTef(ℓ+1)/ε)\\n. (1.62)\\nSuch iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\\ndual relations highlighted in (1.57). Indeed, we recover that at any iteration\\n(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\\n23\\nRemark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\\nusing the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\\ncoordinates, namely\\nminεz=−εlog∑\\nie−zi/ε.\\nNote that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\\ndiﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\\nrewritten\\n(f(ℓ+1))i= minε(Cij−g(ℓ)\\nj)j+εlogai, (1.63)\\n(g(ℓ+1))j= minε(Cij−f(ℓ)\\ni)i+εlogbj. (1.64)\\nHere the term min ε(Cij−g(ℓ)\\nj)jdenotes the soft-minimum of all values of the j-th column of matrix\\n(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\\nnow a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\\nwe deﬁne\\nMinrow\\nε(A)def.=(\\nminε(Ai,j)j)\\ni∈Rn,\\nMincol\\nε(A)def.=(\\nminε(Ai,j)i)\\nj∈Rm.\\nNote that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\\nlar (??)). Using these notations, Sinkhorn’s iterates read\\nf(ℓ+1)= Minrow\\nε(C−1ng(ℓ)T) +εloga, (1.65)\\ng(ℓ+1)= Mincol\\nε(C−f(ℓ)1mT) +εlogb. (1.66)\\nNote that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\\nbecause alternate minimization does not converge for constrained problems (which is the case for the un-\\nregularized dual (1.17)).\\nRemark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\\ntions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\\nofε. Writing z = min z, that trick suggests to evaluate min εzas\\nminεz= z−εlog∑\\nie−(zi−z)/ε. (1.67)\\nInstead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\\npreviously computed scalings. This leads to the following stabilized iteration\\nf(ℓ+1)= Minrow\\nε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\\ng(ℓ+1)= Mincol\\nε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\\nwhere we deﬁned\\nS(f,g) =(\\nCi,j−fi−gj)\\ni,j.\\nIn contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\\narbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\\nrequiresnmcomputations of exp at each step. Computing a Minrow\\nεor Mincol\\nεis typically substantially\\nslower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\\ntherefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\\nIn Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\\nεstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\\n24\\n1.6 Extensions\\nWasserstein Barycenters. Given input histogram {bs}S\\ns=1, wherebs∈Σns, and weights λ∈ΣS, a\\nWasserstein barycenter is computed by minimizing\\nmin\\na∈ΣnS∑\\ns=1λsLCs(a,bs) (1.70)\\nwhere the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\\nbarycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\\nsolves\\nmin\\na∈ΣnS∑\\ns=1λsWp\\np(a,bs).\\nThis barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\\nin particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\\nhas a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\\none guaranteeing the existence of a Monge map, see Remark ??).\\nThe barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\\ncouplings ( Ps)sbetween each input and the barycenter itself\\nmin\\na∈Σn,(Ps∈Rn×ns)s{S∑\\ns=1λs⟨Ps,Cs⟩;∀s,P⊤\\ns1ns=a,P⊤\\ns1n=bs}\\n.\\nAlthough this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\\ncan therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\\nRemark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\\nthe barycenter problem becomes\\nmin\\nα∈M1\\n+(X)S∑\\ns=1λsLc(α,βs). (1.71)\\nIn the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\\nthen this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\\nbarycenters of points ( xs)S\\ns=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\\nsolution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\\nof the barycenter α⋆is necessarily the barycenter of the mean, i.e.\\n∫\\nXxdα⋆(x) =∑\\nsλs∫\\nXxdαs(x),\\nand the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\\napproximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\\nusing discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\\nre-cast (1.71) as a multi-marginal OT problem, see Remark ??.\\nOne can use entropic smoothing and approximate the solution of (1.70) using\\nmin\\na∈ΣnS∑\\ns=1λsLε\\nCs(a,bs) (1.72)\\nfor someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\\ndescent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\\n25\\nuseful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\\nbut eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\\nmin\\n(Ps)s{∑\\nsλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\\n(1.73)\\nwhere we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\\nthe couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\\nthis problem, which also corresponds to iterative projection. This can also be seen as a special case of the\\ngeneralized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\\nform as\\nPs= diag( us)Kdiag(vs), (1.74)\\nand the scalings are sequentially updated as\\n∀s∈J1,SK,v(ℓ+1)\\nsdef.=bs\\nKT\\nsu(ℓ)\\ns, (1.75)\\n∀s∈J1,SK,u(ℓ+1)\\nsdef.=a(ℓ+1)\\nKsv(ℓ+1)\\ns, (1.76)\\nwhere a(ℓ+1)def.=∏\\ns(Ksv(ℓ+1)\\ns)λs. (1.77)\\nAn alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\\nproblem, which detailed in the following proposition.\\nProposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\\n(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\\nmax\\n(fs,gs)s{∑\\nsλs(\\n⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\\n;∑\\nsλsfs= 0}\\n. (1.78)\\nProof. Introducing Lagrange multipliers in (1.73) leads to\\nmin\\n(Ps)s,amax\\n(fs,gs)s∑\\nsλs(\\nεKL(Ps|Ks) +⟨a−Ps1m,fs⟩\\n+⟨bs−PsT1m,gs⟩)\\n.\\nStrong duality holds, so that one can exchange the min and the max, and gets\\nmax\\n(fs,gs)s∑\\nsλs(\\n⟨gs,bs⟩+ min\\nPsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\\n+ min\\na⟨∑\\nsλsfs,a⟩.\\nThe explicit minimization on agives the constraint∑\\nsλsfs= 0 together with\\nmax\\n(fs,gs)s∑\\nsλs⟨gs,bs⟩−εKL∗(fs⊕gs\\nε|Ks)\\nwhere KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\\nKL∗(U|K) =∑\\ni,jKi,j(eUi,j−1), (1.79)\\n26\\nFigure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\\n(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\\nare uniform within the boundaries of the shape and null outside.\\nwhich shows the desired formula. To show (1.79), since this function is separable, one needs to compute\\n∀(u,k)∈R2\\n+,KL∗(u|k)def.= max\\nrur−(rlog(r/k)−r+k)\\nwhose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\\nMinimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\\nform by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\\nto the expression (1.76).\\nFigures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\\nof barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\\nthe computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\\nto [?] for more details and other applications to computer graphics and imaging sciences.\\nWasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\\ndistribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\\nΘ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\\nterm, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\\nsuitable parameter θis obtained by minimizing directly\\nmin\\nθ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\\nOf course, one can consider more complicated problems: for instance, the barycenter problem described\\nin§??consists in a sum of such terms. However, most of these more advanced problems can be usually\\nsolved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\\nor using automatic diﬀerentiation.\\nThe Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\\nas shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\\nΣnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\\ni=1θiαi\\nis a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\\ncorresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\\na Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\\nnot convex.\\n27\\ng✓XZ⇣xz↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\\nA practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\\nsome discrete samples ( xi)n\\ni=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\\nθ↦→αθ∈M (X) to the observed empirical input measure β\\nmin\\nθ∈ΘL(αθ,β) where β=1\\nn∑\\niδxi, (1.81)\\nwhereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\\nure 1.16).\\nIn the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\\nreference measure), the maximum likelihood estimator (MLE) is obtained by solving\\nmin\\nθLMLE(αθ,β)def.=−∑\\nilog(ρθ(xi)).\\nThis corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\\nsamples of some ¯β, then\\nLMLE(α,β)n→+∞−→ KL(α|¯β)\\nThis MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\\nHowever, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\\n(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\\nthe same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\\nin several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\\nA typical setup where both problems (singular and unknown densities) occur is for so-called generative\\nmodels, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\\nαθ=hθ,♯ζwherehθ:Z→X\\nwhere the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\\nthat the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\\nsingular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\\nis usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\\n(zi)iare i.i.d. samples from ζ.\\nIn order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\\nLMLE, which needs to be written in dual form as\\nL(α,β)def.= max\\n(f,g)∈C(X)2{∫\\nXf(x)dα(x) +∫\\nXg(x)dβ(x) ; (f,g)∈R}\\n. (1.82)\\nDual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\\nsetsR=R(c) as deﬁned in (1.22).\\n28\\nFor a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\\nsolving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\\nrespect toθis much more involved, and is typically highly non-convex.\\nThe class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\\nwas initially introduced in [ ?], see also [ ?].\\nGromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\\nthus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\\nthese spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\\nnamely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\\nbetween the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\\nof) distance matrices. The Gromov-Wasserstein problem reads\\nGW(( a,D),(b,D′))2def.= min\\nP∈U(a,b)ED,D′(P)def.=∑\\ni,j,i′,j′|Di,i′−D′\\nj,j′|2Pi,jPi′,j′. (1.83)\\nThis is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\\nfull generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\\nfor a particular cost.\\nOne can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\\nmetric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\\nup to isometries preserving the measures. This distance was introduced and studied in details by Memoli\\nin [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\\nin [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\\nGromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\\nRemark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\\nmetric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\\non their respective spaces. One deﬁnes\\nGW((αX,dX),(αY,dY))2def.= min\\nπ∈U(αX,αY)∫\\nX2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\\nGW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\\n(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\\nRemark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\\nthisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\\n(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\\n((x0,x1),(x′\\n0,x′\\n1))∈(X0×X 1)2,\\ndt((x0,x1),(x′\\n0,x′\\n1))def.= (1−t)dX0(x0,x′\\n0) +tdX1(x1,x′\\n1).\\nThis formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\\nspaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\\nspaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\\nspaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\\ndetailed below.\\nTo approximate the computation of GW, and to help convergence of minimization schemes to better\\nminima, one can consider the entropic regularized variant\\nmin\\nP∈U(a,b)ED,D′(P)−εH(P). (1.85)\\n29\\nFigure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\\niterations (1.86). Extracted from [ ?].\\nAs proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\\nSinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\\nof the objective function lead to consider the succession of updates\\nP(ℓ+1) def.= min\\nP∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\\nC(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\\nwhich can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\\niterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\\ncompute soft maps between domains.\\n30\\nBibliography\\n[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\\nLAB. SIAM, 2014.\\n[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\\nand statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\\nin Machine Learning , 3(1):1–122, 2011.\\n[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\\n[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\\npiecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\\n[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\\nMultiscale Modeling and Simulation , 5:861–899, 2005.\\n[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\\n20:89–97, 2004.\\n[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\\nduction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\\nrecovery , 9(263-340):227, 2010.\\n[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\\nNumerica , 25:161–319, 2016.\\n[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\\non Scientiﬁc Computing , 20(1):33–61, 1999.\\n[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\\n[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\\nMultiscale Modeling and Simulation , 4(4), 2005.\\n[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\\nwith a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\\n[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\\nDec 1994.\\n[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\\n375. Springer Science & Business Media, 1996.\\n[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\\nImage Proc. , 12(8):906–916, 2003.\\n[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\\nBirkh¨ auser Basel, 2013.\\n31\\n[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\\n[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\\ntional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\\n[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\\n1(3):127–239, 2014.\\n[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\\n[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\\nGaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\\n[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\\nD, 60(1-4):259–268, 1992.\\n[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\\nVariational methods in imaging . Springer, 2009.\\n[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\\n27(3):379–423, 1948.\\n[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\\nrelated geometric multiscale analysis . Cambridge university press, 2015.\\n32'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okzDLY9OLSoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aX1ImKYW1vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#semantic chunking MarkdownHeaderTextSplitter\n",
        "'''\n",
        "The effectiveness of semantic chunking techniques depends on various factors such as the nature of your data, your specific use case, and the desired outcomes.\n",
        "The approach we're using, which combines Markdown header-based splitting with text embeddings for semantic analysis, is a reasonable method, especially if we're\n",
        "dealing with Markdown documents or structured text like README files or articles where people use headers to organize their content. while semantic analysis is powerful for understanding text meaning, it's often most effective when combined with other techniques like the one you're using, which leverages document structure (e.g., headers) to provide additional context. By combining both approaches, you can achieve a more robust understanding of the text and its structure.\n",
        "'''\n",
        "\n",
        "headers_to_split_on = [\n",
        "            (\"#\", \"Header 1\"),\n",
        "            (\"##\", \"Header 2\"),\n",
        "            (\"###\", \"Header 3\"),\n",
        "            (\"####\", \"Header 4\"),\n",
        "        ]\n",
        "strip_headers = False\n",
        "\n",
        "md_header_splits = text_chunker.markdown_header_textsplitter(text, headers_to_split_on, strip_headers)\n",
        "\n",
        "cdocs = []\n",
        "for doc in md_header_splits:\n",
        "    document = Document(page_content=doc.page_content)\n",
        "    cdocs.append(document)\n",
        "\n",
        "\n",
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( md_header_splits , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "28da3384a4664f4cbb7c55c9d865e089",
            "5935e35a35de4b8ebb65e8a1cbd8742d",
            "62936512ce2640d881b6d6290dba0ae6",
            "67b4220a7f59493db5063bdc4b7b325d",
            "d235a3b36df74dd2b99013c8f4e78460",
            "4463e4bf3bb84f78bc2d609849a8e5d2",
            "be4a715205434d60a43ee35f704f4a85",
            "e616b82a8e394088b84f91b609eeea2b",
            "0a69b359d1ac4ab884482580ab146e5f",
            "cee1a435638f4123b53c5455340ba6bb",
            "3fd2cb5066f94f6faedc2abb1200e37a"
          ]
        },
        "id": "KD8_am8IRLHB",
        "outputId": "97ba7290-21ab-4e79-cdf0-f1bba26c7297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28da3384a4664f4cbb7c55c9d865e089"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ro0wWLJFiCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZ1SZ4TnR9aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvoFmZJNR9wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MarkdownTextSplitter\n",
        "\n",
        "split_text = text_chunker.markdown_textsplitter(text)\n",
        "for i, text_chunk in enumerate(split_text):\n",
        "    print(f\"Chunk {i + 1}:\\n{text_chunk}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIQGn5YEFiHq",
        "outputId": "61c2083e-97a1-44f4-8eb3-3bea8f4d0f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "\n",
            "Chunk 2:\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "\n",
            "Chunk 3:\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "\n",
            "Chunk 4:\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "\n",
            "Chunk 5:\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "\n",
            "Chunk 6:\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "\n",
            "Chunk 7:\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "\n",
            "Chunk 8:\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "\n",
            "Chunk 9:\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "\n",
            "Chunk 10:\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "\n",
            "Chunk 11:\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "\n",
            "Chunk 12:\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "\n",
            "Chunk 13:\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "\n",
            "Chunk 14:\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "\n",
            "Chunk 15:\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "\n",
            "Chunk 16:\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "\n",
            "Chunk 17:\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "\n",
            "Chunk 18:\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "\n",
            "Chunk 19:\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJbli4d25sLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZgYxeSn5sOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOgfu0a1BJmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MrB77rQo_PDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"PythonCodeTextSplitter\")\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"KonlpyTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Zcna-HqsC5",
        "outputId": "4388f4a9-d1e5-4447-a31d-dae7d87f9931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences Gabriel Peyr´ e CNRS & DMA ´Ecole Normale Sup´ erieure gabriel.peyre @ens .fr https: //mathematical-tours .github .io www.numerical-tours .com August 14, 2019 2 Chapter 1 Optimal Transport 1.1 Radon Measures Measures. We will interchangeably the term histogram or probability vector for any element a∈ Σnthat belongs to the probability simplex Σndef.={ a∈Rn + ;n∑ i=1ai= 1} . A discrete measure with weights aand locations x1,... ,xn ∈X reads α =n∑ i=1ai δxi (1.1) whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location x. Such as measure describes a probability measure if, additionally, a∈ Σn, and more generally a positive measure if each of the “weights” described in vector ais positive itself. Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous “objects” within the same framework. Such objects only need to be modelled as measures. This corresponds to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating) it against continuous functions, denoted f∈C (X). Integration of f∈C (X) against a discrete measure αcomputes a sum ∫ Xf(x )dα (x) =n∑ i=1aif (xi). More general measures, for instance on X=Rd (whered ∈N ∗is the dimension), can have a density dα (x) =ρα (x )dxw .r .t. the Lebesgue measure, often denoted ρα =dα dx, which means that ∀h ∈C (Rd),∫ Rdh(x )dα (x) =∫ Rdh(x)ρα (x )dx. An arbitrary measure α ∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by the fact that it can be integrated agains any continuous function f∈C (X) and obtain∫ Xf(x )dα (x) ∈R. IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity. Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+ (X) the set of all positive measures on X. The set of probability measures is denoted M1 + (X), which means that anyα ∈M1 + (X) is positive, and that α (X) =∫ Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent classes of measures, beyond histograms, considered in this work. 3 Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2 Figure 1.1: Schematic display of discrete distributions α= ∑n i=1ai δxi (red corresponds to empirical uniform distribution ai= 1/n, and blue to arbitrary distributions) and densities d α (x) =ρα (x )dx (in violet), in both 1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai) and in 2-D using point clouds (radius equal to ai). Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator T♯ :M (X) →M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the positions of all the points in the support of the measure T♯ αdef.=∑ iaiδT (xi). For more general measures, for instance for those with a density, the notion of push-forward plays a funda- mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow. Deﬁnition 1 (Push-forward) .ForT :X → Y , the push forward measure β =T♯α∈ M (Y )of some α ∈M (X )reads ∀h ∈C (Y),∫ Yh(y )dβ (y) =∫ Xh(T (x ))dα (x). (1.2) Equivalently, for any measurable set B⊂Y, one has β (B) =α( {x ∈X ;T (x) ∈B}). (1.3) Note thatT♯preserves positivity and total mass, so that if α ∈M1 + (X )thenT♯α ∈M1 + (Y). Intuitively, a measurable map T:X →Y , can be interpreted as a function “moving” a single point from a measurable space to another. The more general extension T♯can now “move” an entire probability measure onXtowards a new probability measure on Y. The operator T♯ “pushes forward” each elementary mass of a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a new measure onY) writtenT♯α. Note that such a push-forward T♯ :M1 + (X) →M1 + (Y) is a linear operator between measures in the sense that for two measures α1, α2onX ,T♯( α1+ α2) =T♯ α1 +T♯ α2. Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on densities linearly as a change of variables in the integration formula, indeed ρα (x) = |det (T′ (x))|ρβ (T (x)) (1.4) whereT′ (x) ∈Rd ×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate ofT). This implies, denoting y=T (x) |det (T′ (x))|=ρα (x) ρβ (y). 4 =Pi \u0000xiT ↵T] ↵def. =Pi \u0000T (xi) TT]gdef. =g \u0000TgPush-forward of measures Pull-back of functions Figure 1.2: Comparison of push-forward T♯and pull-back T♯. Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with the pull-back of function T♯ :C (Y) →C (X) which corresponds to the “warping” of functions. It is the linear map deﬁned, for g∈C (Y) byT♯g =g ◦T. Push-forward and pull-back are actually adjoint one from each others, in the sense that ∀(α ,g) ∈M (X) ×C (Y),∫ Ygd(T♯α) =∫ X(T ♯g )dα. It is important to realize that even if ( α,β) have densities ( ρα,ρβ) ,T♯ αis not equal to T♯ρβ, because of the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction between these push-forward and pull-back operators. Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri- butions of random variables. A random variable XonXis actually a map X: Ω →X from some abstract (often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1 + (X) such thatP(X ∈A) =α (A) =∫ Adα (x). Equivalently, it is the push-forward of PbyX,α =X ♯P. Applying another push-forward β =T♯ αforT :X →Y , following (1.2), is equivalent to deﬁning another random variableY=T (X) :ω∈Ω →T (X(ω)) ∈Y, so thatβis the distribution of Y. Drawing a random sample yfromYis thus simply achieved by computing y=T (x) wherexis drawn from X. Convergence of random variable. Convergence of random variable (in probability, almost sure, in law), convergence of measures (strong, weak). 1.2 Monge Problem Given a cost matrix ( Ci,j )i ∈JnK ,j ∈JmK, assuming n=m, the optimal assignment problem seeks for a bijectionσin the set Perm( n) of permutations of nelements solving min σ ∈Perm (n )1 nn∑ i=1Ci,σ (i). (1.5) One could naively evaluate the cost function above using all permutations in the set Perm( n). However, that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than 10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient algorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5 x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence, either matching σ= (1 ,2) (full line) or σ= (2 ,1) (dotted line) is optimal. (right) a Monge map can associate the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the disk marked at each location. The mapping here is such that T(x1) =T (x2) =y2 ,T (x3) =y3, whereas for 4⩽i ⩽7 we haveT(xi) =y1. Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions. Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4 corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case only two assignments exist, and they share the same cost. For discrete measures α =n∑ i=1ai δxiandβ =m∑ j=1bj δyj (1.6) the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must push the mass of αtoward the mass of β, which is to say that such a map T: {x1,... ,xn}→ {y1,... ,ym} must verify that ∀j ∈JmK ,bj=∑ i:T (xi) =yjai (1.7) which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is parameterized by a function c(x ,y) deﬁned for points ( x,y) ∈X ×Y min T{∑ ic(xi ,T (xi)) ;T♯α=β} . (1.8) Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using indicesσ :JnK →JmKso thatj=σ (i), and the mass conservation is written as ∑ i∈σ −1 (j )ai =bj. In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation constraint implies that Tis a bijection, such that T(xi) =yσ (i), and the Monge problem is equivalent to the optimal matching problem (1.5) where the cost matrix is Ci,jdef. =c (xi ,yj). Whenn̸ =m, note that, optimality aside, Monge maps may not even exist between an empirical measure to another. This happens when their weight vectors are not compatible, which is always the case when the target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows an (optimal) Monge map between αandβ, but there is no Monge map from βtoα. 6 Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces (X ,Y) as ﬁnding a map T:X →Y that minimizes min T{∫ Xc(x ,T (x ))dα (x) ;T♯α=β} (1.9) The constraint T♯α= βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward operator (1.2). 1.3 Kantorovitch Problem The assignment problem has several limitations in practical settings, also encountered when using the Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only be used to compare two points clouds of the same size. A direct generalization to discrete measures with non- uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7) (see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation. Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na- ture of transportation, namely the fact that a source point xican only be assigned to another, or transported to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially dispatched across several locations. Kantorovich moves away from the idea that mass transportation should be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded using, in place of a permutation σor a mapT, a coupling matrix P∈Rn ×m +, where Pi,jdescribes the amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj) ,xitowardsyjin the formalism of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps: U(a ,b )def.={ P∈Rn ×m + ;P1m =aand PT1n =b} , (1.10) where we used the following matrix-vector notation P1m= ∑ jPi,j  i∈Rnand PT1n=(∑ iPi,j) j∈Rm. The set of matrices U(a ,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex polytope (the convex hull of a ﬁnite set of matrices). Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in U(a ,b) if and only if PTis inU(b ,a). Kantorovich’s optimal transport problem now reads LC(a ,b )def.= min P∈U (a ,b) ⟨C ,P ⟩def.=∑ i,jCi ,jPi ,j. (1.11) This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are not necessarily unique. 7 ↵\u0000 ↵ \u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching, corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points). Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to associate two arbitrary discrete measures. Permutation Matrices as Couplings For a permutation σ ∈Perm (n), we write Pσfor the correspond- ing permutation matrix, ∀ (i ,j) ∈JnK2, (Pσ )i ,j= {1 /n ifj= σi, 0 otherwise. (1.12) One can check that in that case ⟨C ,Pσ⟩ =1 nn∑ i=1Ci, σi, which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the couplings Pare restricted to be exactly permutation matrices: min σ ∈Perm (n )1 nn∑ i=1Ci,σ (i)= min σ ∈Perm (n) ⟨C ,Pσ⟩. Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ polytope U(1n /n ,1n ,n). Indeed, for any permutation σwe have Pσ1 =1nandP σT1 =1n, whereas 1n1nT /n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that min σ ∈Perm (n) ⟨C ,Pσ⟩ ⩽LC (1n /n ,1n /n). The following proposition shows that these problems result in fact in the same optimum, namely that one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform measures a=b =1n /n, which shows that the Kantorovich relaxation is tight when considered on assignment problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special case. Proposition 1 (Kantorovich for matching) .Ifm =nanda =b =1n /n, then there exists an optimal solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈ Perm(n )for Problem (1.5) . Proof. Birkhoﬀ ’s theorem states that the set of extremal points of U(1n /n ,1n /n) is equal to the set of permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the polyhedron. 8 ⇡\u0000↵\u0000↵ ⇡\u0000↵\u0000↵ ⇡\u0000↵\u0000↵ Discrete Semi-discrete Continuous Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup. ⇡\u0000↵ ⇡\u0000↵ Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The coupling is localized along the graph of the Monge map ( x,T (x)) (displayed in black). Right: “discrete” couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare display with a black disk at position ( i,j) with radius proportional to Ti,j. Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to arbitrary measures by considering couplings π ∈M1 + (X ×Y ) which are joint distributions over the product space. The discrete case is a special situation where one imposes this product measure to be of the form π=∑ i,jPi ,jδ (xi ,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a marginal constraint on joint probability distributions U(α,β )def.={ π ∈M1 + (X ×Y ) ;PX♯π= αandPY♯π=β} . (1.13) HerePX♯andPY ♯are the push-forward (see Deﬁnition 1) by the projections PX(x ,y) =xandPY (x ,y) =y. Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π (A ×Y) = α (A) andπ (X ×B) =β (B) for setsA⊂X andB⊂Y. The Kantorovich problem (1.11) is then generalized as Lc(α,β )def.= min π ∈U(α,β)∫ X×Yc (x ,y )dπ (x ,y). (1.14) This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings, involving discrete and continuous marginals. On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x ,y) 9 \u0000↵\u0000↵⇡ \u0000↵\u0000↵⇡ \u0000↵\u0000↵⇡ ↵\u0000↵⇡ \u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps above (arrows) and couplings below. Inspired by [ ?]. is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs to impose moment condition on αandβ. Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be understood as a canonical way to lift a ground distance between points to a distance between histogram or measures. We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like to compare. The following proposition states that OT provides a meaningful distance between histograms supported on these bins. Proposition 2. We suppose n=m, and that for some p⩾1 ,C =Dp= (Dp i,j )i ,j ∈Rn ×nwhere D∈Rn ×n + is a distance on JnK,i .e. 1.D ∈Rn ×n + is symmetric; 2.Di ,j= 0if and only if i=j; 3.∀ (i ,j ,k ) ∈JnK3 ,Di ,k ⩽Di ,j +Dj ,k. Then Wp(a ,b )def.= LDp(a ,b )1 /p (1.15) (note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn ,i .e. Wpis symmetric, positive, Wp(a ,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality ∀a ,a′ ,b∈ Σn ,Wp (a ,b) ⩽Wp (a ,a′) + Wp(a′ ,b). Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal, Wp(a ,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ -diagonal elements of Dp, Wp(a ,b) >0 whenever a̸ =b (because in this case, an admissible coupling necessarily has a non-zero element outside the diagonal); by symmetry of Dp, Wp(a ,b) = 0 is itself a symmetric function. To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting, the explicit constuction of this glued coupling is simple. Let a,b ,c∈ Σn. Let PandQbe two optimal solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef. =bjifbj >0 and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne Sdef. =Pdiag (1/ ¯b )Q ∈Rn ×n +. 10 We remark that S∈U (a ,c) because S1n =Pdiag (1/ ¯b )Q1n =P (b/ ¯b) =P1Supp( b) =a where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b) =P1 =b because necessarily Pi,j= 0 forj / ∈Supp( b). Similarly one veriﬁes that S⊤1n =c. The triangle inequality follows from Wp(a ,c) =( min P∈U (a ,c) ⟨P ,Dp⟩ )1 /p ⩽ ⟨S ,Dp ⟩1 /p = ∑ ikDp ik∑ jPijQjk ¯bj 1 /p ⩽ ∑ ijk(Dij +Djk )pPijQjk ¯bj 1 /p ⩽ ∑ ijkDp ijPijQjk ¯bj 1 /p + ∑ ijkDp jkPijQjk ¯bj 1 /p = ∑ ijDp ijPij∑ kQjk ¯bj 1 /p + ∑ jkDp jkQjk∑ iPij ¯bj 1 /p = ∑ ijDp ijPij 1 /p + ∑ jkDp jkQjk 1 /p = Wp(a ,b) + Wp(b ,b). The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements inD, and the third comes from Minkowski’s inequality. Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete. Proposition 3. We assumeX=Y, and that for some p⩾1 ,c (x ,y) =d (x ,y )pwheredis a distance on X,i .e. (i )d (x ,y) =d (y ,x) ⩾0; (ii )d (x ,y) = 0 if and only if x=y; (ii)∀ (x ,y ,z ) ∈X3 ,d (x ,z) ⩽d (x ,y) +d (y ,z). Then Wp(α,β )def. =Ldp(α,β )1 /p (1.16) (note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i .e .Wpis symmetric, positive, Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality ∀(α,β,γ ) ∈M1 + (X )3 ,Wp(α,γ) ⩽Wp(α,β) +Wp(β,γ). Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ). The Wasserstein distance Wphas many important properties, the most important one being that it is a weak distance, i.e .it allows to compare singular distributions (for instance discrete ones) and to quantify spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences) are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to be ﬁxed to work). In sharp contrast, one has that for any p >0 ,Wp p( δx, δy) =d (x ,y). Indeed, it suﬃces to notice thatU( δx, δy) ={ δx ,y }and therefore the Kantorovich problem having only one feasible solution, Wp p( δx, δy) is necessarily ( d(x ,y )p )1 /p =d (x ,y). This shows that Wp( δx, δy) →0 ifx→y. This property corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne. 11 Deﬁnition 2 (Weak convergence) .( αk )kconverges weakly to αinM1 + (X) (denoted αk⇀α ) if and only if for any continuous function g∈C (X),∫ Xgdαk→∫ Xgdα. This notion of weak convergence corresponds to the convergence in law of random vectors. This convergence can be shown to be equivalent to Wp( αk,α) →0 [?, Theorem 6.8] (together with a convergence of the moments up to order pfor unbounded metric spaces). Note that there exists alternative distances which also metrize weak convergence. The simplest one are Hilbertian norms, deﬁned as ||α ||2 kdef. =Eα⊗α (k) =∫ X×Xk (x ,y )dα (x )dα (y) for a suitable choice of kernel k:X2 →R. The most famous of such kernel is the Gaussian one k(x ,y) = e− ||x −y ||2 2σ2for some choice of bandwidth σ >0. This convergence should not be confounded with the strong convergence of measures, which is metrized by the TV norm ||α ||TVdef.=|α| (X), which is the total mass of the absolute value of the measure. Algorithms Since ( ??) ˆA is a linear program, it is possible to use any classical linear program solver, such as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b =1n /n, there exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm, which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the OT problem. 1.4 Duality The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the relationship between the primal and dual problems. Proposition 4. One has LC(a ,b) = max (f ,g) ∈R (a ,b) ⟨f ,a⟩+ ⟨g ,b⟩ (1.17) where the set of admissible potentials is R(a ,b )def.={ (f ,g) ∈Rn ×Rm;∀ (i ,j) ∈JnK ×JmK ,f ⊕g ⩽C} (1.18) Proof. This result is a direct consequence of the more general result on the strong duality for linear pro- grams [ ?, p.148 ,Theo .4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17) is a lower bound on L C(a ,b) is discussed in ??. For the sake of completeness, let us derive this dual problem with the use of Lagrangian duality. The Lagangian associate to (1.11) reads min P⩾0max (f ,g) ∈Rn ×Rm ⟨C ,P⟩+ ⟨a −P1m ,f⟩+ ⟨b −P ⊤1n ,g⟩. (1.19) For linear program, one can always exchange the min and the max and get the same value of the linear program, and one thus consider max (f ,g) ∈Rn ×Rm ⟨a ,f⟩+ ⟨b ,g⟩+ min P⩾0 ⟨C −f1⊤ m−1ng⊤ ,P⟩. We conclude by remarking that min P⩾0 ⟨Q ,P⟩= {0 if Q⩾0 −∞ otherwise so that the constraint reads C−f1⊤ m−1ng⊤ =C −f ⊕g ⩾0. 12 The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal transport plan Supp( P)⊂{ (i ,j) ∈JnK ×JmK ;fi +gj =Ci ,j} . (1.20) To extend this primal-dual construction to arbitrary measures, it is important to realize that measures are naturally paired in duality with continuous functions (a measure can only be accessed through integration against continuous functions). The duality is formalized in the following proposition, which boils down to Proposition 4 when dealing with discrete measures. Proposition 5. One has Lc(α,β) = max (f ,g) ∈R (c)∫ Xf(x )dα (x) +∫ Yg(y )dβ (y), (1.21) where the set of admissible dual potentials is R(c )def.={ (f ,g) ∈C (X) ×C (Y) ;∀ (x ,y) ,f (x) +g (y) ⩽c (x ,y)}. (1.22) Here, (f ,g )is a pair of continuous functions, and are often called “Kantorovich potentials”. The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e. (fi ,gj) = (f (xi) ,g (yj)). The primal-dual optimality conditions allow to track the support of optimal plan, and (1.20) is generalized as Supp(π)⊂{ (x ,y) ∈X ×Y ;f (x) +g (y) =c (x ,y)}. (1.23) Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non- trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily Lipschitz regular, which enable to replace the constraint by a compact one. Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems are equivalent. Theorem 1 (Brenier) .In the caseX=Y =Rdandc (x ,y) = ||x −y ||2, if at least one of the two inputs measures (denoted α) has a density ρ αwith respect to the Lebesgue measure, then the optimal πin the Kantorovich formulation (1.14) is unique, and is supported on the graph (x ,T (x ))of a “Monge map” T: Rd→Rd. This means that π= (Id ,T)♯µ ,i .e. ∀h ∈C (X ×Y ),∫ X×Yh (x ,y )dπ (x ,y) =∫ Xh(x ,T (x ))dµ (x). (1.24) Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ ,T (x) =∇ϕ (x), where ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is related to the dual potential fsolving (1.21) asϕ (x) = ||x ||2 2−f (x). Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark that∫ cdπ =Cα,β −2∫ ⟨x, y⟩dπ (x ,y) where the constant is Cα,β=∫ ||x ||2dα (x) +∫ ||y ||2dβ (y). Instead of solving (1.14), one can thus consider the following problem max π ∈U(α,β)∫ X×Y ⟨x, y⟩dπ (x ,y), whose dual reads min (ϕ,ψ){∫ Xϕdα+∫ Yψdβ;∀ (x ,y), ϕ (x) +ψ (y)⩾ ⟨x, y⟩} . (1.25) 13 The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||· ||2 2−f,||· ||2 2−g). One can replace the constraint by ∀y, ψ (y)⩾ϕ∗ (y )def.= sup x⟨x, y⟩−ϕ (x). (1.26) Hereϕ ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can minimize explicitly with respect to ϕand setψ=ϕ ∗in order to consider the unconstraint problem min ϕ∫ Xϕdα+∫ Yϕ ∗dβ, (1.27) see also Section ??for a generalization of this idea to generic costs c(x ,y). By iterating this argument twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex. Condition (1.23) shows that an optimal πis supported on{ (x ,y) ;ϕ (x) +ϕ∗ (y) = ⟨x, y⟩ }which shows that such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads y∈∂ϕ (x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also diﬀerentiable α -almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α -almost everywhere as y=∇ϕ (x), and shows that necessarily π= (Id,∇ϕ)♯α. This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9) and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map). Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should be examined under the light that a convex function is the natural generalization of the notion of increasing functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?]. Note also that this theorem can be extended in many directions. The condition that αhas a density can be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller thand−1 (e .g. hypersurfaces). One can also consider costs of the form c(x ,y) =h (x −y) wherehis a strictly convex function. For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a constant) convex function which solves the following Monge-Amp ˜A ¨re-type equation det( ∂2ϕ (x))ρβ(∇ϕ (x)) =ρα (x) (1.28) where∂2ϕ (x) ∈Rd ×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ (x)) can be understood as a non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the Laplacian ∆ as a linearization since for smooth maps det( ∂2ϕ (x)) = 1 +ε∆ϕ (x) +o(ε). The convexity constraint forces det( ∂2ϕ (x)) ⩾0 and is necessary for this equation to have a solution. Special cases In general, computing OT distances is numerically involved. We review special favorable cases where the resolution of the OT problem is easy. Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on the diagonal and 1 elsewhere, namely when C=1n ×n −In, the OT distance between aandbis equal to the 1-norm of their diﬀerence, L C(a ,b) = ||a −b ||1. One can also easily check that this result extends to discrete and discrete measures in the case where c(x ,y) is 0 ifx=yand 1 when x̸ =y. The OT distance between two discrete measures αand βis equal to their total variation distance. 14 \u0000\u0000 ↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling. Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This corresponds to monotone rearrangements, if xi⩽xi ′are such that Pi,j̸= 0,Pi′ ,j′̸= 0, then necessarily yj⩽yj′. Remark 7 (1-D case – Empirical measures) .HereX =R. Assuming α =1 n∑n i=1 δxiandβ =1 n∑n j=1 δyj, and assuming (without loss of generality) that the points are ordered, i.e .x1 ⩽x2⩽... ⩽xnand y1 ⩽y2⩽... ⩽yn, then one has the simple formula Wp(α,β )p =p∑ i=1 |xi −yi |p, (1.29) i.e .locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of αandβ. That statement is only valid locally, in the sense that the order (and those vector representations) might change whenever some of the values change. That formula is a simple consequence of the more general remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case. Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function ∀x ∈R ,Cα (x )def.= ∫x − ∞dα, (1.30) which is a function Cα :R→ [0 ,1], and its pseudo-inverse C−1 α: [0 ,1] →R∪{−∞} ∀r∈ [0 ,1] ,C −1 α (r) = min x{x ∈R∪{−∞} ;Cα (x) ⩾r}. That function is also called the generalized quantile function of α. For anyp⩾1, one has Wp(α,β )p= ||C −1 α −C −1 β ||p Lp( [0 ,1])= ∫1 0|C −1 α (r) −C −1 β (r) |pdr. (1.31) This means that through the map α↦ →C −1 α, the Wasserstein distance is isometric to a linear space equipped with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally in§??. Forp= 1, one even has the simpler formula W1(α,β) = ||Cα −Cβ ||L1 (R)=∫ R|Cα (x) −Cβ (x) |dx (1.32) =∫ R⏐⏐⏐⏐ ∫x − ∞d(α−β) ⏐⏐⏐⏐dx. (1.33) 15 µ ν (tT+ (1 −t )Id)♯µ 0 0.5 10.5Cµ Cν 0 0.5 100.51 Cµ-1 Cν-1 0 0.5 100.51 T T-1 0 0.5 100.51 (Cα ,Cβ) (C −1 α ,C −1 β) ( T,T −1) (1 −t )C −1 α +tC −1 β Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant function as detailed in (1.34). which shows that W1is a norm (see§ ??for the generalization to arbitrary dimensions). An optimal Monge mapTsuch thatT♯α= βis then deﬁned by T=C −1 β ◦Cα. (1.34) Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of optimal transport in 1-D, we refer the reader to [ ?, Chapter 2]. Remark 9 (Distance between Gaussians) .Ifα =N (mα,Σα) andβ =N (mβ,Σβ) are two Gaussians in Rd, then one can show that the following map T:x↦ →mβ +A (x −mα), (1.35) where A=Σ −1 2α( Σ1 2αΣβ Σ1 2α )1 2Σ −1 2α =AT, is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed since ρβ (T (x)) = det(2πΣβ) −1 2exp(− ⟨T (x) −mβ,Σ −1 β (T (x) −mβ)⟩) = det(2πΣβ) −1 2exp(− ⟨x −mα, ATΣ −1 βA (x −mα)⟩) = det(2πΣβ) −1 2exp(− ⟨x −mα,Σ −1 α (x −mα)⟩), and sinceTis a linear map we have that |detT′ (x)|= detA= (detΣβ detΣα )1 2 and we therefore recover ρα= |detT′|ρ βmeaningT♯α=β. Notice now that Tis the gradient of the convex functionψ :x↦ →1 2⟨x −mα, A(x −mα)⟩+ ⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??) thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ?? 16 -4 -2 0 2 4 6-3-2-101234 ρβρ αFigure 1.10: Two Gaussians ρ αandρβ, represented using the contour plots of their densities, with respective mean and variance matrices mα= ( −2 ,0),Σα =1 2( 1−1 2; −1 21) andmβ= (3 ,1),Σβ=( 2,1 2;1 2,1) . The arrows originate at random points xtaken on the plane and end at the corresponding mappings of those pointsT(x) =mβ +A (x −mα). \u0000m Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ (x )def.= 1√ 2πse− (x −m )2 2s2the Gaussian density, it thus shows the interpolation G(1 −t )m0 +tm1, (1 −t) σ0 +t σ1. With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport cost of that map is W2 2(α,β) = ||mα −mβ ||2 +B(Σα,Σβ )2 (1.36) whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]), B(Σα,Σβ )2def.= tr( Σα+Σβ −2( Σ1 /2 αΣβ Σ1 /2 α )1 /2) , (1.37) where Σ1 /2is the matrix square root. One can show that Bis a distance on covariance matrices, and that B2is convex with respect to both its arguments. In the case where Σα= diag(ri )iandΣβ= diag(si )iare diagonals, the Bures metric is the Hellinger distance B(Σα,Σβ) =|| √r− √s ||2. For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√ Σ), as illustrated in Figure 1.11. For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?]. 1.5 Sinkhorn This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to the original problem. This regularization has several important advantages, but a few stand out particularly: The minimization of the regularized problen can be solved using a simple alternate minimization scheme; that scheme translates into iterations that are simple matrix products, making them particularly suited to execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights and positions of the Diracs. 17 c\"P \"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε= argminP∈ Σ3 ⟨C ,P⟩− εH (P) for a varying ε. Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as H(P )def.=−∑ i,jPi ,j (log (Pi ,j) −1), (1.38) with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis 0 or negative. The function His 1-strongly concave, because its hessian is ∂2H (P) = −diag (1 /Pi ,j) and Pi,j ⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function to obtain approximate solutions to the original transport problem (1.11): Lε C(a ,b )def.= min P∈U (a ,b) ⟨P ,C⟩− εH (P). (1.39) Since the objective is a ε -strongly convex function, problem 1.39 has a unique optimal solution. The idea to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a more “blurred” traﬃc prediction. Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized problem towards an optimal solution of the original linear program has been studied by [ ?]. Proposition 6 (Convergence with ε) .The unique solution Pεof (1.39) converges to the optimal solution with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely Pεε →0− →argmin P{ −H (P) ;P ∈U (a ,b), ⟨P ,C⟩= LC(a ,b)} (1.40) so that in particular Lε C(a ,b)ε →0− →LC (a ,b). One has Pεε→∞− →abT= (aibj )i ,j. (1.41) Proof. We consider a sequence ( εℓ) ℓsuch thatεℓ →0 andεℓ >0. We denote Pℓthe solution of (1.39) for ε=εℓ. Since U(a ,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity) such that Pℓ →P⋆. Since U(a ,b) is closed, P⋆ ∈U (a ,b). We consider any Psuch that⟨C ,P⟩= LC(a ,b). By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has 0⩽ ⟨C ,Pℓ⟩− ⟨C ,P⟩⩽εℓ (H (Pℓ) −H (P)). (1.42) 18 ⇡\"↵\u0000 \"\u0000 ↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6. Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number n=mof points (only entries of the optimal ( Pi,j )i ,jabove a small threshold are displayed as segments betweenxiandyj). Since His continuous, taking the limit ℓ→+ ∞in this expression shows that ⟨C ,P⋆⟩= ⟨C ,P ⟩so that P⋆is a feasible point of (1.40). Furthermore, dividing by ε ℓin (1.42) and taking the limit shows that H(P) ⩽H (P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆ 0to this program is unique by strict convexity of −H, one has P⋆ =P⋆ 0, and the whole sequence is converging. Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is performed in [ ?], including a ﬁrst order expansion in ε (resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13 shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to faster statistical convergence (as exposed in §??). Deﬁning the Kullback-Leibler divergence between couplings as KL(P |K )def.=∑ i,jPi ,jlog (Pi ,j Ki,j) −Pi ,j +Ki ,j, (1.43) the unique solution Pεof (1.39) is a projection onto U(a ,b) of the Gibbs kernel associated to the cost matrix Cas Ki,jdef. =e −Ci ,j ε Indeed one has that using the deﬁnition above Pε= ProjKL U(a ,b) (K )def.= argmin P∈U (a ,b )KL (P |K). (1.44) Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy by the relative entropy with respect to the product measure d α ⊗dβ (x ,y )def.= dα (x )dβ (y), and propose a regularized counterpart to (1.14) using Lε c(α,β )def.= min π ∈U(α,β)∫ X×Yc (x ,y )dπ (x ,y) + εKL(π|α⊗β) (1.45) where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43) KL(π|ξ )def.=∫ X×Ylog (dπ dξ (x ,y)) dπ (x ,y)+ ∫ X×Y (dξ (x ,y) −dπ (x ,y)), (1.46) 19 and by convention KL( π|ξ) = + ∞if πdoes not have a densitydπ dξwith respect to ξ. It is important to realize that the reference measure α⊗ βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β) plays no speciﬁc role, only its support matters. Formula (1.45) can be re-factored as a projection problem min π ∈U(α,β )KL(π |K) (1.47) whereKis the Gibbs distributions d K(x ,y )def. =e −c (x ,y) εdµ (x )dν (y). This problem is often referred to as the “static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?]. Asε →0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§?? details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting the points of two measures. Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form, which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in the sense that a coupling PinU(a ,b) hasnmvariables but n+mconstraints. Proposition 7. The solution to (1.39) is unique and has the form ∀ (i ,j) ∈JnK ×JmK ,Pi ,j =uiKi ,jvj (1.48) for two (unknown) scaling variable (u ,v) ∈Rn + ×Rm +. Proof. Introducing two dual variables f∈Rn ,g ∈Rmfor each marginal constraint, the Lagrangian of (1.39) reads E(P ,f ,g) = ⟨P ,C⟩− εH (P)− ⟨f ,P1m −a⟩− ⟨g ,PT1n −b⟩. Considering ﬁrst order conditions, we have ∂E (P ,f ,g) ∂Pi ,j =Ci ,j− εlog (Pi ,j) −fi −gj. which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j =efi/ εe −Ci ,j/ εegj/ε which can be rewritten in the form provided in the proposition using non-negative vectors uandv. The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in matrix form as P= diag( u)Kdiag (v) .u ,vmust therefore satisfy the following non-linear equations which correspond to the mass conservation constraints inherent to U(a ,b), diag(u )Kdiag (v )1m =a ,and diag( v)K ⊤diag (u )1n =b, (1.49) These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u) times Kvis u⊙ (Kv) =aand v⊙ (KTu) =b (1.50) where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm: u(ℓ +1 )def. =a Kv(ℓ )and v(ℓ +1 )def. =b KTu(ℓ +1), (1.51) initialized with an arbitrary positive vector v(0) =1m. The division operator used above between two vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent 20 `⇡(`)\" 1000 2000 3000 4000 5000-2-1 .5-1-0.50 `Figure 1.14: Left: evolution of the coupling πℓ ε= diag( U(ℓ ))Kdiag (V(ℓ)) computed at iteration ℓof Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured in term of marginal constraint violation log( ||πℓ ε1m −b ||1). solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then so doλu ,v/ λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in the same optimal coupling diag( u)Kdiag (v). Figure 1.14, top row, shows the evolution of the coupling diag(U(ℓ ))Kdiag (V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal. Remark 11 (Relation with iterative projections) .Denoting C1 adef.= {P ;P1m =a }andC2 bdef.={ P;PT1m =b} the rows and columns constraints, one has U(a ,b) =C1 a∩C2 b. One can use Bregman iterative projections [ ?] P(ℓ +1) def.= ProjKL C1a (P(ℓ)) and P(ℓ +2) def.= ProjKL C2 b(P(ℓ +1)). (1.52) Since the setsC1 aandC2 bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?]. These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning P(2ℓ )def.= diag( u(ℓ ))Kdiag (v(ℓ)), one has P(2ℓ +1) def.= diag( u(ℓ +1 ))Kdiag (v(ℓ)) and P(2ℓ +2) def.= diag( u(ℓ +1 ))Kdiag (v(ℓ +1)) In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??). Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is greatly simpliﬁed using Hilbert projective metric on Rn +,∗ (positive vectors), deﬁned as ∀ (u ,u′)∈ (Rn +,∗ )2, dH(u ,u′ )def.= log max i,i ′uiu′ i′ ui′u′ i. This can be shows to be a distance on the projective cone Rn +,∗/∼, where u∼u ′means that∃s >0 ,u =su′ (the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the triangular inequality and dH(u ,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original distance on bounded open convex sets [ ?]. The projective cone Rn +,∗/ ∼is a complete metric space for this distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the cone of positive vectors. 21 Theorem 2. Let K∈Rn ×m +,∗, then for (v ,v′)∈ (Rm +,∗ )2 dH(Kv ,Kv′)⩽λ (K )dH (v ,v′ )where  λ (K )def.=√ η (K) −1√ η (K) +1 <1 η (K )def.= max i,j ,k, ℓKi ,kKj,ℓ Kj,kKi,ℓ. Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to show the linear convergence of Sinkhorn’s iterations. Theorem 3. One has (u(ℓ) ,v(ℓ))→ (u⋆ ,v⋆ )and dH(u(ℓ) ,u⋆) =O(λ (K )2ℓ), dH(v(ℓ) ,v⋆) =O(λ (K )2ℓ). (1.53) One also has dH(u(ℓ) ,u⋆) ⩽dH (P(ℓ )1m ,a) 1−λ (K) dH(v(ℓ) ,v⋆) ⩽dH (P(ℓ), ⊤1n ,b) 1−λ (K) (1.54) where we denoted P(ℓ )def.= diag( u(ℓ ))Kdiag (v(ℓ)). Lastly, one has ∥log (P(ℓ)) −log (P⋆)∥∞ ⩽dH (u(ℓ) ,u⋆) +dH (v(ℓ) ,v⋆) (1.55) where P⋆is the unique solution of (1.39) . Proof. One notice that for any ( v,v′)∈ (Rm +,∗ )2, one has dH(v ,v′) =dH (v /v′ ,1m) =dH (1m /v ,1m /v′). This shows that dH(u(ℓ +1) ,u⋆) =dH (a Kv(ℓ) ,a Kv⋆) =dH (Kv(ℓ) ,Kv⋆)⩽λ (K )dH (v(ℓ) ,v⋆). where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality dH(u(ℓ) ,u⋆) ⩽dH (u(ℓ +1) ,u(ℓ)) +dH (u(ℓ +1) ,u⋆) ⩽dH (a Kv(ℓ) ,u(ℓ)) +λ (K )dH (u(ℓ) ,u⋆) =dH( a,u(ℓ)⊙ (Kv(ℓ))) +λ (K )dH (u(ℓ) ,u⋆), which gives the ﬁrst part of (1.54) since u(ℓ)⊙ (Kv(ℓ)) =P(ℓ )1m (the second one being similar). The proof of (1.55) follows from [ ?, Lemma 3] The bound (1.54) shows that some error measures on the marginal constraints violation, for instance ∥P(ℓ )1m −a ∥1and ∥P(ℓ )T1n −b ∥1, are useful stopping criteria to monitor the convergence. Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate degrades as ε →0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent of the scaled coupling matrix. 22 Regularized Dual and Log-domain Computations The following proposition details the dual problem associated to (1.39). Proposition 8. One has Lε C(a ,b) = max f∈Rn ,g ∈Rm ⟨f ,a⟩+ ⟨g ,b⟩−ε ⟨ef/ε ,Keg/ε⟩. (1.56) The optimal (f ,g )are linked to scalings (u ,v )appearing in (1.48) through (u ,v) = (ef/ε ,eg/ε). (1.57) Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P and dual multipliers fandgfor the marginal constraints as Pi,j =efi/ εe −Ci ,j/ εegj/ε. Substituting in the LagrangianE(P ,f ,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange dual function equals f,g↦→ ⟨ef/ε, (K ⊙C )eg/ε⟩− εH (diag (ef/ε )Kdiag (eg/ε)). (1.58) The entropy of Pscaled byε, namelyε ⟨P ,logP −1n ×m ⟩can be stated explicitly as a function of f,g ,C ⟨diag (ef/ε )Kdiag (eg/ε) ,f1mT +1ngT −C− ε1n ×m⟩ =− ⟨ef/ε, (K ⊙C )eg/ε⟩+ ⟨f ,a⟩+ ⟨g ,b⟩−ε ⟨ef/ε ,Keg/ε⟩ therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times are those displayed in (1.56). Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual problem (1.56) reads sup f,g ∈C (X) ×C (Y)∫ Xf(x )dα (x) +∫ Yg(x )dβ (x)−ε∫ X×Ye −c (x ,y) +f (x) +g (y) ε dα (x )dβ (y) This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which is retrieved in the limit ε →0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the convergence of Sinkhorn iterations, see [ ?] for more details. Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one can easily notice that, writing Q(f ,g) for the objective of (1.56) that ∇ |fQ (f ,g) =a −ef/ε⊙( Keg/ε) , (1.59) ∇ |gQ (f ,g) =b −eg/ε⊙( KTef/ε) . (1.60) Block coordinate ascent can therefore be implemented in a closed form by applying successively the following updates, starting from any arbitrary g(0), forl⩾0, f(ℓ +1)= εloga− εlog( Keg(ℓ)/ε) , (1.61) g(ℓ +1)= εlogb− εlog( KTef(ℓ +1)/ε) . (1.62) Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal- dual relations highlighted in (1.57). Indeed, we recover that at any iteration (f(ℓ) ,g(ℓ)) =ε (log (u(ℓ)) ,log (v(ℓ))). 23 Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation, using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its coordinates, namely minεz=− εlog∑ ie−zi/ε. Note that min ε (z) converges to min zfor any vector zasε →0. Indeed, min εcan be interpreted as a diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be rewritten (f(ℓ +1 ))i= minε (Cij −g(ℓ) j)j+ εlogai, (1.63) (g(ℓ +1 ))j= minε (Cij −f(ℓ) i)i+ εlogbj. (1.64) Here the term min ε (Cij −g(ℓ) j)jdenotes the soft-minimum of all values of the j-th column of matrix (C −1n (g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn ×m, we deﬁne Minrow ε (A )def.=( minε (Ai ,j )j) i∈Rn, Mincol ε (A )def.=( minε (Ai ,j )i) j∈Rm. Note that these operations are equivalent to the entropic c-transform introduced in §?? (see in particu- lar (??)). Using these notations, Sinkhorn’s iterates read f(ℓ +1)= Minrow ε (C −1ng(ℓ )T) + εloga, (1.65) g(ℓ +1)= Mincol ε (C −f(ℓ )1mT) + εlogb. (1.66) Note that as ε →0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0, because alternate minimization does not converge for constrained problems (which is the case for the un- regularized dual (1.17)). Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera- tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values ofε. Writing z = min z, that trick suggests to evaluate min εzas minεz= z− εlog∑ ie− (zi −z)/ε. (1.67) Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the previously computed scalings. This leads to the following stabilized iteration f(ℓ +1)= Minrow ε (S (f(ℓ) ,g(ℓ))) −f(ℓ)+ εlog (a) (1.68) g(ℓ +1)= Mincol ε (S (f(ℓ +1) ,g(ℓ))) −g(ℓ)+ εlog (b), (1.69) where we deﬁned S(f ,g) =( Ci,j −fi −gj) i,j. In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for arbitraryε >0, because the quantity S(f ,g) stays bounded during the iterations. The downside is that it requiresnmcomputations of exp at each step. Computing a Minrow εor Mincol εis typically substantially slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously. In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?]. 24 1.6 Extensions Wasserstein Barycenters. Given input histogram {bs }S s=1, wherebs∈ Σns, and weights λ∈ ΣS, a Wasserstein barycenter is computed by minimizing min a∈ ΣnS∑ s=1 λsLCs (a ,bs) (1.70) where the cost matrices Cs∈Rn ×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the barycenters are deﬁned on the same grid, ns=n ,Cs =C =Dpis set to be a distance matrix, so that one solves min a∈ ΣnS∑ s=1 λsWp p(a ,bs). This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved in particular uniqueness of the barycenter for c(x ,y) = ||x −y ||2overX =Rd, if one of the input measure has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the one guaranteeing the existence of a Monge map, see Remark ??). The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S couplings ( Ps)sbetween each input and the barycenter itself min a∈ Σn, (Ps ∈Rn ×ns )s {S∑ s=1 λs ⟨Ps ,Cs⟩; ∀s ,P⊤ s1ns =a ,P⊤ s1n =bs} . Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?]. Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs )sde ﬁned on some space X, the barycenter problem becomes min α ∈M1 + (X )S∑ s=1 λsLc(α, βs). (1.71) In the case where X=Rdandc (x ,y) = ||x −y ||2, [?] shows that if one of the input measures has a density, then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing barycenters of points ( xs)S s=1 ∈XSto arbitrary measures. Indeed, if βs= δxsis a single Dirac mass, then a solution to (1.71) is δx ⋆wherex ⋆is a Fr´ echet mean solving ( ??). Note that for c(x ,y) = ||x −y ||2, the mean of the barycenter α ⋆is necessarily the barycenter of the mean, i.e. ∫ Xxdα⋆ (x) =∑ sλs∫ Xxdαs (x), and the support of α ⋆is located in the convex hull of the supports of the ( αs )s. The consistency of the approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to re-cast (1.71) as a multi-marginal OT problem, see Remark ??. One can use entropic smoothing and approximate the solution of (1.70) using min a∈ ΣnS∑ s=1 λsLε Cs(a ,bs) (1.72) for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is 25 useful to integrate additional regularizations on the barycenter (e .g. to impose some smoothness). A simple but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem min (Ps )s{∑ sλsKL (Ps |Ks) ; ∀s ,PsT1m =bs ,P111=... =PS1S} (1.73) where we denoted Ksdef. =e −Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all the couplings Ps∈Rn ×nsasa =P111=... =PS1S. As detailed in [ ?], one can generalize Sinkhorn to this problem, which also corresponds to iterative projection. This can also be seen as a special case of the generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling form as Ps= diag( us)Kdiag (vs), (1.74) and the scalings are sequentially updated as ∀s ∈J1 ,SK ,v(ℓ +1) sdef. =bs KT su(ℓ) s, (1.75) ∀s ∈J1 ,SK ,u(ℓ +1) sdef. =a(ℓ +1) Ksv(ℓ +1) s, (1.76) where a(ℓ +1 )def.=∏ s(Ksv(ℓ +1) s) λs. (1.77) An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual problem, which detailed in the following proposition. Proposition 9. The optimal (us ,vs )appearing in (1.74) can be written as (us ,vs) = (efs/ε ,egs/ε )where (fs ,gs )sare the solutions of the following program (whose value matches the one of (1.72) ) max (fs ,gs )s{∑ sλs( ⟨gs ,bs⟩−ε ⟨Ksegs/ε, efs/ε⟩) ;∑ sλsfs= 0} . (1.78) Proof. Introducing Lagrange multipliers in (1.73) leads to min (Ps )s ,amax (fs ,gs )s∑ sλs( εKL (Ps |Ks) + ⟨a −Ps1m ,fs⟩ + ⟨bs −PsT1m ,gs⟩) . Strong duality holds, so that one can exchange the min and the max, and gets max (fs ,gs )s∑ sλs( ⟨gs ,bs⟩+ min PsεKL (Ps |Ks)− ⟨Ps ,fs ⊕gs⟩) + min a⟨∑ sλsfs ,a⟩. The explicit minimization on agives the constraint∑ sλsfs= 0 together with max (fs ,gs )s∑ sλs ⟨gs ,bs⟩− εKL∗ (fs ⊕gs ε |Ks) where KL∗(· |Ks) is the Legendre transform ( ??) of the function KL∗(· |Ks). This Legendre transform reads KL∗ (U |K) =∑ i,jKi ,j (eUi ,j −1), (1.79) 26 Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights ( λs )sare bilinear with respect to the four corners of the square. Shapes are represented as measures that are uniform within the boundaries of the shape and null outside. which shows the desired formula. To show (1.79), since this function is separable, one needs to compute ∀ (u ,k) ∈R2 + ,KL∗ (u |k )def.= max rur− (rlog (r /k) −r +k) whose optimality condition reads u= log(r /k), i.e .r =keu, hence the result. Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads to the expression (1.76). Figures ??and ??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure, the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer to [?] for more details and other applications to computer graphics and imaging sciences. Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈ Θ }where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ ﬁdelity” term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a suitable parameter θis obtained by minimizing directly min θ∈ ΘE(θ )def. =Lc(αθ,β). (1.80) Of course, one can consider more complicated problems: for instance, the barycenter problem described in§ ??consists in a sum of such terms. However, most of these more advanced problems can be usually solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives, or using automatic diﬀerentiation. The Wasserstein distance between two histograms or two densities is convex with respect to these inputs, as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ = Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ= ∑K i=1 θi αi is a convex combination of known atoms α1,..., αKin ΣN, Problem (1.80) remains convex (the ﬁrst case corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general not convex. 27 g✓XZ ⇣xz\u0000↵ ✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81. A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given some discrete samples ( xi)n i=1 ⊂X from some unknown distribution, the goal is to ﬁt a parametric model θ↦→αθ ∈M (X) to the observed empirical input measure β min θ∈ ΘL(αθ,β) where β =1 n∑ iδxi, (1.81) whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig- ure 1.16). In the case where α θas a densify ρ θdef.=ρα θwith respect to the Lebesgue measure (or any other ﬁxed reference measure), the maximum likelihood estimator (MLE) is obtained by solving min θLMLE(αθ,β )def.=−∑ ilog(ρθ (xi)). This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i .d. samples of some ¯β, then LMLE(α,β )n→+∞−→ KL(α|¯β) This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]). However, it fails to work when estimating singular distributions, typically when the α θdoes not has a density (so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β (so that the α θshould share the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that in several cases of practical interest, the density ρ θis inaccessible (or too hard to compute). A typical setup where both problems (singular and unknown densities) occur is for so-called generative models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ ∈M (Z) αθ =hθ,♯ ζwherehθ :Z →X where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so that the support of α θis localized along a low-dimensional “manifold” and the resulting density is highly singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density is usually intractable, while generating i.i .d. samples from α θis achieved by computing xi=hθ (zi) where (zi )iare i.i .d. samples from ζ. In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional LMLE, which needs to be written in dual form as L(α,β )def.= max (f ,g) ∈C (X )2{∫ Xf(x )dα (x) +∫ Xg(x )dβ (x) ; (f ,g) ∈R} . (1.82) Dual norms exposed in § ??correspond to imposing R={ (f, −f) ;f ∈B}, while optimal transport (1.21) setsR=R (c) as deﬁned in (1.22). 28 For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with respect toθis much more involved, and is typically highly non-convex. The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE), was initially introduced in [ ?], see also [ ?]. Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption, namely that one has at its disposal two matrices D∈Rn ×nandD′ ∈Rm ×mthat represent some relationship between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power of) distance matrices. The Gromov-Wasserstein problem reads GW(( a,D), (b ,D′ ))2def.= min P∈U (a ,b )ED ,D′ (P )def.=∑ i,j ,i′ ,j′ |Di ,i′ −D′ j,j′ |2Pi ,jPi′ ,j′. (1.83) This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?] for a particular cost. One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83)) up to isometries preserving the measures. This distance was introduced and studied in details by Memoli in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?]. Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between metric measure spaces ( X,dX, αX) and (Y ,dY, αY) where (dX ,dY) are distances and ( αX, αY) are measures on their respective spaces. One deﬁnes GW(( αX ,dX),( αY ,dY ))2def.= min π ∈U( αX, αY)∫ X2 ×Y2 |dX (x ,x′) −dY (y ,y′) |2dπ (x ,y )dπ (x′ ,y′). (1.84) GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX ,dX) and ( αY ,dY) are isometric if there exists ϕ :X →Y such thatϕ♯ αX= αYanddY(ϕ (x),ϕ (x′)) =dX (x ,x′). Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0 ,dX0, α0) and (X1 ,dX1, α1) can be chosen to be t∈ [0 ,1]↦→ (X0 ×X 1,dt,π⋆) whereπ ⋆is a solution of (1.84) and for all ((x0 ,x1), (x′ 0,x′ 1))∈ (X0 ×X 1)2, dt((x0 ,x1), (x′ 0,x′ 1))def.= (1 −t )dX0 (x0 ,x′ 0) +tdX1 (x1 ,x′ 1). This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product spaceX0 ×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85) detailed below. To approximate the computation of GW, and to help convergence of minimization schemes to better minima, one can consider the entropic regularized variant min P∈U (a ,b )ED ,D′ (P)− εH (P). (1.85) 29 Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn iterations (1.86). Extracted from [ ?]. As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations of the objective function lead to consider the succession of updates P(ℓ +1) def.= min P∈U (a ,b) ⟨P ,C(ℓ)⟩− εH (P) where (1.86) C(ℓ )def.= ∇ED ,D′ (P(ℓ)) = −D ′TP(ℓ )D, which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to compute soft maps between domains. 30 Bibliography [1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT- LAB. SIAM, 2014. [2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝ in Machine Learning , 3(1) :1 –122, 2011. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004. [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2) :219 –266, 2004. [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM Multiscale Modeling and Simulation , 5:861 –899, 2005. [6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. , 20:89 –97, 2004. [7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro- duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse recovery , 9(263-340) :227, 2010. [8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta Numerica , 25:161 –319, 2016. [9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing , 20(1) :33 –61, 1999. [10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982. [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM Multiscale Modeling and Simulation , 4(4), 2005. [12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413 –1541, 2004. [13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425 –455, Dec 1994. [14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume 375. Springer Science & Business Media, 1996. [15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans. Image Proc. , 12(8) :906 –916, 2003. [16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1. Birkh¨ auser Basel, 2013. 31 [17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008. [18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia- tional problems. Commun. on Pure and Appl. Math. , 42:577 –685, 1989. [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization , 1(3) :127 –239, 2014. [20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004. [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11) :1338 –1351, November 2003. [22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4) :259 –268, 1992. [23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich. Variational methods in imaging . Springer, 2009. [24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal , 27(3) :379 –423, 1948. [25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and related geometric multiscale analysis . Cambridge university press, 2015. 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"PythonCodeTextSplitter\")\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"PythonCodeTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "GBO7EgWi0Xp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5bca42-490b-4815-e712-c513e5543fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chunk 2: www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "Chunk 3: belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "Chunk 4: α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "Chunk 5: x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Chunk 6: Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "Chunk 7: “objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "Chunk 8: to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "Chunk 9: equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Chunk 10: it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "Chunk 11: ∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "Chunk 12: dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "Chunk 13: the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "Chunk 14: Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Chunk 15: Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "Chunk 16: dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "Chunk 17: +(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Chunk 18: 3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "Chunk 19: i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "Chunk 20: 1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Chunk 21: Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "Chunk 22: positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "Chunk 23: mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "Chunk 24: α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Chunk 25: β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Chunk 26: +(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "Chunk 27: measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "Chunk 28: onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "Chunk 29: new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Chunk 30: Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "Chunk 31: with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "Chunk 32: ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "Chunk 33: |det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Chunk 34: Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "Chunk 35: the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "Chunk 36: map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "Chunk 37: ∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "Chunk 38: the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "Chunk 39: registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Chunk 40: Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "Chunk 41: (often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "Chunk 42: another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "Chunk 43: yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "Chunk 44: convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "Chunk 45: bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "Chunk 46: min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "Chunk 47: that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "Chunk 48: 10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "Chunk 49: algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "Chunk 50: 5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "Chunk 51: either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "Chunk 52: the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "Chunk 53: disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Chunk 54: 4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Chunk 55: Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "Chunk 56: corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "Chunk 57: For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "Chunk 58: push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "Chunk 59: must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "Chunk 60: parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Chunk 61: min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "Chunk 62: indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "Chunk 63: constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Chunk 64: Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "Chunk 65: to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "Chunk 66: target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Chunk 67: 6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "Chunk 68: min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "Chunk 69: operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Chunk 70: Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "Chunk 71: be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "Chunk 72: uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "Chunk 73: also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "Chunk 74: (see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "Chunk 75: set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "Chunk 76: constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "Chunk 77: ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "Chunk 78: to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "Chunk 79: dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "Chunk 80: be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "Chunk 81: commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "Chunk 82: +, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "Chunk 83: of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "Chunk 84: U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "Chunk 85: P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Chunk 86: Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "Chunk 87: asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "Chunk 88: Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "Chunk 89: P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "Chunk 90: not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "Chunk 91: indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "Chunk 92: corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Chunk 93: Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Chunk 94: associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "Chunk 95: ∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "Chunk 96: ⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "Chunk 97: min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "Chunk 98: polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "Chunk 99: min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "Chunk 100: one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "Chunk 101: measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "Chunk 102: problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "Chunk 103: solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Chunk 104: Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "Chunk 105: permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "Chunk 106: minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Chunk 107: ⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "Chunk 108: scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "Chunk 109: coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "Chunk 110: couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Chunk 111: Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "Chunk 112: space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "Chunk 113: π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "Chunk 114: U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Chunk 115: Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "Chunk 116: measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "Chunk 117: α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "Chunk 118: π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "Chunk 119: and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "Chunk 120: involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "Chunk 121: weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "Chunk 122: 9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "Chunk 123: is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Chunk 124: to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "Chunk 125: and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "Chunk 126: understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "Chunk 127: measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "Chunk 128: is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "Chunk 129: to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "Chunk 130: i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "Chunk 131: Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Chunk 132: Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Chunk 133: ∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Chunk 134: Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "Chunk 135: elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "Chunk 136: a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "Chunk 137: To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "Chunk 138: gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "Chunk 139: the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "Chunk 140: and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "Chunk 141: S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "Chunk 142: because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "Chunk 143: ⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "Chunk 144: ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "Chunk 145: 1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "Chunk 146: inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Chunk 147: Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Chunk 148: (ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Chunk 149: Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Chunk 150: ∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "Chunk 151: between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "Chunk 152: weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "Chunk 153: spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "Chunk 154: are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "Chunk 155: with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "Chunk 156: p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "Chunk 157: Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Chunk 158: 11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "Chunk 159: the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "Chunk 160: convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Chunk 161: Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "Chunk 162: e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "Chunk 163: by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Chunk 164: Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "Chunk 165: as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "Chunk 166: pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "Chunk 167: exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "Chunk 168: the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "Chunk 169: which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "Chunk 170: OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "Chunk 171: naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "Chunk 172: following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "Chunk 173: Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Chunk 174: Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "Chunk 175: grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "Chunk 176: is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "Chunk 177: min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "Chunk 178: program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "Chunk 179: −∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "Chunk 180: transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "Chunk 181: are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "Chunk 182: against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "Chunk 183: Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Chunk 184: Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "Chunk 185: (fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Chunk 186: Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "Chunk 187: trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "Chunk 188: machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Chunk 189: Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Chunk 190: Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Chunk 191: are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "Chunk 192: measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Chunk 193: Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "Chunk 194: ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Chunk 195: 2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "Chunk 196: ||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            "Chunk 197: (ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "Chunk 198: 2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "Chunk 199: also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "Chunk 200: minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "Chunk 201: min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "Chunk 202: twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Chunk 203: Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "Chunk 204: such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "Chunk 205: y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "Chunk 206: diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "Chunk 207: This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "Chunk 208: and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "Chunk 209: of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "Chunk 210: problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Chunk 211: Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "Chunk 212: be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "Chunk 213: functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "Chunk 214: functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Chunk 215: Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "Chunk 216: be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "Chunk 217: thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "Chunk 218: strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "Chunk 219: constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "Chunk 220: det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "Chunk 221: non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "Chunk 222: det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Chunk 223: Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Chunk 224: Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "Chunk 225: the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "Chunk 226: discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "Chunk 227: 14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Chunk 228: Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "Chunk 229: yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "Chunk 230: y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "Chunk 231: αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "Chunk 232: might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "Chunk 233: remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "Chunk 234: with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "Chunk 235: discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "Chunk 236: circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "Chunk 237: of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "Chunk 238: ∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "Chunk 239: ∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "Chunk 240: α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "Chunk 241: with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "Chunk 242: metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "Chunk 243: geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "Chunk 244: W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "Chunk 245: T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "Chunk 246: function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Chunk 247: T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "Chunk 248: interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Chunk 249: Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "Chunk 250: where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "Chunk 251: since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "Chunk 252: α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "Chunk 253: functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "Chunk 254: thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "Chunk 255: 16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "Chunk 256: 2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Chunk 257: pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "Chunk 258: 1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "Chunk 259: With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "Chunk 260: W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "Chunk 261: Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "Chunk 262: B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "Chunk 263: B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "Chunk 264: Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "Chunk 265: 1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "Chunk 266: of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "Chunk 267: the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "Chunk 268: The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "Chunk 269: that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "Chunk 270: execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "Chunk 271: and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Chunk 272: argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "Chunk 273: H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "Chunk 274: 0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Chunk 275: Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "Chunk 276: Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "Chunk 277: to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "Chunk 278: transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "Chunk 279: solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "Chunk 280: to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "Chunk 281: that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Chunk 282: more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "Chunk 283: can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "Chunk 284: from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "Chunk 285: triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "Chunk 286: problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "Chunk 287: with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "Chunk 288: so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "Chunk 289: ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "Chunk 290: By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "Chunk 291: 0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Chunk 292: Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "Chunk 293: n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Chunk 294: betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "Chunk 295: P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "Chunk 296: 0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Chunk 297: 0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "Chunk 298: transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "Chunk 299: coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "Chunk 300: two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "Chunk 301: performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "Chunk 302: shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "Chunk 303: becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "Chunk 304: turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Chunk 305: Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "Chunk 306: i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Chunk 307: Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Chunk 308: U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "Chunk 309: by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "Chunk 310: Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "Chunk 311: KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "Chunk 312: dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Chunk 313: plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "Chunk 314: εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Chunk 315: Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "Chunk 316: details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Chunk 317: the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "Chunk 318: which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Chunk 319: Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Chunk 320: +×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Chunk 321: reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "Chunk 322: ∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "Chunk 323: which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "Chunk 324: The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "Chunk 325: matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "Chunk 326: diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "Chunk 327: times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "Chunk 328: community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "Chunk 329: these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Chunk 330: Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "Chunk 331: Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "Chunk 332: vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "Chunk 333: 20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Chunk 334: Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "Chunk 335: ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "Chunk 336: so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "Chunk 337: a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "Chunk 338: the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "Chunk 339: diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Chunk 340: Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "Chunk 341: a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "Chunk 342: C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "Chunk 343: These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "Chunk 344: and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "Chunk 345: multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Chunk 346: Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "Chunk 347: +,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "Chunk 348: +,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "Chunk 349: triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "Chunk 350: +,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "Chunk 351: theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "Chunk 352: proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "Chunk 353: +,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Chunk 354: η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "Chunk 355: show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "Chunk 356: One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "Chunk 357: ∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "Chunk 358: dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "Chunk 359: Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "Chunk 360: ⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "Chunk 361: of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "Chunk 362: ∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "Chunk 363: degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Chunk 364: Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "Chunk 365: convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Chunk 366: of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "Chunk 367: Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Chunk 368: (u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "Chunk 369: and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "Chunk 370: LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "Chunk 371: The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "Chunk 372: =−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Chunk 373: are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Chunk 374: sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "Chunk 375: is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "Chunk 376: potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "Chunk 377: usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Chunk 378: Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "Chunk 379: unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "Chunk 380: update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Chunk 381: ∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "Chunk 382: updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Chunk 383: , (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "Chunk 384: dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Chunk 385: (f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "Chunk 386: using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Chunk 387: coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "Chunk 388: diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "Chunk 389: j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "Chunk 390: (C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "Chunk 391: now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Chunk 392: i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "Chunk 393: lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Chunk 394: g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "Chunk 395: because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Chunk 396: regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "Chunk 397: tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "Chunk 398: minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "Chunk 399: previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "Chunk 400: where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "Chunk 401: arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "Chunk 402: εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "Chunk 403: therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "Chunk 404: In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "Chunk 405: 24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "Chunk 406: min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "Chunk 407: barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "Chunk 408: solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "Chunk 409: in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "Chunk 410: has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "Chunk 411: The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "Chunk 412: min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "Chunk 413: can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "Chunk 414: the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "Chunk 415: then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "Chunk 416: barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "Chunk 417: solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Chunk 418: ∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "Chunk 419: approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "Chunk 420: using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "Chunk 421: One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "Chunk 422: min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "Chunk 423: descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "Chunk 424: 25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "Chunk 425: but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "Chunk 426: sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "Chunk 427: the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "Chunk 428: this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "Chunk 429: generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "Chunk 430: and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "Chunk 431: where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "Chunk 432: problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "Chunk 433: (fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Chunk 434: ⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Chunk 435: sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "Chunk 436: ⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "Chunk 437: max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Chunk 438: KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "Chunk 439: (λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "Chunk 440: which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "Chunk 441: ∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Chunk 442: Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "Chunk 443: form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Chunk 444: to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "Chunk 445: of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "Chunk 446: the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Chunk 447: Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "Chunk 448: distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Chunk 449: Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "Chunk 450: term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Chunk 451: min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "Chunk 452: in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "Chunk 453: solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "Chunk 454: or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "Chunk 455: as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "Chunk 456: i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "Chunk 457: corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "Chunk 458: a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "Chunk 459: A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "Chunk 460: some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "Chunk 461: min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "Chunk 462: ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "Chunk 463: min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "Chunk 464: samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "Chunk 465: However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "Chunk 466: (so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "Chunk 467: the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "Chunk 468: in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "Chunk 469: models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "Chunk 470: αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "Chunk 471: that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "Chunk 472: singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "Chunk 473: is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "Chunk 474: (zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "Chunk 475: L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "Chunk 476: setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "Chunk 477: solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "Chunk 478: The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Chunk 479: was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "Chunk 480: thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "Chunk 481: these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "Chunk 482: namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "Chunk 483: between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "Chunk 484: GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "Chunk 485: full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "Chunk 486: for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "Chunk 487: metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "Chunk 488: up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "Chunk 489: in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "Chunk 490: in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Chunk 491: Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Chunk 492: Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "Chunk 493: metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "Chunk 494: GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "Chunk 495: (αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "Chunk 496: thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "Chunk 497: (X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "Chunk 498: 0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "Chunk 499: spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "Chunk 500: spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "Chunk 501: spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "Chunk 502: detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "Chunk 503: min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "Chunk 504: iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Chunk 505: Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "Chunk 506: P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "Chunk 507: iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "Chunk 508: 30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "Chunk 509: LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "Chunk 510: and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "Chunk 511: in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "Chunk 512: [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "Chunk 513: [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "Chunk 514: [6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "Chunk 515: 20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "Chunk 516: duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "Chunk 517: recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "Chunk 518: Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "Chunk 519: on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "Chunk 520: [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "Chunk 521: Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "Chunk 522: with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "Chunk 523: Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "Chunk 524: 375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "Chunk 525: Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "Chunk 526: Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "Chunk 527: [18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "Chunk 528: [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "Chunk 529: 1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "Chunk 530: [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "Chunk 531: [22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "Chunk 532: D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "Chunk 533: Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "Chunk 534: 27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "Chunk 535: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn_pKOZ4M6JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#char_count_chunking_with_custom_delimiter with semantic chunking\n",
        "\n",
        "\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"PythonCodeTextSplitter\") #You can use any splitter\n",
        "# Create a Document instance\n",
        "cdocs = []\n",
        "for doc in chunks_char_count:\n",
        "    document = Document(page_content=doc)\n",
        "    cdocs.append(document)\n",
        "\n",
        "\n",
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( cdocs , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e9a21b58bb204beba0ebe0333abba1f7",
            "ff1dbaf9c15542e4a909f7ebecf25e51",
            "d01f0f56165944d9b175e8cc76dae4b1",
            "c2a1bb648bb84105b0bff9123b8d0655",
            "49cc3e3ec5464e67834028dfc83be975",
            "7df8daafbce3453d8bd6cd65f2214912",
            "1d1d1bb0e00b408893bc424430fa4ddd",
            "34046841112e4a70871f9b2bbaf3a92c",
            "dbe6af47694049debf494950affb916c",
            "94ec5195371e4dae903a31dc1f3de1cb",
            "b383286bbcba408eba64b4eae0e2d7a9"
          ]
        },
        "id": "Pd2S4THoLzeC",
        "outputId": "b25b02fc-3793-4942-ccef-7742e8aef551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9a21b58bb204beba0ebe0333abba1f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "semantic chunking with text embeddings:\n",
            "Chunk 1: page_content='Mathematical Foundations of Data Sciences\\nGabriel Peyr´ e\\nCNRS & DMA\\n´Ecole Normale Sup´ erieure\\ngabriel.peyre@ens.fr\\nhttps://mathematical-tours.github.io\\nwww.numerical-tours.com\\nAugust 14, 2019\\n2'\n",
            "Chunk 2: page_content='www.numerical-tours.com\\nAugust 14, 2019\\n2\\nChapter 1\\nOptimal Transport\\n1.1 Radon Measures\\nMeasures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat'\n",
            "Chunk 3: page_content='belongs to the probability simplex\\nΣndef.={\\na∈Rn\\n+;n∑\\ni=1ai= 1}\\n. A discrete measure with weights aand locations x1,...,xn∈X reads\\nα=n∑\\ni=1aiδxi (1.1)'\n",
            "Chunk 4: page_content='α=n∑\\ni=1aiδxi (1.1)\\nwhereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location'\n",
            "Chunk 5: page_content='x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\\nmeasure if each of the “weights” described in vector ais positive itself.'\n",
            "Chunk 6: page_content='Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous'\n",
            "Chunk 7: page_content='“objects” within the same framework. Such objects only need to be modelled as measures.'\n",
            "Chunk 8: page_content='This corresponds'\n",
            "Chunk 9: page_content='to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis'\n",
            "Chunk 10: page_content='equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\\nit against continuous functions, denoted f∈C(X).'\n",
            "Chunk 11: page_content='it against continuous functions, denoted f∈C(X). Integration of f∈C(X) against a discrete measure αcomputes a sum\\n∫\\nXf(x)dα(x) =n∑\\ni=1aif(xi).'\n",
            "Chunk 12: page_content='∫\\nXf(x)dα(x) =n∑\\ni=1aif(xi). More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\\ndα(x) =ρα(x)dxw.r.t.'\n",
            "Chunk 13: page_content='the Lebesgue measure, often denoted ρα=dα'\n",
            "Chunk 14: page_content='dx, which means that\\n∀h∈C(Rd),∫\\nRdh(x)dα(x) =∫\\nRdh(x)ρα(x)dx. An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by'\n",
            "Chunk 15: page_content='the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\\nXf(x)dα(x)∈R.'\n",
            "Chunk 16: page_content='Xf(x)dα(x)∈R. IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.'\n",
            "Chunk 17: page_content='Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are'\n",
            "Chunk 18: page_content='dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\\nset of all positive measures on X.'\n",
            "Chunk 19: page_content='The set of probability measures is denoted M1'\n",
            "Chunk 20: page_content='+(X), which means that\\nanyα∈M1\\n+(X) is positive, and that α(X) =∫\\nXdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\\nclasses of measures, beyond histograms, considered in this work.'\n",
            "Chunk 21: page_content='3'\n",
            "Chunk 22: page_content='3\\nDiscreted= 1 Discrete d= 2 Density d= 1 Density d= 2\\nFigure 1.1: Schematic display of discrete distributions α=∑n\\ni=1aiδxi(red corresponds to empirical uniform'\n",
            "Chunk 23: page_content='i=1aiδxi(red corresponds to empirical uniform\\ndistribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both'\n",
            "Chunk 24: page_content='1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\\nand in 2-D using point clouds (radius equal to ai).'\n",
            "Chunk 25: page_content='Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\\nT♯:M(X)→M (Y).'\n",
            "Chunk 26: page_content='For discrete measures (1.1), the pushforward operation consists simply in moving the'\n",
            "Chunk 27: page_content='positions of all the points in the support of the measure\\nT♯αdef.=∑\\niaiδT(xi). For more general measures, for instance for those with a density, the notion of push-forward plays a funda-'\n",
            "Chunk 28: page_content='mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.'\n",
            "Chunk 29: page_content='Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some'\n",
            "Chunk 30: page_content='α∈M (X)reads\\n∀h∈C(Y),∫\\nYh(y)dβ(y) =∫\\nXh(T(x))dα(x). (1.2)\\nEquivalently, for any measurable set B⊂Y, one has\\nβ(B) =α({x∈X;T(x)∈B}).'\n",
            "Chunk 31: page_content='(1.3)'\n",
            "Chunk 32: page_content='β(B) =α({x∈X;T(x)∈B}). (1.3)\\nNote thatT♯preserves positivity and total mass, so that if α∈M1\\n+(X)thenT♯α∈M1\\n+(Y).'\n",
            "Chunk 33: page_content='+(X)thenT♯α∈M1\\n+(Y). Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a'\n",
            "Chunk 34: page_content='measurable space to another. The more general extension T♯can now “move” an entire probability measure'\n",
            "Chunk 35: page_content='onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\\na measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a'\n",
            "Chunk 36: page_content='new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\\n+(X)→M1\\n+(Y) is a linear operator\\nbetween measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.'\n",
            "Chunk 37: page_content='Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures'\n",
            "Chunk 38: page_content='with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\\ndensities linearly as a change of variables in the integration formula, indeed'\n",
            "Chunk 39: page_content='ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\\nwhereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\\nofT). This implies, denoting y=T(x)\\n|det(T′(x))|=ρα(x)\\nρβ(y).'\n",
            "Chunk 40: page_content='|det(T′(x))|=ρα(x)\\nρβ(y). 4\\n=Pi\\x00xiT↵T]↵def.=Pi\\x00T(xi)\\nTT]gdef.=g\\x00TgPush-forward of measures Pull-back of functions\\nFigure 1.2: Comparison of push-forward T♯and pull-back T♯.'\n",
            "Chunk 41: page_content='Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with'\n",
            "Chunk 42: page_content='the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear'\n",
            "Chunk 43: page_content='map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\\nothers, in the sense that\\n∀(α,g)∈M (X)×C(Y),∫\\nYgd(T♯α) =∫\\nX(T♯g)dα.'\n",
            "Chunk 44: page_content='∀(α,g)∈M (X)×C(Y),∫\\nYgd(T♯α) =∫\\nX(T♯g)dα. It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of'\n",
            "Chunk 45: page_content='the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image'\n",
            "Chunk 46: page_content='registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\\nbetween these push-forward and pull-back operators.'\n",
            "Chunk 47: page_content='Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\\nbutions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract'\n",
            "Chunk 48: page_content='(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\\n+(X) such\\nthatP(X∈A) =α(A) =∫\\nAdα(x). Equivalently, it is the push-forward of PbyX,α=X♯P.'\n",
            "Chunk 49: page_content='Applying'\n",
            "Chunk 50: page_content='another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\\nvariableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample'\n",
            "Chunk 51: page_content='yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X. Convergence of random variable.'\n",
            "Chunk 52: page_content='Convergence of random variable (in probability, almost sure, in law),'\n",
            "Chunk 53: page_content='convergence of measures (strong, weak). 1.2 Monge Problem\\nGiven a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a'\n",
            "Chunk 54: page_content='bijectionσin the set Perm( n) of permutations of nelements solving\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i). (1.5)'\n",
            "Chunk 55: page_content='min\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i). (1.5)\\nOne could naively evaluate the cost function above using all permutations in the set Perm( n).'\n",
            "Chunk 56: page_content='However,'\n",
            "Chunk 57: page_content='that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than'\n",
            "Chunk 58: page_content='10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient'\n",
            "Chunk 59: page_content='algorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5'\n",
            "Chunk 60: page_content='5\\nx1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,'\n",
            "Chunk 61: page_content='either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate'\n",
            "Chunk 62: page_content='the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the'\n",
            "Chunk 63: page_content='disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\\n4⩽i⩽7 we haveT(xi) =y1.'\n",
            "Chunk 64: page_content='4⩽i⩽7 we haveT(xi) =y1. Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.'\n",
            "Chunk 65: page_content='Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4'\n",
            "Chunk 66: page_content='corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\\nonly two assignments exist, and they share the same cost.'\n",
            "Chunk 67: page_content='For discrete measures\\nα=n∑'\n",
            "Chunk 68: page_content='For discrete measures\\nα=n∑\\ni=1aiδxiandβ=m∑\\nj=1bjδyj (1.6)\\nthe Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must'\n",
            "Chunk 69: page_content='push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\\nmust verify that\\n∀j∈JmK,bj=∑\\ni:T(xi)=yjai (1.7)'\n",
            "Chunk 70: page_content='must verify that\\n∀j∈JmK,bj=∑\\ni:T(xi)=yjai (1.7)\\nwhich we write in compact form as T♯α=β. This map should minimize some transportation cost, which is'\n",
            "Chunk 71: page_content='parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\\nmin\\nT{∑\\nic(xi,T(xi)) ;T♯α=β}\\n. (1.8)'\n",
            "Chunk 72: page_content='min\\nT{∑\\nic(xi,T(xi)) ;T♯α=β}\\n. (1.8)\\nSuch a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using'\n",
            "Chunk 73: page_content='indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\\n∑\\ni∈σ−1(j)ai=bj. In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation'\n",
            "Chunk 74: page_content='constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\\noptimal matching problem (1.5) where the cost matrix is\\nCi,jdef.=c(xi,yj).'\n",
            "Chunk 75: page_content='Ci,jdef.=c(xi,yj). Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure'\n",
            "Chunk 76: page_content='to another. This happens when their weight vectors are not compatible, which is always the case when the'\n",
            "Chunk 77: page_content='target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\\nan (optimal) Monge map between αandβ, but there is no Monge map from βtoα.'\n",
            "Chunk 78: page_content='6'\n",
            "Chunk 79: page_content='6\\nMonge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\\n(X,Y) as ﬁnding a map T:X→Y that minimizes\\nmin\\nT{∫\\nXc(x,T(x))dα(x) ;T♯α=β}\\n(1.9)'\n",
            "Chunk 80: page_content='min\\nT{∫\\nXc(x,T(x))dα(x) ;T♯α=β}\\n(1.9)\\nThe constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\\noperator (1.2). 1.3 Kantorovitch Problem'\n",
            "Chunk 81: page_content='operator (1.2). 1.3 Kantorovitch Problem\\nThe assignment problem has several limitations in practical settings, also encountered when using the'\n",
            "Chunk 82: page_content='Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only'\n",
            "Chunk 83: page_content='be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-'\n",
            "Chunk 84: page_content='uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may'\n",
            "Chunk 85: page_content='also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)'\n",
            "Chunk 86: page_content='(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible'\n",
            "Chunk 87: page_content='set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation'\n",
            "Chunk 88: page_content='constraint, is non-convex .'\n",
            "Chunk 89: page_content='Both are therefore diﬃcult to solve in their original formulation. Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-'\n",
            "Chunk 90: page_content='ture of transportation, namely the fact that a source point xican only be assigned to another, or transported'\n",
            "Chunk 91: page_content='to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially'\n",
            "Chunk 92: page_content='dispatched across several locations. Kantorovich moves away from the idea that mass transportation should'\n",
            "Chunk 93: page_content='be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is'\n",
            "Chunk 94: page_content='commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\\nusing, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m'\n",
            "Chunk 95: page_content='+, where Pi,jdescribes the\\namount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism'\n",
            "Chunk 96: page_content='of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\\nU(a,b)def.={\\nP∈Rn×m\\n+ ;P1m=aand PT1n=b}\\n, (1.10)'\n",
            "Chunk 97: page_content='U(a,b)def.={\\nP∈Rn×m\\n+ ;P1m=aand PT1n=b}\\n, (1.10)\\nwhere we used the following matrix-vector notation\\nP1m=\\uf8eb\\n\\uf8ed∑\\njPi,j\\uf8f6\\n\\uf8f8\\ni∈Rnand PT1n=(∑\\niPi,j)\\nj∈Rm.'\n",
            "Chunk 98: page_content='P1m=\\uf8eb\\n\\uf8ed∑\\njPi,j\\uf8f6\\n\\uf8f8\\ni∈Rnand PT1n=(∑\\niPi,j)\\nj∈Rm. The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\\npolytope (the convex hull of a ﬁnite set of matrices).'\n",
            "Chunk 99: page_content='Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically'\n",
            "Chunk 100: page_content='asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\\nU(a,b) if and only if PTis inU(b,a). Kantorovich’s optimal transport problem now reads'\n",
            "Chunk 101: page_content='Kantorovich’s optimal transport problem now reads\\nLC(a,b)def.= min\\nP∈U(a,b)⟨C,P⟩def.=∑\\ni,jCi,jPi,j. (1.11)'\n",
            "Chunk 102: page_content='P∈U(a,b)⟨C,P⟩def.=∑\\ni,jCi,jPi,j. (1.11)\\nThis is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\\nnot necessarily unique.'\n",
            "Chunk 103: page_content='7\\n↵\\x00'\n",
            "Chunk 104: page_content='not necessarily unique.'\n",
            "Chunk 105: page_content='7\\n↵\\x00\\n↵\\x00Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj'\n",
            "Chunk 106: page_content='indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,'\n",
            "Chunk 107: page_content='corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).'\n",
            "Chunk 108: page_content='Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\\nassociate two arbitrary discrete measures.'\n",
            "Chunk 109: page_content='associate two arbitrary discrete measures. Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\\ning permutation matrix,\\n∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,'\n",
            "Chunk 110: page_content='∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\\n0 otherwise.(1.12)\\nOne can check that in that case\\n⟨C,Pσ⟩=1\\nnn∑\\ni=1Ci,σi,'\n",
            "Chunk 111: page_content='⟨C,Pσ⟩=1\\nnn∑\\ni=1Ci,σi,\\nwhich shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\\ncouplings Pare restricted to be exactly permutation matrices:\\nmin'\n",
            "Chunk 112: page_content='min\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i)= min\\nσ∈Perm(n)⟨C,Pσ⟩. Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ'\n",
            "Chunk 113: page_content='polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\\n1n1nT/n2is a valid coupling but not a permutation matrix.'\n",
            "Chunk 114: page_content='Therefore, one has naturally that\\nmin'\n",
            "Chunk 115: page_content='min\\nσ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n). The following proposition shows that these problems result in fact in the same optimum, namely that'\n",
            "Chunk 116: page_content='one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform'\n",
            "Chunk 117: page_content='measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment'\n",
            "Chunk 118: page_content='problems.'\n",
            "Chunk 119: page_content='Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\\ncase. Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal'\n",
            "Chunk 120: page_content='solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\\nPerm(n)for Problem (1.5) .'\n",
            "Chunk 121: page_content='Perm(n)for Problem (1.5) .'\n",
            "Chunk 122: page_content='Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of'\n",
            "Chunk 123: page_content='permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the'\n",
            "Chunk 124: page_content='minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\\npolyhedron. 8\\n⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\nDiscrete Semi-discrete Continuous'\n",
            "Chunk 125: page_content='⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\nDiscrete Semi-discrete Continuous\\nFigure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main'\n",
            "Chunk 126: page_content='scenario for Kantorovich OT.'\n",
            "Chunk 127: page_content='Chapter ??is dedicated to the semi-discrete setup. ⇡\\x00↵\\n⇡\\x00↵\\nFigure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The'\n",
            "Chunk 128: page_content='coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”'\n",
            "Chunk 129: page_content='couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\\ndisplay with a black disk at position ( i,j) with radius proportional to Ti,j.'\n",
            "Chunk 130: page_content='Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\\narbitrary measures by considering couplings π∈M1\\n+(X×Y ) which are joint distributions over the product'\n",
            "Chunk 131: page_content='space. The discrete case is a special situation where one imposes this product measure to be of the form\\nπ=∑'\n",
            "Chunk 132: page_content='π=∑\\ni,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\\nmarginal constraint on joint probability distributions\\nU(α,β)def.={\\nπ∈M1'\n",
            "Chunk 133: page_content='U(α,β)def.={\\nπ∈M1\\n+(X×Y ) ;PX♯π=αandPY♯π=β}\\n. (1.13)\\nHerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.'\n",
            "Chunk 134: page_content='Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete'\n",
            "Chunk 135: page_content='measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\\nα(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.'\n",
            "Chunk 136: page_content='α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y. The Kantorovich problem (1.11) is then generalized as\\nLc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y).'\n",
            "Chunk 137: page_content='(1.14)'\n",
            "Chunk 138: page_content='π∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y). (1.14)\\nThis is an inﬁnite-dimensional linear program over a space of measures.'\n",
            "Chunk 139: page_content='Figure 1.6 shows examples of discrete'\n",
            "Chunk 140: page_content='and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\\ninvolving discrete and continuous marginals.'\n",
            "Chunk 141: page_content='involving discrete and continuous marginals. On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called'\n",
            "Chunk 142: page_content='weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\\n9\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡'\n",
            "Chunk 143: page_content='9\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n↵\\x00↵⇡\\x00Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\\nabove (arrows) and couplings below. Inspired by [ ?].'\n",
            "Chunk 144: page_content='is weak-* continuous.'\n",
            "Chunk 145: page_content='And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\\nto impose moment condition on αandβ.'\n",
            "Chunk 146: page_content='to impose moment condition on αandβ. Wasserstein distances.'\n",
            "Chunk 147: page_content='An important feature of OT is that it deﬁnes a distance between histograms'\n",
            "Chunk 148: page_content='and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be'\n",
            "Chunk 149: page_content='understood as a canonical way to lift a ground distance between points to a distance between histogram or\\nmeasures.'\n",
            "Chunk 150: page_content='measures. We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C'\n",
            "Chunk 151: page_content='is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like'\n",
            "Chunk 152: page_content='to compare. The following proposition states that OT provides a meaningful distance between histograms\\nsupported on these bins. Proposition 2.'\n",
            "Chunk 153: page_content='We suppose n=m, and that for some p⩾1,C=Dp= (Dp'\n",
            "Chunk 154: page_content='i,j)i,j∈Rn×nwhere D∈Rn×n\\n+\\nis a distance on JnK,i.e.'\n",
            "Chunk 155: page_content='1.D∈Rn×n\\n+ is symmetric;\\n2.Di,j= 0if and only if i=j;\\n3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k. Then\\nWp(a,b)def.= LDp(a,b)1/p(1.15)'\n",
            "Chunk 156: page_content='Then\\nWp(a,b)def.= LDp(a,b)1/p(1.15)\\n(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,'\n",
            "Chunk 157: page_content='Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\\n∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).'\n",
            "Chunk 158: page_content='∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b). Proof.'\n",
            "Chunk 159: page_content='Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,'\n",
            "Chunk 160: page_content='Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal'\n",
            "Chunk 161: page_content='elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has'\n",
            "Chunk 162: page_content='a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.'\n",
            "Chunk 163: page_content='To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the'\n",
            "Chunk 164: page_content='gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,'\n",
            "Chunk 165: page_content='the explicit constuction of this glued coupling is simple.'\n",
            "Chunk 166: page_content='Let a,b,c∈Σn. Let PandQbe two optimal\\nsolutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0'\n",
            "Chunk 167: page_content='and set otherwise ¯bj= 1 (or actually any other value).'\n",
            "Chunk 168: page_content='We then deﬁne\\nSdef.=Pdiag(1/¯b)Q∈Rn×n\\n+. 10\\nWe remark that S∈U(a,c) because\\nS1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a'\n",
            "Chunk 169: page_content='S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\\nwhere we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b'\n",
            "Chunk 170: page_content='because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.'\n",
            "Chunk 171: page_content='The triangle inequality follows from\\nWp(a,c) =(\\nmin\\nP∈U(a,c)⟨P,Dp⟩)1/p\\n⩽⟨S,Dp⟩1/p\\n=\\uf8eb\\n\\uf8ed∑\\nikDp\\nik∑\\njPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb'\n",
            "Chunk 172: page_content='⩽⟨S,Dp⟩1/p\\n=\\uf8eb\\n\\uf8ed∑\\nikDp\\nik∑\\njPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijk(Dij+Djk)pPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijkDp\\nijPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\nijkDp\\njkPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij∑\\nkQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk∑\\niPij'\n",
            "Chunk 173: page_content='ijDp\\nijPij∑\\nkQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk∑\\niPij\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk\\uf8f6\\n\\uf8f81/p\\n= Wp(a,b) + Wp(b,b).'\n",
            "Chunk 174: page_content='\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk\\uf8f6\\n\\uf8f81/p\\n= Wp(a,b) + Wp(b,b). The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements'\n",
            "Chunk 175: page_content='inD, and the third comes from Minkowski’s inequality. Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.'\n",
            "Chunk 176: page_content='Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\\nX,i.e. (i)d(x,y) =d(y,x)⩾0;\\n(ii)d(x,y) = 0 if and only if x=y;\\n(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).'\n",
            "Chunk 177: page_content='Then'\n",
            "Chunk 178: page_content='(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z). Then\\nWp(α,β)def.=Ldp(α,β)1/p(1.16)\\n(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,'\n",
            "Chunk 179: page_content='Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\\n∀(α,β,γ )∈M1\\n+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).'\n",
            "Chunk 180: page_content='∀(α,β,γ )∈M1\\n+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ). Proof.'\n",
            "Chunk 181: page_content='The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling'\n",
            "Chunk 182: page_content='between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ). The Wasserstein distance Wphas many important properties, the most important one being that it is a'\n",
            "Chunk 183: page_content='weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify'\n",
            "Chunk 184: page_content='spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)'\n",
            "Chunk 185: page_content='are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures'\n",
            "Chunk 186: page_content='with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\\nbe ﬁxed to work). In sharp contrast, one has that for any p >0,Wp'\n",
            "Chunk 187: page_content='p(δx,δy) =d(x,y). Indeed, it suﬃces\\nto notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\\nWp'\n",
            "Chunk 188: page_content='Wp\\np(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\\ncorresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.'\n",
            "Chunk 189: page_content='11'\n",
            "Chunk 190: page_content='11\\nDeﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\\n+(X)(denotedαk⇀α ) if and only if\\nfor any continuous function g∈C(X),∫\\nXgdαk→∫\\nXgdα. This notion of weak convergence corresponds to'\n",
            "Chunk 191: page_content='the convergence in law of random vectors. This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a'\n",
            "Chunk 192: page_content='convergence of the moments up to order pfor unbounded metric spaces). Note that there exists alternative distances which also metrize weak convergence.'\n",
            "Chunk 193: page_content='The simplest one are'\n",
            "Chunk 194: page_content='Hilbertian norms, deﬁned as\\n||α||2\\nkdef.=Eα⊗α(k) =∫\\nX×Xk(x,y)dα(x)dα(y)\\nfor a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\\ne−||x−y||2'\n",
            "Chunk 195: page_content='e−||x−y||2\\n2σ2for some choice of bandwidth σ>0. This convergence should not be confounded with the strong convergence of measures, which is metrized'\n",
            "Chunk 196: page_content='by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.'\n",
            "Chunk 197: page_content='Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such'\n",
            "Chunk 198: page_content='as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used'\n",
            "Chunk 199: page_content='pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there'\n",
            "Chunk 200: page_content='exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and'\n",
            "Chunk 201: page_content='the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,'\n",
            "Chunk 202: page_content='which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\\nOT problem. 1.4 Duality'\n",
            "Chunk 203: page_content='OT problem. 1.4 Duality\\nThe Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be'\n",
            "Chunk 204: page_content='naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The'\n",
            "Chunk 205: page_content='following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\\nrelationship between the primal and dual problems. Proposition 4.'\n",
            "Chunk 206: page_content='One has\\nLC(a,b) = max'\n",
            "Chunk 207: page_content='Proposition 4. One has\\nLC(a,b) = max\\n(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\\nwhere the set of admissible potentials is\\nR(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)'\n",
            "Chunk 208: page_content='Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-'\n",
            "Chunk 209: page_content='grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)'\n",
            "Chunk 210: page_content='is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\\nwith the use of Lagrangian duality.'\n",
            "Chunk 211: page_content='The Lagangian associate to (1.11) reads\\nmin\\nP⩾0max'\n",
            "Chunk 212: page_content='min\\nP⩾0max\\n(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\\nFor linear program, one can always exchange the min and the max and get the same value of the linear\\nprogram, and one thus consider\\nmax'\n",
            "Chunk 213: page_content='program, and one thus consider\\nmax\\n(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\\nP⩾0⟨C−f1⊤\\nm−1ng⊤,P⟩. We conclude by remarking that\\nmin\\nP⩾0⟨Q,P⟩={0 if Q⩾0\\n−∞ otherwise\\nso that the constraint reads C−f1⊤'\n",
            "Chunk 214: page_content='−∞ otherwise\\nso that the constraint reads C−f1⊤\\nm−1ng⊤=C−f⊕g⩾0. 12\\nThe primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\\ntransport plan\\nSupp( P)⊂{'\n",
            "Chunk 215: page_content='transport plan\\nSupp( P)⊂{\\n(i,j)∈JnK×JmK;fi+gj=Ci,j}\\n. (1.20)\\nTo extend this primal-dual construction to arbitrary measures, it is important to realize that measures'\n",
            "Chunk 216: page_content='are naturally paired in duality with continuous functions (a measure can only be accessed through integration'\n",
            "Chunk 217: page_content='against continuous functions). The duality is formalized in the following proposition, which boils down to\\nProposition 4 when dealing with discrete measures.'\n",
            "Chunk 218: page_content='Proposition 5. One has\\nLc(α,β) = max'\n",
            "Chunk 219: page_content='Proposition 5. One has\\nLc(α,β) = max\\n(f,g)∈R(c)∫\\nXf(x)dα(x) +∫\\nYg(y)dβ(y), (1.21)\\nwhere the set of admissible dual potentials is\\nR(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}.'\n",
            "Chunk 220: page_content='(1.22)'\n",
            "Chunk 221: page_content='Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”. The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.'\n",
            "Chunk 222: page_content='(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\\nand (1.20) is generalized as\\nSupp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}.'\n",
            "Chunk 223: page_content='(1.23)'\n",
            "Chunk 224: page_content='Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\\nNote that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-'\n",
            "Chunk 225: page_content='trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the'\n",
            "Chunk 226: page_content='machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\\nLipschitz regular, which enable to replace the constraint by a compact one.'\n",
            "Chunk 227: page_content='Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in'\n",
            "Chunk 228: page_content='Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\\nare equivalent.'\n",
            "Chunk 229: page_content='are equivalent. Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs'\n",
            "Chunk 230: page_content='measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\\nKantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:'\n",
            "Chunk 231: page_content='Rd→Rd. This means that π= (Id,T)♯µ,i.e.'\n",
            "Chunk 232: page_content='∀h∈C(X×Y ),∫\\nX×Yh(x,y)dπ(x,y) =∫\\nXh(x,T(x))dµ(x). (1.24)\\nFurthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where'\n",
            "Chunk 233: page_content='ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\\nrelated to the dual potential fsolving (1.21) asϕ(x) =||x||2\\n2−f(x).'\n",
            "Chunk 234: page_content='2−f(x). Proof.'\n",
            "Chunk 235: page_content='We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\\nthat∫\\ncdπ=Cα,β−2∫\\n⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\\n||x||2dα(x) +∫'\n",
            "Chunk 236: page_content='||x||2dα(x) +∫\\n||y||2dβ(y). Instead of\\nsolving (1.14), one can thus consider the following problem\\nmax\\nπ∈U(α,β)∫\\nX×Y⟨x, y⟩dπ(x,y),\\nwhose dual reads\\nmin\\n(ϕ,ψ){∫\\nXϕdα+∫\\nYψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}'\n",
            "Chunk 237: page_content='(ϕ,ψ){∫\\nXϕdα+∫\\nYψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\\n. (1.25)\\n13\\nThe relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\\n2−f,||·||2\\n2−g).'\n",
            "Chunk 238: page_content='One can replace the\\nconstraint by'\n",
            "Chunk 239: page_content='2−g).'\n",
            "Chunk 240: page_content='One can replace the\\nconstraint by\\n∀y, ψ (y)⩾ϕ∗(y)def.= sup\\nx⟨x, y⟩−ϕ(x). (1.26)\\nHereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see'\n",
            "Chunk 241: page_content='also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can'\n",
            "Chunk 242: page_content='minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\\nmin\\nϕ∫\\nXϕdα+∫\\nYϕ∗dβ, (1.27)'\n",
            "Chunk 243: page_content='min\\nϕ∫\\nXϕdα+∫\\nYϕ∗dβ, (1.27)\\nsee also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument'\n",
            "Chunk 244: page_content='twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.'\n",
            "Chunk 245: page_content='Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that'\n",
            "Chunk 246: page_content='such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads'\n",
            "Chunk 247: page_content='y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also'\n",
            "Chunk 248: page_content='diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\\neverywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.'\n",
            "Chunk 249: page_content='This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)'\n",
            "Chunk 250: page_content='and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog'\n",
            "Chunk 251: page_content='of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport'\n",
            "Chunk 252: page_content='problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).'\n",
            "Chunk 253: page_content='Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should'\n",
            "Chunk 254: page_content='be examined under the light that a convex function is the natural generalization of the notion of increasing'\n",
            "Chunk 255: page_content='functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile'\n",
            "Chunk 256: page_content='functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].'\n",
            "Chunk 257: page_content='Note also that this theorem can be extended in many directions. The condition that αhas a density can'\n",
            "Chunk 258: page_content='be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller'\n",
            "Chunk 259: page_content='thand−1 (e.g.'\n",
            "Chunk 260: page_content='hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\\nstrictly convex function.'\n",
            "Chunk 261: page_content='strictly convex function. For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a'\n",
            "Chunk 262: page_content='constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\\ndet(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)'\n",
            "Chunk 263: page_content='det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\\nwhere∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a'\n",
            "Chunk 264: page_content='non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\\nLaplacian ∆ as a linearization since for smooth maps\\ndet(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).'\n",
            "Chunk 265: page_content='det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε). The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.'\n",
            "Chunk 266: page_content='Special cases In general, computing OT distances is numerically involved. We review special favorable\\ncases where the resolution of the OT problem is easy.'\n",
            "Chunk 267: page_content='Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\\nthe diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to'\n",
            "Chunk 268: page_content='the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to'\n",
            "Chunk 269: page_content='discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\\nbetween two discrete measures αandβis equal to their total variation distance.'\n",
            "Chunk 270: page_content='14'\n",
            "Chunk 271: page_content='14\\n\\x00\\x00↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.'\n",
            "Chunk 272: page_content='Top: empirical measures with same number of points (optimal matching).'\n",
            "Chunk 273: page_content='Bottom: generic case. This\\ncorresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily'\n",
            "Chunk 274: page_content='yj⩽yj′.'\n",
            "Chunk 275: page_content='Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\\nn∑n\\ni=1δxiandβ=1\\nn∑n\\nj=1δyj,\\nand assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand'\n",
            "Chunk 276: page_content='y1⩽y2⩽...⩽yn, then one has the simple formula\\nWp(α,β)p=p∑\\ni=1|xi−yi|p, (1.29)\\ni.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of'\n",
            "Chunk 277: page_content='αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)'\n",
            "Chunk 278: page_content='might change whenever some of the values change. That formula is a simple consequence of the more general'\n",
            "Chunk 279: page_content='remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures'\n",
            "Chunk 280: page_content='with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary'\n",
            "Chunk 281: page_content='discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the'\n",
            "Chunk 282: page_content='circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour'\n",
            "Chunk 283: page_content='of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case. Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function'\n",
            "Chunk 284: page_content='∀x∈R,Cα(x)def.=∫x\\n−∞dα, (1.30)\\nwhich is a function Cα:R→[0,1], and its pseudo-inverse C−1\\nα: [0,1]→R∪{−∞}\\n∀r∈[0,1],C−1\\nα(r) = min\\nx{x∈R∪{−∞} ;Cα(x)⩾r}.'\n",
            "Chunk 285: page_content='∀r∈[0,1],C−1\\nα(r) = min\\nx{x∈R∪{−∞} ;Cα(x)⩾r}.'\n",
            "Chunk 286: page_content='That function is also called the generalized quantile function of α. For anyp⩾1, one has\\nWp(α,β)p=||C−1\\nα−C−1\\nβ||p\\nLp([0,1])=∫1\\n0|C−1\\nα(r)−C−1'\n",
            "Chunk 287: page_content='α−C−1\\nβ||p\\nLp([0,1])=∫1\\n0|C−1\\nα(r)−C−1\\nβ(r)|pdr. (1.31)\\nThis means that through the map α↦→C−1\\nα, the Wasserstein distance is isometric to a linear space equipped'\n",
            "Chunk 288: page_content='with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian'\n",
            "Chunk 289: page_content='metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its'\n",
            "Chunk 290: page_content='geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\\nin§??. Forp= 1, one even has the simpler formula\\nW1(α,β) =||Cα−Cβ||L1(R)=∫'\n",
            "Chunk 291: page_content='W1(α,β) =||Cα−Cβ||L1(R)=∫\\nR|Cα(x)−Cβ(x)|dx (1.32)\\n=∫\\nR⏐⏐⏐⏐∫x\\n−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\\n15\\nµ ν (tT+ (1−t)Id)♯µ\\n0 0.5 10.5Cµ\\nCν\\n0 0.5 100.51\\nCµ-1\\nCν-1\\n0 0.5 100.51\\nT\\nT-1\\n0 0.5 100.51\\n(Cα,Cβ) (C−1\\nα,C−1'\n",
            "Chunk 292: page_content='T\\nT-1\\n0 0.5 100.51\\n(Cα,Cβ) (C−1\\nα,C−1\\nβ) ( T,T−1) (1−t)C−1\\nα+tC−1\\nβ\\nFigure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant'\n",
            "Chunk 293: page_content='function as detailed in (1.34).'\n",
            "Chunk 294: page_content='which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\\nmapTsuch thatT♯α=βis then deﬁned by\\nT=C−1\\nβ◦Cα. (1.34)'\n",
            "Chunk 295: page_content='T=C−1\\nβ◦Cα. (1.34)\\nFigure 1.9 illustrates the computation of 1-D OT through cumulative functions.'\n",
            "Chunk 296: page_content='It also displays displacement'\n",
            "Chunk 297: page_content='interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\\noptimal transport in 1-D, we refer the reader to [ ?, Chapter 2].'\n",
            "Chunk 298: page_content='Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\\nthen one can show that the following map\\nT:x↦→mβ+A(x−mα), (1.35)\\nwhere\\nA=Σ−1\\n2α(\\nΣ1\\n2αΣβΣ1\\n2α)1\\n2Σ−1\\n2α=AT,'\n",
            "Chunk 299: page_content='where\\nA=Σ−1\\n2α(\\nΣ1\\n2αΣβΣ1\\n2α)1\\n2Σ−1\\n2α=AT,\\nis such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\\nsince\\nρβ(T(x)) = det(2πΣβ)−1'\n",
            "Chunk 300: page_content='since\\nρβ(T(x)) = det(2πΣβ)−1\\n2exp(−⟨T(x)−mβ,Σ−1\\nβ(T(x)−mβ)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα, ATΣ−1\\nβA(x−mα)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα,Σ−1\\nα(x−mα)⟩),\\nand sinceTis a linear map we have that'\n",
            "Chunk 301: page_content='α(x−mα)⟩),\\nand sinceTis a linear map we have that\\n|detT′(x)|= detA=(detΣβ\\ndetΣα)1\\n2\\nand we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\\nfunctionψ:x↦→1'\n",
            "Chunk 302: page_content='functionψ:x↦→1\\n2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)'\n",
            "Chunk 303: page_content='thatTis optimal.'\n",
            "Chunk 304: page_content='Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ?? 16\\n-4 -2 0 2 4 6-3-2-101234'\n",
            "Chunk 305: page_content='16\\n-4 -2 0 2 4 6-3-2-101234\\nρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\\nmean and variance matrices mα= (−2,0),Σα=1\\n2(\\n1−1\\n2;−1\\n21)'\n",
            "Chunk 306: page_content='2(\\n1−1\\n2;−1\\n21)\\nandmβ= (3,1),Σβ=(\\n2,1\\n2;1\\n2,1)\\n. The\\narrows originate at random points xtaken on the plane and end at the corresponding mappings of those\\npointsT(x) =mβ+A(x−mα).'\n",
            "Chunk 307: page_content='\\x00m'\n",
            "Chunk 308: page_content='pointsT(x) =mβ+A(x−mα). \\x00m\\nFigure 1.11: Computation of displacement interpolation between two 1-D Gaussians.'\n",
            "Chunk 309: page_content='Denoting Gm,σ(x)def.=\\n1√\\n2πse−(x−m)2'\n",
            "Chunk 310: page_content='1√\\n2πse−(x−m)2\\n2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.'\n",
            "Chunk 311: page_content='With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\\ncost of that map is\\nW2\\n2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)'\n",
            "Chunk 312: page_content='W2\\n2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\\nwhereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\\nB(Σα,Σβ)2def.= tr(\\nΣα+Σβ−2(Σ1/2\\nαΣβΣ1/2\\nα)1/2)\\n, (1.37)'\n",
            "Chunk 313: page_content='Σα+Σβ−2(Σ1/2\\nαΣβΣ1/2\\nα)1/2)\\n, (1.37)\\nwhere Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that'\n",
            "Chunk 314: page_content='B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\\ndiagonals, the Bures metric is the Hellinger distance\\nB(Σα,Σβ) =||√r−√s||2.'\n",
            "Chunk 315: page_content='B(Σα,Σβ) =||√r−√s||2. For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\\nΣ), as illustrated in Figure 1.11.'\n",
            "Chunk 316: page_content='Σ), as illustrated in Figure 1.11.'\n",
            "Chunk 317: page_content='For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?]. 1.5 Sinkhorn'\n",
            "Chunk 318: page_content='1.5 Sinkhorn\\nThis section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation'\n",
            "Chunk 319: page_content='of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to'\n",
            "Chunk 320: page_content='the original problem. This regularization has several important advantages, but a few stand out particularly:'\n",
            "Chunk 321: page_content='The minimization of the regularized problen can be solved using a simple alternate minimization scheme;'\n",
            "Chunk 322: page_content='that scheme translates into iterations that are simple matrix products, making them particularly suited to'\n",
            "Chunk 323: page_content='execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\\nand positions of the Diracs. 17'\n",
            "Chunk 324: page_content='and positions of the Diracs. 17\\nc\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\\nargminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.'\n",
            "Chunk 325: page_content='argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.'\n",
            "Chunk 326: page_content='Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\\nH(P)def.=−∑\\ni,jPi,j(log(Pi,j)−1), (1.38)'\n",
            "Chunk 327: page_content='H(P)def.=−∑\\ni,jPi,j(log(Pi,j)−1), (1.38)\\nwith an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis'\n",
            "Chunk 328: page_content='0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and'\n",
            "Chunk 329: page_content='Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\\nto obtain approximate solutions to the original transport problem (1.11):\\nLε'\n",
            "Chunk 330: page_content='Lε\\nC(a,b)def.= min\\nP∈U(a,b)⟨P,C⟩−εH(P). (1.39)\\nSince the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution.'\n",
            "Chunk 331: page_content='The idea'\n",
            "Chunk 332: page_content='to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in'\n",
            "Chunk 333: page_content='transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the'\n",
            "Chunk 334: page_content='solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend'\n",
            "Chunk 335: page_content='to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for'\n",
            "Chunk 336: page_content='that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\\nmore “blurred” traﬃc prediction.'\n",
            "Chunk 337: page_content='more “blurred” traﬃc prediction. Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which'\n",
            "Chunk 338: page_content='can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away'\n",
            "Chunk 339: page_content='from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the'\n",
            "Chunk 340: page_content='triangle. This is further detailed in the proposition below.'\n",
            "Chunk 341: page_content='The convergence of the solution of that regularized'\n",
            "Chunk 342: page_content='problem towards an optimal solution of the original linear program has been studied by [ ?]. Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution'\n",
            "Chunk 343: page_content='with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\\nPεε→0−→argmin\\nP{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\\nso that in particular\\nLε\\nC(a,b)ε→0−→LC(a,b).'\n",
            "Chunk 344: page_content='so that in particular\\nLε\\nC(a,b)ε→0−→LC(a,b). One has\\nPεε→∞−→abT= (aibj)i,j.'\n",
            "Chunk 345: page_content='(1.41)\\nProof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for'\n",
            "Chunk 346: page_content='ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\\nsuch that Pℓ→P⋆.'\n",
            "Chunk 347: page_content='Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).'\n",
            "Chunk 348: page_content='By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\\n0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\\n18\\n⇡\"↵\\x00'\n",
            "Chunk 349: page_content='0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\\n18\\n⇡\"↵\\x00\\n\"\\x00↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.'\n",
            "Chunk 350: page_content='Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number'\n",
            "Chunk 351: page_content='n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\\nbetweenxiandyj).'\n",
            "Chunk 352: page_content='betweenxiandyj). Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that'\n",
            "Chunk 353: page_content='P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\\nH(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40).'\n",
            "Chunk 354: page_content='Since the solution P⋆'\n",
            "Chunk 355: page_content='0to this program is unique\\nby strict convexity of −H, one has P⋆=P⋆\\n0, and the whole sequence is converging.'\n",
            "Chunk 356: page_content='0, and the whole sequence is converging. Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal'\n",
            "Chunk 357: page_content='transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the'\n",
            "Chunk 358: page_content='coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between'\n",
            "Chunk 359: page_content='two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is'\n",
            "Chunk 360: page_content='performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞).'\n",
            "Chunk 361: page_content='Figure 1.13'\n",
            "Chunk 362: page_content='shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling'\n",
            "Chunk 363: page_content='becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in'\n",
            "Chunk 364: page_content='turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\\nfaster statistical convergence (as exposed in §??).'\n",
            "Chunk 365: page_content='Deﬁning the Kullback-Leibler divergence between couplings as\\nKL(P|K)def.=∑\\ni,jPi,jlog(Pi,j\\nKi,j)\\n−Pi,j+Ki,j, (1.43)'\n",
            "Chunk 366: page_content='i,jPi,jlog(Pi,j\\nKi,j)\\n−Pi,j+Ki,j, (1.43)\\nthe unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\\nCas\\nKi,jdef.=e−Ci,j\\nε'\n",
            "Chunk 367: page_content='Cas\\nKi,jdef.=e−Ci,j\\nε\\nIndeed one has that using the deﬁnition above\\nPε= ProjKL\\nU(a,b)(K)def.= argmin\\nP∈U(a,b)KL(P|K). (1.44)'\n",
            "Chunk 368: page_content='U(a,b)(K)def.= argmin\\nP∈U(a,b)KL(P|K). (1.44)\\nRemark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy'\n",
            "Chunk 369: page_content='by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\\nregularized counterpart to (1.14) using\\nLε\\nc(α,β)def.= min\\nπ∈U(α,β)∫'\n",
            "Chunk 370: page_content='Lε\\nc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\\nwhere the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\\nKL(π|ξ)def.=∫\\nX×Ylog(dπ\\ndξ(x,y))'\n",
            "Chunk 371: page_content='KL(π|ξ)def.=∫\\nX×Ylog(dπ\\ndξ(x,y))\\ndπ(x,y)+\\n∫\\nX×Y(dξ(x,y)−dπ(x,y)),(1.46)\\n19\\nand by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\\ndξwith respect to ξ. It is important to'\n",
            "Chunk 372: page_content='dξwith respect to ξ. It is important to\\nrealize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\\nplays no speciﬁc role, only its support matters.'\n",
            "Chunk 373: page_content='plays no speciﬁc role, only its support matters. Formula (1.45) can be re-factored as a projection problem\\nmin\\nπ∈U(α,β)KL(π|K) (1.47)\\nwhereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)'\n",
            "Chunk 374: page_content='εdµ(x)dν(y). This problem is often referred to as the\\n“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].'\n",
            "Chunk 375: page_content='Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??'\n",
            "Chunk 376: page_content='details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\\nthe points of two measures.'\n",
            "Chunk 377: page_content='the points of two measures. Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,'\n",
            "Chunk 378: page_content='which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\\nthe sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.'\n",
            "Chunk 379: page_content='Proposition 7. The solution to (1.39) is unique and has the form\\n∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\\nfor two (unknown) scaling variable (u,v)∈Rn\\n+×Rm\\n+.'\n",
            "Chunk 380: page_content='+×Rm\\n+.'\n",
            "Chunk 381: page_content='Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\\nreads\\nE(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.'\n",
            "Chunk 382: page_content='reads\\nE(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩. Considering ﬁrst order conditions, we have\\n∂E(P,f,g)\\n∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.'\n",
            "Chunk 383: page_content='∂E(P,f,g)\\n∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj. which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε'\n",
            "Chunk 384: page_content='which can be rewritten in the form provided in the proposition using non-negative vectors uandv.'\n",
            "Chunk 385: page_content='The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in'\n",
            "Chunk 386: page_content='matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\\ncorrespond to the mass conservation constraints inherent to U(a,b),'\n",
            "Chunk 387: page_content='diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\\nThese two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\\ntimes Kvis'\n",
            "Chunk 388: page_content='times Kvis\\nu⊙(Kv) =aand v⊙(KTu) =b (1.50)\\nwhere⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis'\n",
            "Chunk 389: page_content='community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve'\n",
            "Chunk 390: page_content='these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of'\n",
            "Chunk 391: page_content='Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\\nu(ℓ+1)def.=a\\nKv(ℓ)and v(ℓ+1)def.=b\\nKTu(ℓ+1), (1.51)'\n",
            "Chunk 392: page_content='Kv(ℓ)and v(ℓ+1)def.=b\\nKTu(ℓ+1), (1.51)\\ninitialized with an arbitrary positive vector v(0)=1m. The division operator used above between two'\n",
            "Chunk 393: page_content='vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\\n20\\n`⇡(`)\"'\n",
            "Chunk 394: page_content='20\\n`⇡(`)\"\\n1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\\nε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof'\n",
            "Chunk 395: page_content='Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\\nin term of marginal constraint violation log( ||πℓ\\nε1m−b||1).'\n",
            "Chunk 396: page_content='ε1m−b||1). solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then'\n",
            "Chunk 397: page_content='so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for'\n",
            "Chunk 398: page_content='a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in'\n",
            "Chunk 399: page_content='the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling'\n",
            "Chunk 400: page_content='diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\\noptimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.'\n",
            "Chunk 401: page_content='Remark 11 (Relation with iterative projections) .Denoting\\nC1\\nadef.={P;P1m=a}andC2\\nbdef.={\\nP;PT1m=b}\\nthe rows and columns constraints, one has U(a,b) =C1\\na∩C2'\n",
            "Chunk 402: page_content='a∩C2\\nb. One can use Bregman iterative projections [ ?]\\nP(ℓ+1) def.= ProjKL\\nC1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\\nC2\\nb(P(ℓ+1)).'\n",
            "Chunk 403: page_content='(1.52)\\nSince the setsC1\\naandC2'\n",
            "Chunk 404: page_content='C2\\nb(P(ℓ+1)). (1.52)\\nSince the setsC1\\naandC2\\nbare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].'\n",
            "Chunk 405: page_content='These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\\nP(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\\none has\\nP(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\\nand P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))'\n",
            "Chunk 406: page_content='and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\\nIn practice however one should prefer using (1.51) which only requires manipulating scaling vectors and'\n",
            "Chunk 407: page_content='multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).'\n",
            "Chunk 408: page_content='Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\\ngreatly simpliﬁed using Hilbert projective metric on Rn\\n+,∗(positive vectors), deﬁned as'\n",
            "Chunk 409: page_content='+,∗(positive vectors), deﬁned as\\n∀(u,u′)∈(Rn\\n+,∗)2, dH(u,u′)def.= log max\\ni,i′uiu′\\ni′\\nui′u′\\ni. This can be shows to be a distance on the projective cone Rn\\n+,∗/∼, where u∼u′means that∃s>0,u=su′'\n",
            "Chunk 410: page_content='+,∗/∼, where u∼u′means that∃s>0,u=su′\\n(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the'\n",
            "Chunk 411: page_content='triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\\ndistance on bounded open convex sets [ ?].'\n",
            "Chunk 412: page_content='The projective cone Rn'\n",
            "Chunk 413: page_content='+,∗/∼is a complete metric space for this\\ndistance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius'\n",
            "Chunk 414: page_content='theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They'\n",
            "Chunk 415: page_content='proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\\ncone of positive vectors. 21\\nTheorem 2.'\n",
            "Chunk 416: page_content='Let K∈Rn×m\\n+,∗, then for (v,v′)∈(Rm\\n+,∗)2'\n",
            "Chunk 417: page_content='+,∗, then for (v,v′)∈(Rm\\n+,∗)2\\ndH(Kv,Kv′)⩽λ(K)dH(v,v′)where\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3λ(K)def.=√\\nη(K)−1√\\nη(K)+1<1\\nη(K)def.= max\\ni,j,k,ℓKi,kKj,ℓ\\nKj,kKi,ℓ.'\n",
            "Chunk 418: page_content='η(K)+1<1\\nη(K)def.= max\\ni,j,k,ℓKi,kKj,ℓ\\nKj,kKi,ℓ. Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to'\n",
            "Chunk 419: page_content='show the linear convergence of Sinkhorn’s iterations.'\n",
            "Chunk 420: page_content='Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\\ndH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\\nOne also has\\ndH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\\n1−λ(K)'\n",
            "Chunk 421: page_content='One also has\\ndH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\\n1−λ(K)\\ndH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\\n1−λ(K)(1.54)\\nwhere we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has'\n",
            "Chunk 422: page_content='∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\\nwhere P⋆is the unique solution of (1.39) . Proof.'\n",
            "Chunk 423: page_content='One notice that for any ( v,v′)∈(Rm\\n+,∗)2, one has\\ndH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).'\n",
            "Chunk 424: page_content='dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′). This shows that\\ndH(u(ℓ+1),u⋆) =dH(a\\nKv(ℓ),a\\nKv⋆)\\n=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).'\n",
            "Chunk 425: page_content='Kv(ℓ),a\\nKv⋆)\\n=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆). where we used Theorem 2.'\n",
            "Chunk 426: page_content='This shows (1.53). One also has, using the triangular inequality\\ndH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\\n⩽dH(a\\nKv(ℓ),u(ℓ))'\n",
            "Chunk 427: page_content='⩽dH(a\\nKv(ℓ),u(ℓ))\\n+λ(K)dH(u(ℓ),u⋆)\\n=dH(\\na,u(ℓ)⊙(Kv(ℓ)))\\n+λ(K)dH(u(ℓ),u⋆),\\nwhich gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof'\n",
            "Chunk 428: page_content='of (1.55) follows from [ ?, Lemma 3]\\nThe bound (1.54) shows that some error measures on the marginal constraints violation, for instance'\n",
            "Chunk 429: page_content='∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence. Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate'\n",
            "Chunk 430: page_content='degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius'\n",
            "Chunk 431: page_content='Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??).'\n",
            "Chunk 432: page_content='This'\n",
            "Chunk 433: page_content='convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\\nof the scaled coupling matrix. 22'\n",
            "Chunk 434: page_content='of the scaled coupling matrix. 22\\nRegularized Dual and Log-domain Computations The following proposition details the dual problem\\nassociated to (1.39). Proposition 8.'\n",
            "Chunk 435: page_content='One has\\nLε\\nC(a,b) = max'\n",
            "Chunk 436: page_content='Proposition 8. One has\\nLε\\nC(a,b) = max\\nf∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\\nThe optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\\n(u,v) = (ef/ε,eg/ε).'\n",
            "Chunk 437: page_content='(1.57)'\n",
            "Chunk 438: page_content='(u,v) = (ef/ε,eg/ε). (1.57)\\nProof.'\n",
            "Chunk 439: page_content='We start from the end of the proof of Proposition 7, which links the optimal primal solution P'\n",
            "Chunk 440: page_content='and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the'\n",
            "Chunk 441: page_content='LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\\ndual function equals\\nf,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)'\n",
            "Chunk 442: page_content='The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\\n⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\\n=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩'\n",
            "Chunk 443: page_content='=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\\ntherefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\\nare those displayed in (1.56).'\n",
            "Chunk 444: page_content='are those displayed in (1.56). Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\\nproblem (1.56) reads\\nsup\\nf,g∈C(X)×C(Y)∫\\nXf(x)dα(x) +∫'\n",
            "Chunk 445: page_content='sup\\nf,g∈C(X)×C(Y)∫\\nXf(x)dα(x) +∫\\nYg(x)dβ(x)−ε∫\\nX×Ye−c(x,y)+f(x)+g(y)\\nε dα(x)dβ(y)\\nThis corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which'\n",
            "Chunk 446: page_content='is retrieved in the limit ε→0.'\n",
            "Chunk 447: page_content='Proving existence ( i.e. the sup is actually a max) of these Kantorovich'\n",
            "Chunk 448: page_content='potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot'\n",
            "Chunk 449: page_content='usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\\nconvergence of Sinkhorn iterations, see [ ?] for more details.'\n",
            "Chunk 450: page_content='Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the'\n",
            "Chunk 451: page_content='unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to'\n",
            "Chunk 452: page_content='update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\\ncan easily notice that, writing Q(f,g) for the objective of (1.56) that\\n∇|fQ(f,g) =a−ef/ε⊙('\n",
            "Chunk 453: page_content='∇|fQ(f,g) =a−ef/ε⊙(\\nKeg/ε)\\n, (1.59)\\n∇|gQ(f,g) =b−eg/ε⊙(\\nKTef/ε)\\n. (1.60)\\nBlock coordinate ascent can therefore be implemented in a closed form by applying successively the following'\n",
            "Chunk 454: page_content='updates, starting from any arbitrary g(0), forl⩾0,\\nf(ℓ+1)=εloga−εlog(\\nKeg(ℓ)/ε)\\n, (1.61)\\ng(ℓ+1)=εlogb−εlog(\\nKTef(ℓ+1)/ε)\\n. (1.62)'\n",
            "Chunk 455: page_content=', (1.61)\\ng(ℓ+1)=εlogb−εlog(\\nKTef(ℓ+1)/ε)\\n. (1.62)\\nSuch iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-'\n",
            "Chunk 456: page_content='dual relations highlighted in (1.57). Indeed, we recover that at any iteration\\n(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).'\n",
            "Chunk 457: page_content='23'\n",
            "Chunk 458: page_content='(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))). 23\\nRemark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,'\n",
            "Chunk 459: page_content='using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\\ncoordinates, namely\\nminεz=−εlog∑\\nie−zi/ε.'\n",
            "Chunk 460: page_content='coordinates, namely\\nminεz=−εlog∑\\nie−zi/ε. Note that min ε(z) converges to min zfor any vector zasε→0.'\n",
            "Chunk 461: page_content='Indeed, min εcan be interpreted as a'\n",
            "Chunk 462: page_content='diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\\nrewritten\\n(f(ℓ+1))i= minε(Cij−g(ℓ)\\nj)j+εlogai, (1.63)\\n(g(ℓ+1))j= minε(Cij−f(ℓ)'\n",
            "Chunk 463: page_content='j)j+εlogai, (1.63)\\n(g(ℓ+1))j= minε(Cij−f(ℓ)\\ni)i+εlogbj. (1.64)\\nHere the term min ε(Cij−g(ℓ)\\nj)jdenotes the soft-minimum of all values of the j-th column of matrix'\n",
            "Chunk 464: page_content='(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs'\n",
            "Chunk 465: page_content='now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\\nwe deﬁne\\nMinrow\\nε(A)def.=(\\nminε(Ai,j)j)\\ni∈Rn,\\nMincol\\nε(A)def.=(\\nminε(Ai,j)i)\\nj∈Rm.'\n",
            "Chunk 466: page_content='i∈Rn,\\nMincol\\nε(A)def.=(\\nminε(Ai,j)i)\\nj∈Rm. Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-'\n",
            "Chunk 467: page_content='lar (??)). Using these notations, Sinkhorn’s iterates read\\nf(ℓ+1)= Minrow\\nε(C−1ng(ℓ)T) +εloga, (1.65)\\ng(ℓ+1)= Mincol\\nε(C−f(ℓ)1mT) +εlogb.'\n",
            "Chunk 468: page_content='(1.66)'\n",
            "Chunk 469: page_content='g(ℓ+1)= Mincol\\nε(C−f(ℓ)1mT) +εlogb. (1.66)\\nNote that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,'\n",
            "Chunk 470: page_content='because alternate minimization does not converge for constrained problems (which is the case for the un-\\nregularized dual (1.17)).'\n",
            "Chunk 471: page_content='regularized dual (1.17)). Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-'\n",
            "Chunk 472: page_content='tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\\nofε. Writing z = min z, that trick suggests to evaluate min εzas\\nminεz= z−εlog∑'\n",
            "Chunk 473: page_content='minεz= z−εlog∑\\nie−(zi−z)/ε. (1.67)\\nInstead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the'\n",
            "Chunk 474: page_content='previously computed scalings. This leads to the following stabilized iteration\\nf(ℓ+1)= Minrow\\nε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\\ng(ℓ+1)= Mincol\\nε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\\nwhere we deﬁned'\n",
            "Chunk 475: page_content='where we deﬁned\\nS(f,g) =(\\nCi,j−fi−gj)\\ni,j. In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for'\n",
            "Chunk 476: page_content='arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\\nrequiresnmcomputations of exp at each step.'\n",
            "Chunk 477: page_content='Computing a Minrow\\nεor Mincol'\n",
            "Chunk 478: page_content='εor Mincol\\nεis typically substantially\\nslower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is'\n",
            "Chunk 479: page_content='therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.'\n",
            "Chunk 480: page_content='In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\\nεstrategy to signiﬁcantly speed up the computation using sparse grids [ ?]. 24'\n",
            "Chunk 481: page_content='24\\n1.6 Extensions\\nWasserstein Barycenters. Given input histogram {bs}S\\ns=1, wherebs∈Σns, and weights λ∈ΣS, a\\nWasserstein barycenter is computed by minimizing\\nmin\\na∈ΣnS∑\\ns=1λsLCs(a,bs) (1.70)'\n",
            "Chunk 482: page_content='min\\na∈ΣnS∑\\ns=1λsLCs(a,bs) (1.70)\\nwhere the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the'\n",
            "Chunk 483: page_content='barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\\nsolves\\nmin\\na∈ΣnS∑\\ns=1λsWp\\np(a,bs).'\n",
            "Chunk 484: page_content='solves\\nmin\\na∈ΣnS∑\\ns=1λsWp\\np(a,bs). This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?].'\n",
            "Chunk 485: page_content='They proved'\n",
            "Chunk 486: page_content='in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure'\n",
            "Chunk 487: page_content='has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\\none guaranteeing the existence of a Monge map, see Remark ??).'\n",
            "Chunk 488: page_content='The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\\ncouplings ( Ps)sbetween each input and the barycenter itself\\nmin\\na∈Σn,(Ps∈Rn×ns)s{S∑'\n",
            "Chunk 489: page_content='min\\na∈Σn,(Ps∈Rn×ns)s{S∑\\ns=1λs⟨Ps,Cs⟩;∀s,P⊤\\ns1ns=a,P⊤\\ns1n=bs}\\n. Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems.'\n",
            "Chunk 490: page_content='One'\n",
            "Chunk 491: page_content='can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?]. Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,'\n",
            "Chunk 492: page_content='the barycenter problem becomes\\nmin\\nα∈M1\\n+(X)S∑\\ns=1λsLc(α,βs). (1.71)\\nIn the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,'\n",
            "Chunk 493: page_content='then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\\nbarycenters of points ( xs)S'\n",
            "Chunk 494: page_content='barycenters of points ( xs)S\\ns=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a'\n",
            "Chunk 495: page_content='solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\\nof the barycenter α⋆is necessarily the barycenter of the mean, i.e.'\n",
            "Chunk 496: page_content='∫\\nXxdα⋆(x) =∑\\nsλs∫'\n",
            "Chunk 497: page_content='∫\\nXxdα⋆(x) =∑\\nsλs∫\\nXxdαs(x),\\nand the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the'\n",
            "Chunk 498: page_content='approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution'\n",
            "Chunk 499: page_content='using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\\nre-cast (1.71) as a multi-marginal OT problem, see Remark ??.'\n",
            "Chunk 500: page_content='One can use entropic smoothing and approximate the solution of (1.70) using\\nmin\\na∈ΣnS∑\\ns=1λsLε\\nCs(a,bs) (1.72)'\n",
            "Chunk 501: page_content='min\\na∈ΣnS∑\\ns=1λsLε\\nCs(a,bs) (1.72)\\nfor someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient'\n",
            "Chunk 502: page_content='descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\\n25'\n",
            "Chunk 503: page_content='25\\nuseful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness).'\n",
            "Chunk 504: page_content='A simple'\n",
            "Chunk 505: page_content='but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\\nmin\\n(Ps)s{∑\\nsλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\\n(1.73)'\n",
            "Chunk 506: page_content='sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\\n(1.73)\\nwhere we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all'\n",
            "Chunk 507: page_content='the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to'\n",
            "Chunk 508: page_content='this problem, which also corresponds to iterative projection. This can also be seen as a special case of the'\n",
            "Chunk 509: page_content='generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\\nform as\\nPs= diag( us)Kdiag(vs), (1.74)\\nand the scalings are sequentially updated as'\n",
            "Chunk 510: page_content='and the scalings are sequentially updated as\\n∀s∈J1,SK,v(ℓ+1)\\nsdef.=bs\\nKT\\nsu(ℓ)\\ns, (1.75)\\n∀s∈J1,SK,u(ℓ+1)\\nsdef.=a(ℓ+1)\\nKsv(ℓ+1)\\ns, (1.76)\\nwhere a(ℓ+1)def.=∏\\ns(Ksv(ℓ+1)\\ns)λs. (1.77)'\n",
            "Chunk 511: page_content='where a(ℓ+1)def.=∏\\ns(Ksv(ℓ+1)\\ns)λs. (1.77)\\nAn alternative way to derive these iterations is to perform alternate minimization on the variables of a dual'\n",
            "Chunk 512: page_content='problem, which detailed in the following proposition.'\n",
            "Chunk 513: page_content='Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where'\n",
            "Chunk 514: page_content='(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\\nmax\\n(fs,gs)s{∑\\nsλs(\\n⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\\n;∑\\nsλsfs= 0}\\n. (1.78)'\n",
            "Chunk 515: page_content='⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\\n;∑\\nsλsfs= 0}\\n. (1.78)\\nProof.'\n",
            "Chunk 516: page_content='Introducing Lagrange multipliers in (1.73) leads to\\nmin\\n(Ps)s,amax\\n(fs,gs)s∑\\nsλs(\\nεKL(Ps|Ks) +⟨a−Ps1m,fs⟩\\n+⟨bs−PsT1m,gs⟩)\\n.'\n",
            "Chunk 517: page_content='sλs(\\nεKL(Ps|Ks) +⟨a−Ps1m,fs⟩\\n+⟨bs−PsT1m,gs⟩)\\n. Strong duality holds, so that one can exchange the min and the max, and gets\\nmax\\n(fs,gs)s∑\\nsλs(\\n⟨gs,bs⟩+ min\\nPsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\\n+ min\\na⟨∑'\n",
            "Chunk 518: page_content='⟨gs,bs⟩+ min\\nPsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\\n+ min\\na⟨∑\\nsλsfs,a⟩. The explicit minimization on agives the constraint∑\\nsλsfs= 0 together with\\nmax\\n(fs,gs)s∑\\nsλs⟨gs,bs⟩−εKL∗(fs⊕gs\\nε|Ks)'\n",
            "Chunk 519: page_content='max\\n(fs,gs)s∑\\nsλs⟨gs,bs⟩−εKL∗(fs⊕gs\\nε|Ks)\\nwhere KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\\nKL∗(U|K) =∑\\ni,jKi,j(eUi,j−1), (1.79)\\n26'\n",
            "Chunk 520: page_content='KL∗(U|K) =∑\\ni,jKi,j(eUi,j−1), (1.79)\\n26\\nFigure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights'\n",
            "Chunk 521: page_content='(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\\nare uniform within the boundaries of the shape and null outside.'\n",
            "Chunk 522: page_content='which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\\n∀(u,k)∈R2\\n+,KL∗(u|k)def.= max\\nrur−(rlog(r/k)−r+k)'\n",
            "Chunk 523: page_content='∀(u,k)∈R2\\n+,KL∗(u|k)def.= max\\nrur−(rlog(r/k)−r+k)\\nwhose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.'\n",
            "Chunk 524: page_content='Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed'\n",
            "Chunk 525: page_content='form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\\nto the expression (1.76).'\n",
            "Chunk 526: page_content='to the expression (1.76). Figures ??and??show applications to 2-D and 3-D shapes interpolation.'\n",
            "Chunk 527: page_content='Figure ??shows a computation'\n",
            "Chunk 528: page_content='of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,'\n",
            "Chunk 529: page_content='the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\\nto [?] for more details and other applications to computer graphics and imaging sciences.'\n",
            "Chunk 530: page_content='Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability'\n",
            "Chunk 531: page_content='distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈'\n",
            "Chunk 532: page_content='Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”'\n",
            "Chunk 533: page_content='term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\\nsuitable parameter θis obtained by minimizing directly\\nmin\\nθ∈ΘE(θ)def.=Lc(αθ,β).'\n",
            "Chunk 534: page_content='(1.80)'\n",
            "Chunk 535: page_content='min\\nθ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\\nOf course, one can consider more complicated problems: for instance, the barycenter problem described'\n",
            "Chunk 536: page_content='in§??consists in a sum of such terms. However, most of these more advanced problems can be usually'\n",
            "Chunk 537: page_content='solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\\nor using automatic diﬀerentiation.'\n",
            "Chunk 538: page_content='or using automatic diﬀerentiation. The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,'\n",
            "Chunk 539: page_content='as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\\nΣnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K'\n",
            "Chunk 540: page_content='i=1θiαi\\nis a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case'\n",
            "Chunk 541: page_content='corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with'\n",
            "Chunk 542: page_content='a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\\nnot convex.'\n",
            "Chunk 543: page_content='27\\ng✓XZ⇣xz\\x00↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.'\n",
            "Chunk 544: page_content='A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\\nsome discrete samples ( xi)n'\n",
            "Chunk 545: page_content='some discrete samples ( xi)n\\ni=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\\nθ↦→αθ∈M (X) to the observed empirical input measure β\\nmin\\nθ∈ΘL(αθ,β) where β=1\\nn∑\\niδxi, (1.81)'\n",
            "Chunk 546: page_content='min\\nθ∈ΘL(αθ,β) where β=1\\nn∑\\niδxi, (1.81)\\nwhereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\\nure 1.16).'\n",
            "Chunk 547: page_content='ure 1.16). In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\\nreference measure), the maximum likelihood estimator (MLE) is obtained by solving\\nmin'\n",
            "Chunk 548: page_content='min\\nθLMLE(αθ,β)def.=−∑\\nilog(ρθ(xi)). This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.'\n",
            "Chunk 549: page_content='samples of some ¯β, then\\nLMLE(α,β)n→+∞−→ KL(α|¯β)'\n",
            "Chunk 550: page_content='samples of some ¯β, then\\nLMLE(α,β)n→+∞−→ KL(α|¯β)\\nThis MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).'\n",
            "Chunk 551: page_content='However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density'\n",
            "Chunk 552: page_content='(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share'\n",
            "Chunk 553: page_content='the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that'\n",
            "Chunk 554: page_content='in several cases of practical interest, the density ρθis inaccessible (or too hard to compute). A typical setup where both problems (singular and unknown densities) occur is for so-called generative'\n",
            "Chunk 555: page_content='models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\\nαθ=hθ,♯ζwherehθ:Z→X'\n",
            "Chunk 556: page_content='αθ=hθ,♯ζwherehθ:Z→X\\nwhere the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so'\n",
            "Chunk 557: page_content='that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly'\n",
            "Chunk 558: page_content='singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density'\n",
            "Chunk 559: page_content='is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\\n(zi)iare i.i.d.'\n",
            "Chunk 560: page_content='samples from ζ.'\n",
            "Chunk 561: page_content='(zi)iare i.i.d.'\n",
            "Chunk 562: page_content='samples from ζ. In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\\nLMLE, which needs to be written in dual form as\\nL(α,β)def.= max'\n",
            "Chunk 563: page_content='L(α,β)def.= max\\n(f,g)∈C(X)2{∫\\nXf(x)dα(x) +∫\\nXg(x)dβ(x) ; (f,g)∈R}\\n. (1.82)\\nDual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)'\n",
            "Chunk 564: page_content='setsR=R(c) as deﬁned in (1.22). 28\\nFor a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to'\n",
            "Chunk 565: page_content='solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\\nrespect toθis much more involved, and is typically highly non-convex.'\n",
            "Chunk 566: page_content='The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\\nwas initially introduced in [ ?], see also [ ?].'\n",
            "Chunk 567: page_content='was initially introduced in [ ?], see also [ ?].'\n",
            "Chunk 568: page_content='Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can'\n",
            "Chunk 569: page_content='thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register'\n",
            "Chunk 570: page_content='these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,'\n",
            "Chunk 571: page_content='namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship'\n",
            "Chunk 572: page_content='between the points on which the histograms are deﬁned.'\n",
            "Chunk 573: page_content='A typical scenario is when these matrices are (power\\nof) distance matrices. The Gromov-Wasserstein problem reads\\nGW(( a,D),(b,D′))2def.= min'\n",
            "Chunk 574: page_content='GW(( a,D),(b,D′))2def.= min\\nP∈U(a,b)ED,D′(P)def.=∑\\ni,j,i′,j′|Di,i′−D′\\nj,j′|2Pi,jPi′,j′. (1.83)\\nThis is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in'\n",
            "Chunk 575: page_content='full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\\nfor a particular cost.'\n",
            "Chunk 576: page_content='for a particular cost. One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between'\n",
            "Chunk 577: page_content='metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))'\n",
            "Chunk 578: page_content='up to isometries preserving the measures. This distance was introduced and studied in details by Memoli'\n",
            "Chunk 579: page_content='in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given'\n",
            "Chunk 580: page_content='in [?].'\n",
            "Chunk 581: page_content='See also [ ?] for applications in computer vision. This distance is also tightly connected with the'\n",
            "Chunk 582: page_content='Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].'\n",
            "Chunk 583: page_content='Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between'\n",
            "Chunk 584: page_content='metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\\non their respective spaces. One deﬁnes\\nGW((αX,dX),(αY,dY))2def.= min\\nπ∈U(αX,αY)∫'\n",
            "Chunk 585: page_content='GW((αX,dX),(αY,dY))2def.= min\\nπ∈U(αX,αY)∫\\nX2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\\nGW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and'\n",
            "Chunk 586: page_content='(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′). Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with'\n",
            "Chunk 587: page_content='thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and'\n",
            "Chunk 588: page_content='(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\\n((x0,x1),(x′\\n0,x′\\n1))∈(X0×X 1)2,\\ndt((x0,x1),(x′\\n0,x′\\n1))def.= (1−t)dX0(x0,x′\\n0) +tdX1(x1,x′\\n1).'\n",
            "Chunk 589: page_content='0,x′\\n1))def.= (1−t)dX0(x0,x′\\n0) +tdX1(x1,x′\\n1). This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric'\n",
            "Chunk 590: page_content='spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product'\n",
            "Chunk 591: page_content='spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure'\n",
            "Chunk 592: page_content='spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\\ndetailed below.'\n",
            "Chunk 593: page_content='detailed below. To approximate the computation of GW, and to help convergence of minimization schemes to better\\nminima, one can consider the entropic regularized variant\\nmin'\n",
            "Chunk 594: page_content='min\\nP∈U(a,b)ED,D′(P)−εH(P). (1.85)\\n29\\nFigure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\\niterations (1.86).'\n",
            "Chunk 595: page_content='Extracted from [ ?].'\n",
            "Chunk 596: page_content='iterations (1.86).'\n",
            "Chunk 597: page_content='Extracted from [ ?]. As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively'\n",
            "Chunk 598: page_content='Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\\nof the objective function lead to consider the succession of updates\\nP(ℓ+1) def.= min'\n",
            "Chunk 599: page_content='P(ℓ+1) def.= min\\nP∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\\nC(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\\nwhich can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn'\n",
            "Chunk 600: page_content='iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\\ncompute soft maps between domains.'\n",
            "Chunk 601: page_content='30\\nBibliography'\n",
            "Chunk 602: page_content='30\\nBibliography\\n[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\\nLAB.'\n",
            "Chunk 603: page_content='SIAM, 2014.'\n",
            "Chunk 604: page_content='LAB.'\n",
            "Chunk 605: page_content='SIAM, 2014. [2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization'\n",
            "Chunk 606: page_content='and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\\nin Machine Learning , 3(1):1–122, 2011.'\n",
            "Chunk 607: page_content='in Machine Learning , 3(1):1–122, 2011. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization .'\n",
            "Chunk 608: page_content='Cambridge university press, 2004.'\n",
            "Chunk 609: page_content='[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\\npiecewise C2singularities. Commun.'\n",
            "Chunk 610: page_content='on Pure and Appl. Math. , 57(2):219–266, 2004.'\n",
            "Chunk 611: page_content='[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L.'\n",
            "Chunk 612: page_content='Ying. Fast discrete curvelet transforms. SIAM\\nMultiscale Modeling and Simulation , 5:861–899, 2005.'\n",
            "Chunk 613: page_content='[6] A. Chambolle. An algorithm for total variation minimization and applications. J.'\n",
            "Chunk 614: page_content='Math. Imaging Vis. ,\\n20:89–97, 2004.'\n",
            "Chunk 615: page_content='20:89–97, 2004. [7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock.'\n",
            "Chunk 616: page_content='An intro-'\n",
            "Chunk 617: page_content='duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\\nrecovery , 9(263-340):227, 2010.'\n",
            "Chunk 618: page_content='recovery , 9(263-340):227, 2010.'\n",
            "Chunk 619: page_content='[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\\nNumerica , 25:161–319, 2016.'\n",
            "Chunk 620: page_content='Numerica , 25:161–319, 2016. [9] S.S. Chen, D.L. Donoho, and M.A.'\n",
            "Chunk 621: page_content='Saunders. Atomic decomposition by basis pursuit. SIAM Journal\\non Scientiﬁc Computing , 20(1):33–61, 1999.'\n",
            "Chunk 622: page_content='on Scientiﬁc Computing , 20(1):33–61, 1999. [10] Philippe G Ciarlet.'\n",
            "Chunk 623: page_content='Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.'\n",
            "Chunk 624: page_content='[11] P. L. Combettes and V. R.'\n",
            "Chunk 625: page_content='Wajs. Signal recovery by proximal forward-backward splitting. SIAM\\nMultiscale Modeling and Simulation , 4(4), 2005.'\n",
            "Chunk 626: page_content='Multiscale Modeling and Simulation , 4(4), 2005. [12] I. Daubechies, M. Defrise, and C.'\n",
            "Chunk 627: page_content='De Mol. An iterative thresholding algorithm for linear inverse problems'\n",
            "Chunk 628: page_content='with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004. [13] D.'\n",
            "Chunk 629: page_content='Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\\nDec 1994.'\n",
            "Chunk 630: page_content='Dec 1994.'\n",
            "Chunk 631: page_content='[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\\n375. Springer Science & Business Media, 1996.'\n",
            "Chunk 632: page_content='375. Springer Science & Business Media, 1996. [15] M. Figueiredo and R.'\n",
            "Chunk 633: page_content='Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans. Image Proc. , 12(8):906–916, 2003.'\n",
            "Chunk 634: page_content='Image Proc. , 12(8):906–916, 2003. [16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1. Birkh¨ auser Basel, 2013.'\n",
            "Chunk 635: page_content='31'\n",
            "Chunk 636: page_content='Birkh¨ auser Basel, 2013.'\n",
            "Chunk 637: page_content='31\\n[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.'\n",
            "Chunk 638: page_content='[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\\ntional problems. Commun.'\n",
            "Chunk 639: page_content='on Pure and Appl. Math. , 42:577–685, 1989.'\n",
            "Chunk 640: page_content='[19] Neal Parikh, Stephen Boyd, et al.'\n",
            "Chunk 641: page_content='Proximal algorithms. Foundations and Trends R⃝in Optimization ,\\n1(3):127–239, 2014.'\n",
            "Chunk 642: page_content='1(3):127–239, 2014.'\n",
            "Chunk 643: page_content='[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.'\n",
            "Chunk 644: page_content='[21] J. Portilla, V. Strela, M.J.'\n",
            "Chunk 645: page_content='Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\\nGaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.'\n",
            "Chunk 646: page_content='[22] L. I. Rudin, S. Osher, and E.'\n",
            "Chunk 647: page_content='Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4):259–268, 1992.'\n",
            "Chunk 648: page_content='D, 60(1-4):259–268, 1992.'\n",
            "Chunk 649: page_content='[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich. Variational methods in imaging . Springer, 2009.'\n",
            "Chunk 650: page_content='Variational methods in imaging . Springer, 2009.'\n",
            "Chunk 651: page_content='[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\\n27(3):379–423, 1948.'\n",
            "Chunk 652: page_content='27(3):379–423, 1948.'\n",
            "Chunk 653: page_content='[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\\nrelated geometric multiscale analysis . Cambridge university press, 2015.'\n",
            "Chunk 654: page_content='32'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhuUQzmvLzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"LatexTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qzCTCiPpLKn",
        "outputId": "021ab908-fc92-438f-c381-18ac2c201642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14,\n",
            "Chunk 2: 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability\n",
            "Chunk 3: any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at\n",
            "Chunk 4: reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if,\n",
            "Chunk 5: as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures)\n",
            "Chunk 6: ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be\n",
            "Chunk 7: the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped\n",
            "Chunk 8: deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted\n",
            "Chunk 9: against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the\n",
            "Chunk 10: measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x)\n",
            "Chunk 11: ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be\n",
            "Chunk 12: of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at\n",
            "Chunk 13: also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth\n",
            "Chunk 14: than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability\n",
            "Chunk 15: positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the\n",
            "Chunk 16: 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic\n",
            "Chunk 17: 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and\n",
            "Chunk 18: ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to\n",
            "Chunk 19: using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward\n",
            "Chunk 20: continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the\n",
            "Chunk 21: in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a\n",
            "Chunk 22: a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y ,\n",
            "Chunk 23: follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one\n",
            "Chunk 24: for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be\n",
            "Chunk 25: a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability\n",
            "Chunk 26: extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain\n",
            "Chunk 27: of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear\n",
            "Chunk 28: a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the\n",
            "Chunk 29: for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities\n",
            "Chunk 30: shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the\n",
            "Chunk 31: the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward\n",
            "Chunk 32: of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the\n",
            "Chunk 33: T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and\n",
            "Chunk 34: deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if\n",
            "Chunk 35: is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to\n",
            "Chunk 36: explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and\n",
            "Chunk 37: the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random\n",
            "Chunk 38: as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the\n",
            "Chunk 39: space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y ,\n",
            "Chunk 40: Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random\n",
            "Chunk 41: thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in\n",
            "Chunk 42: variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the\n",
            "Chunk 43: cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One\n",
            "Chunk 44: solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for\n",
            "Chunk 45: set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there\n",
            "Chunk 46: problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of\n",
            "Chunk 47: set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either\n",
            "Chunk 48: measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The\n",
            "Chunk 49: blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas\n",
            "Chunk 50: is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance\n",
            "Chunk 51: several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in\n",
            "Chunk 52: square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj\n",
            "Chunk 53: discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β,\n",
            "Chunk 54: must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This\n",
            "Chunk 55: we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ".\n",
            "Chunk 56: points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the\n",
            "Chunk 57: using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass\n",
            "Chunk 58: are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5)\n",
            "Chunk 59: equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to\n",
            "Chunk 60: not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source\n",
            "Chunk 61: measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is\n",
            "Chunk 62: no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x)\n",
            "Chunk 63: map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch\n",
            "Chunk 64: the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the\n",
            "Chunk 65: when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization\n",
            "Chunk 66: clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be\n",
            "Chunk 67: maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment\n",
            "Chunk 68: end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass\n",
            "Chunk 69: all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete\n",
            "Chunk 70: formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or\n",
            "Chunk 71: point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several\n",
            "Chunk 72: point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”)\n",
            "Chunk 73: consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in\n",
            "Chunk 74: targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or\n",
            "Chunk 75: ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge\n",
            "Chunk 76: admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand\n",
            "Chunk 77: matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a\n",
            "Chunk 78: therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric,\n",
            "Chunk 79: plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal\n",
            "Chunk 80: and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the\n",
            "Chunk 81: program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black\n",
            "Chunk 82: optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the\n",
            "Chunk 83: Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a\n",
            "Chunk 84: point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor\n",
            "Chunk 85: For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which\n",
            "Chunk 86: that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation\n",
            "Chunk 87: Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the\n",
            "Chunk 88: permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a\n",
            "Chunk 89: whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in\n",
            "Chunk 90: proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures\n",
            "Chunk 91: problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of\n",
            "Chunk 92: Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution\n",
            "Chunk 93: then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that\n",
            "Chunk 94: (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7]\n",
            "Chunk 95: theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of\n",
            "Chunk 96: if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β)\n",
            "Chunk 97: of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous”\n",
            "Chunk 98: setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black).\n",
            "Chunk 99: of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at\n",
            "Chunk 100: entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary\n",
            "Chunk 101: of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one\n",
            "Chunk 102: discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as\n",
            "Chunk 103: constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition\n",
            "Chunk 104: the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures\n",
            "Chunk 105: for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The\n",
            "Chunk 106: =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over\n",
            "Chunk 107: is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D\n",
            "Chunk 108: Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so\n",
            "Chunk 109: a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7:\n",
            "Chunk 110: c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-*\n",
            "Chunk 111: and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein\n",
            "Chunk 112: impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes\n",
            "Chunk 113: measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram\n",
            "Chunk 114: between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs\n",
            "Chunk 115: matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance\n",
            "Chunk 116: states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on\n",
            "Chunk 117: (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that\n",
            "Chunk 118: LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle\n",
            "Chunk 119: if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null\n",
            "Chunk 120: distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0\n",
            "Chunk 121: of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is\n",
            "Chunk 122: the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing\n",
            "Chunk 123: measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is\n",
            "Chunk 124: explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and\n",
            "Chunk 125: bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c)\n",
            "Chunk 126: remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0\n",
            "Chunk 127: that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c)\n",
            "Chunk 128: =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/\n",
            "Chunk 129: jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "=\n",
            "Chunk 130: Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2\n",
            "Chunk 131: comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis\n",
            "Chunk 132: and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y)\n",
            "Chunk 133: and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0\n",
            "Chunk 134: on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as\n",
            "Chunk 135: The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein\n",
            "Chunk 136: between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for\n",
            "Chunk 137: allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or\n",
            "Chunk 138: In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base\n",
            "Chunk 139: measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y).\n",
            "Chunk 140: one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily (\n",
            "Chunk 141: feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now\n",
            "Chunk 142: a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function\n",
            "Chunk 143: ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be\n",
            "Chunk 144: vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists\n",
            "Chunk 145: unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k)\n",
            "Chunk 146: norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of\n",
            "Chunk 147: one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X),\n",
            "Chunk 148: is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear\n",
            "Chunk 149: it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the\n",
            "Chunk 150: option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being\n",
            "Chunk 151: optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is\n",
            "Chunk 152: 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11)\n",
            "Chunk 153: Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization\n",
            "Chunk 154: which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal\n",
            "Chunk 155: explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials\n",
            "Chunk 156: (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for\n",
            "Chunk 157: the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L\n",
            "Chunk 158: side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to\n",
            "Chunk 159: of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the\n",
            "Chunk 160: always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking\n",
            "Chunk 161: min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian\n",
            "Chunk 162: optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to\n",
            "Chunk 163: (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be\n",
            "Chunk 164: with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4\n",
            "Chunk 165: proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual\n",
            "Chunk 166: (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich\n",
            "Chunk 167: functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The\n",
            "Chunk 168: potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}.\n",
            "Chunk 169: as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is\n",
            "Chunk 170: non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal (\n",
            "Chunk 171: Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following\n",
            "Chunk 172: Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are\n",
            "Chunk 173: density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith\n",
            "Chunk 174: inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge\n",
            "Chunk 175: and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as\n",
            "Chunk 176: this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related\n",
            "Chunk 177: that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance\n",
            "Chunk 178: the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus\n",
            "Chunk 179: Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation\n",
            "Chunk 180: ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x).\n",
            "Chunk 181: by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27)\n",
            "Chunk 182: ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint\n",
            "Chunk 183: ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice,\n",
            "Chunk 184: costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported\n",
            "Chunk 185: (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition\n",
            "Chunk 186: Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This\n",
            "Chunk 187: is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows\n",
            "Chunk 188: that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is\n",
            "Chunk 189: relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is\n",
            "Chunk 190: the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map\n",
            "Chunk 191: theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of\n",
            "Chunk 192: is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions,\n",
            "Chunk 193: deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The\n",
            "Chunk 194: theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1\n",
            "Chunk 195: sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using\n",
            "Chunk 196: function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type\n",
            "Chunk 197: which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as\n",
            "Chunk 198: ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for\n",
            "Chunk 199: the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special\n",
            "Chunk 200: for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is\n",
            "Chunk 201: where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when\n",
            "Chunk 202: zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result\n",
            "Chunk 203: One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to\n",
            "Chunk 204: two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical\n",
            "Chunk 205: Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸=\n",
            "Chunk 206: rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without\n",
            "Chunk 207: assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes\n",
            "Chunk 208: (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those\n",
            "Chunk 209: locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below.\n",
            "Chunk 210: of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone\n",
            "Chunk 211: of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?].\n",
            "Chunk 212: compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?],\n",
            "Chunk 213: transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative\n",
            "Chunk 214: .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞}\n",
            "Chunk 215: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one\n",
            "Chunk 216: quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a\n",
            "Chunk 217: the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the\n",
            "Chunk 218: real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in\n",
            "Chunk 219: which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx.\n",
            "Chunk 220: (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9:\n",
            "Chunk 221: ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm\n",
            "Chunk 222: detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates\n",
            "Chunk 223: by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??.\n",
            "Chunk 224: as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians)\n",
            "Chunk 225: Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα),\n",
            "Chunk 226: can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is\n",
            "Chunk 227: that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "=\n",
            "Chunk 228: det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover\n",
            "Chunk 229: detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [\n",
            "Chunk 230: x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4\n",
            "Chunk 231: illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance\n",
            "Chunk 232: densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the\n",
            "Chunk 233: random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting\n",
            "Chunk 234: interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving\n",
            "Chunk 235: additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?]\n",
            "Chunk 236: the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that\n",
            "Chunk 237: Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ=\n",
            "Chunk 238: arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the\n",
            "Chunk 239: W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5\n",
            "Chunk 240: of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many\n",
            "Chunk 241: formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages,\n",
            "Chunk 242: regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme\n",
            "Chunk 243: simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance\n",
            "Chunk 244: of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on\n",
            "Chunk 245: of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned\n",
            "Chunk 246: discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or\n",
            "Chunk 247: that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of\n",
            "Chunk 248: The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.=\n",
            "Chunk 249: original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to\n",
            "Chunk 250: 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns\n",
            "Chunk 251: theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to\n",
            "Chunk 252: are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation\n",
            "Chunk 253: balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the\n",
            "Chunk 254: 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution\n",
            "Chunk 255: how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the\n",
            "Chunk 256: of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been\n",
            "Chunk 257: solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the\n",
            "Chunk 258: optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in\n",
            "Chunk 259: ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution\n",
            "Chunk 260: thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is\n",
            "Chunk 261: of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one\n",
            "Chunk 262: optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating\n",
            "Chunk 263: and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal\n",
            "Chunk 264: number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that\n",
            "Chunk 265: the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is\n",
            "Chunk 266: shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is\n",
            "Chunk 267: of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp\n",
            "Chunk 268: entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b,\n",
            "Chunk 269: entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed\n",
            "Chunk 270: reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key\n",
            "Chunk 271: the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which\n",
            "Chunk 272: larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in\n",
            "Chunk 273: to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof\n",
            "Chunk 274: (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε=\n",
            "Chunk 275: one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete\n",
            "Chunk 276: arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14)\n",
            "Chunk 277: and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler\n",
            "Chunk 278: a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a\n",
            "Chunk 279: by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL(\n",
            "Chunk 280: to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the\n",
            "Chunk 281: problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was\n",
            "Chunk 282: Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to\n",
            "Chunk 283: converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two\n",
            "Chunk 284: the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using\n",
            "Chunk 285: a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but\n",
            "Chunk 286: that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable\n",
            "Chunk 287: (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g)\n",
            "Chunk 288: the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to\n",
            "Chunk 289: results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors\n",
            "Chunk 290: in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust\n",
            "Chunk 291: in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and\n",
            "Chunk 292: inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times\n",
            "Chunk 293: v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as\n",
            "Chunk 294: is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by\n",
            "Chunk 295: equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s\n",
            "Chunk 296: side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between\n",
            "Chunk 297: v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000\n",
            "Chunk 298: lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D\n",
            "Chunk 299: at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for\n",
            "Chunk 300: violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these\n",
            "Chunk 301: anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same\n",
            "Chunk 302: contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It\n",
            "Chunk 303: computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with\n",
            "Chunk 304: away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use\n",
            "Chunk 305: one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these\n",
            "Chunk 306: (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since\n",
            "Chunk 307: equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice\n",
            "Chunk 308: def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be\n",
            "Chunk 309: against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly\n",
            "Chunk 310: convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be\n",
            "Chunk 311: log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”).\n",
            "Chunk 312: up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance\n",
            "Chunk 313: projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?]\n",
            "Chunk 314: It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s\n",
            "Chunk 315: linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive\n",
            "Chunk 316: is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.=\n",
            "Chunk 317: max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3.\n",
            "Chunk 318: convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also\n",
            "Chunk 319: dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one\n",
            "Chunk 320: P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one\n",
            "Chunk 321: One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2.\n",
            "Chunk 322: we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ))\n",
            "Chunk 323: triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ))\n",
            "Chunk 324: gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal\n",
            "Chunk 325: shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row,\n",
            "Chunk 326: monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly\n",
            "Chunk 327: These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??).\n",
            "Chunk 328: the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling\n",
            "Chunk 329: increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One\n",
            "Chunk 330: problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) =\n",
            "Chunk 331: scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers\n",
            "Chunk 332: optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of\n",
            "Chunk 333: of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε,\n",
            "Chunk 334: (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of\n",
            "Chunk 335: be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in\n",
            "Chunk 336: term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input\n",
            "Chunk 337: For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a\n",
            "Chunk 338: dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of\n",
            "Chunk 339: existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and\n",
            "Chunk 340: OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark\n",
            "Chunk 341: iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an\n",
            "Chunk 342: maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can\n",
            "Chunk 343: to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block\n",
            "Chunk 344: (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0),\n",
            "Chunk 345: starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations\n",
            "Chunk 346: equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark\n",
            "Chunk 347: =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we\n",
            "Chunk 348: notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min\n",
            "Chunk 349: to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i=\n",
            "Chunk 350: (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of\n",
            "Chunk 351: the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the\n",
            "Chunk 352: as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we\n",
            "Chunk 353: or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform\n",
            "Chunk 354: are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)=\n",
            "Chunk 355: Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate\n",
            "Chunk 356: anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While\n",
            "Chunk 357: (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid\n",
            "Chunk 358: use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting\n",
            "Chunk 359: (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized\n",
            "Chunk 360: scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g)\n",
            "Chunk 361: (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity\n",
            "Chunk 362: stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis\n",
            "Chunk 363: at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no\n",
            "Chunk 364: soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is\n",
            "Chunk 365: Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6\n",
            "Chunk 366: the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by\n",
            "Chunk 367: λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all\n",
            "Chunk 368: A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter\n",
            "Chunk 369: barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input\n",
            "Chunk 370: for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a\n",
            "Chunk 371: as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween\n",
            "Chunk 372: one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids\n",
            "Chunk 373: this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark\n",
            "Chunk 374: as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem\n",
            "Chunk 375: on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this\n",
            "Chunk 376: of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary\n",
            "Chunk 377: of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the\n",
            "Chunk 378: ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of\n",
            "Chunk 379: the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input\n",
            "Chunk 380: optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a\n",
            "Chunk 381: note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs)\n",
            "Chunk 382: of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent\n",
            "Chunk 383: [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some\n",
            "Chunk 384: on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks)\n",
            "Chunk 385: KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings\n",
            "Chunk 386: encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can\n",
            "Chunk 387: corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form\n",
            "Chunk 388: ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s,\n",
            "Chunk 389: updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to\n",
            "Chunk 390: alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal\n",
            "Chunk 391: following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the\n",
            "Chunk 392: of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads\n",
            "Chunk 393: Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and\n",
            "Chunk 394: so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0\n",
            "Chunk 395: minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre\n",
            "Chunk 396: ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The\n",
            "Chunk 397: shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of\n",
            "Chunk 398: that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.=\n",
            "Chunk 399: one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while\n",
            "Chunk 400: (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77)\n",
            "Chunk 401: all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters\n",
            "Chunk 402: Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat\n",
            "Chunk 403: are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In\n",
            "Chunk 404: and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized\n",
            "Chunk 405: measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in\n",
            "Chunk 406: through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing\n",
            "Chunk 407: a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem\n",
            "Chunk 408: problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic\n",
            "Chunk 409: usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two\n",
            "Chunk 410: Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram,\n",
            "Chunk 411: when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin\n",
            "Chunk 412: a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem\n",
            "Chunk 413: one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16:\n",
            "Chunk 414: in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting.\n",
            "Chunk 415: statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical\n",
            "Chunk 416: model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see\n",
            "Chunk 417: and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum\n",
            "Chunk 418: any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a\n",
            "Chunk 419: to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal\n",
            "Chunk 420: MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not\n",
            "Chunk 421: distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be\n",
            "Chunk 422: share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to\n",
            "Chunk 423: the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is\n",
            "Chunk 424: where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually\n",
            "Chunk 425: introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not\n",
            "Chunk 426: resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from\n",
            "Chunk 427: intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics\n",
            "Chunk 428: diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ".\n",
            "Chunk 429: +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the\n",
            "Chunk 430: deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter\n",
            "Chunk 431: optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc,\n",
            "Chunk 432: class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground\n",
            "Chunk 433: Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces\n",
            "Chunk 434: space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices\n",
            "Chunk 435: that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are\n",
            "Chunk 436: A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.=\n",
            "Chunk 437: problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP)\n",
            "Chunk 438: be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular\n",
            "Chunk 439: to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability\n",
            "Chunk 440: between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in\n",
            "Chunk 441: This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for\n",
            "Chunk 442: gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have\n",
            "Chunk 443: distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric\n",
            "Chunk 444: corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One\n",
            "Chunk 445: are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up\n",
            "Chunk 446: a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark\n",
            "Chunk 447: thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows\n",
            "Chunk 448: (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for\n",
            "Chunk 449: 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze\n",
            "Chunk 450: formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over\n",
            "Chunk 451: because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the\n",
            "Chunk 452: while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization\n",
            "Chunk 453: of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy\n",
            "Chunk 454: (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later\n",
            "Chunk 455: [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85).\n",
            "Chunk 456: compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where\n",
            "Chunk 457: def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations\n",
            "Chunk 458: can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1]\n",
            "Chunk 459: soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric\n",
            "Chunk 460: SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers.\n",
            "Chunk 461: the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university\n",
            "Chunk 462: Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on\n",
            "Chunk 463: with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale\n",
            "Chunk 464: discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis.\n",
            "Chunk 465: and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image\n",
            "Chunk 466: An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An\n",
            "Chunk 467: 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic\n",
            "Chunk 468: S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´\n",
            "Chunk 469: G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale\n",
            "Chunk 470: forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a\n",
            "Chunk 471: algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet\n",
            "Chunk 472: Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems ,\n",
            "Chunk 473: Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE\n",
            "Chunk 474: for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser\n",
            "Chunk 475: to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal\n",
            "Chunk 476: press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal\n",
            "Chunk 477: and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr`\n",
            "Chunk 478: 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale\n",
            "Chunk 479: and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi.\n",
            "Chunk 480: 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer,\n",
            "Chunk 481: Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of\n",
            "Chunk 482: C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal\n",
            "Chunk 483: and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"SentenceTransformersTokenTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")\n"
      ],
      "metadata": {
        "id": "n203jCDsnKIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "36un5gZLleJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23nXLdrOmfbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_overlap(text, chunk_size=500, chunk_overlap=40,splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_overlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "10PLJKJR1cQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_nonoveralp(text, chunk_size=500,splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_nonoverlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "tpHaI24h1sI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPuKFgO0AS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "Y37xIl_nATRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYfQlZCjArl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "7oiY0B-aAro-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "SldDgWylAsFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdwXXyzVb36K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArUE-fNXb39a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ED-HH9jtc02B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#HTMLHeaderTextSplitter\n",
        "\n",
        "from langchain_text_splitters.html import HTMLHeaderTextSplitter\n",
        "\n",
        "# Sample HTML document\n",
        "html_document = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample HTML Document</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Introduction</h1>\n",
        "    <p>This is the introductory paragraph.</p>\n",
        "\n",
        "    <h2>Section 1</h2>\n",
        "    <p>This is the content of section 1.</p>\n",
        "\n",
        "    <h2>Section 2</h2>\n",
        "    <p>This is the content of section 2.</p>\n",
        "\n",
        "    <h3>Subsection 2.1</h3>\n",
        "    <p>This is the content of subsection 2.1.</p>\n",
        "\n",
        "    <h2>Section 3</h2>\n",
        "    <p>This is the content of section 3.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the HTMLHeaderTextSplitter with header tags to split on\n",
        "splitter = HTMLHeaderTextSplitter(headers_to_split_on=[\"h1\", \"h2\", \"h3\"])\n",
        "\n",
        "# Split the HTML document\n",
        "split_chunks = splitter.split_text(html_document)\n",
        "\n",
        "# Print the split chunks\n",
        "for i, chunk in enumerate(split_chunks):\n",
        "    print(f\"Chunk {i + 1}:\\n{chunk}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSlOq5rJc04z",
        "outputId": "8a4c3605-a728-4d3d-9ee7-22ea583d4c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "page_content='This is the introductory paragraph.  \\nThis is the content of section 1.  \\nThis is the content of section 2.  \\nThis is the content of subsection 2.1.  \\nThis is the content of section 3.'\n",
            "\n"
          ]
        }
      ]
    }
  ]
}