
# Initialize counters and sums
correct_matches = 0  # Total correctly matched paragraphs
matched_completeness_sum = 0  # Sum of completeness scores for matched paragraphs
matched_count = 0  # Count of matched paragraphs
total_gt_paragraphs = len(ground_truth_paragraphs)  # Total ground truth paragraphs
completeness_sum = 0  # Sum of completeness scores for all predictions

# Process predictions and ground truth
for gt_para, predicted_para in zip(ground_truth_paragraphs, predicted_paragraphs):
    # Find best match score and index for each ground truth paragraph
    best_match_score, best_match_idx = get_best_match(gt_para, predicted_para)

    if best_match_score > 0:  # Match found
        correct_matches += 1
        matched_completeness_sum += best_match_score  # Only for matched paragraphs
        matched_count += 1

    completeness_sum += best_match_score  # Sum completeness for all paragraphs

    # Append row-wise results
    result_rows.append({
        "groundTruth": gt_para,
        "predicted": predicted_para if predicted_para else "",
        "extracted": best_match_score > 0,  # True if a match exists
        "matching_ai_index": best_match_idx if best_match_score > 0 else -1,
        "completeness": best_match_score,
        "correctness": 1 if best_match_score > 0 else 0
    })

# Process extra ground truth paragraphs not matched with any prediction
extra_gts = [gt for gt in ground_truth_paragraphs if gt not in [row["groundTruth"] for row in result_rows]]

for pr_gt in extra_gts:
    result_rows.append({
        "groundTruth": pr_gt,
        "predicted": None,
        "extracted": False,
        "matching_ai_index": None,
        "completeness": 0,
        "correctness": 0
    })

# Calculate Overall Metrics
overall_correctness = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
overall_completeness = (
    matched_completeness_sum / matched_count if matched_count > 0 else 0
)

# Calculate Recall and Precision
count_based_recall = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
count_based_precision = correct_matches / len(predicted_paragraphs) if len(predicted_paragraphs) > 0 else 0

# Calculate Business Score
business_score = (0.6 * overall_correctness) + (0.4 * overall_completeness)

# Summary of results
summary = {
    "Overall Correctness": overall_correctness,
    "Overall Completeness": overall_completeness,
    "Recall": count_based_recall,
    "Precision": count_based_precision,
    "Business Score": business_score
}

# Return final result rows and summary
return result_rows, summary






Algorithm for the Document Processing Workflow
Here's a step-by-step algorithm that outlines the complete process for the document processing workflow using Azure and AI services:

Initialize Connections and Load Configurations

Establish connections to Azure Blob Storage using managed identity configurations (pm_azure_blob_connection and di_azure_blob_connection).
Load configurations for AI models, regex patterns, and document processing settings.
Input Document Acquisition

Retrieve the input document from Azure Blob Storage using the specified connection details.
Validate the document's format and accessibility.
Artifact Generation and Initialization

Initialize artifacts to capture essential document metadata:
Page Dimensions: Record the size of each page in the document.
Document Layout: Identify structural elements such as text blocks, tables, and images.
Logical Chunks: Segment the document into manageable sections for targeted analysis.
Document Parsing using Azure Document Intelligence

Parse the document using the Azure Cognitive Services model (prebuilt-layout).
Extract and store the document layout, page dimensions, and logical chunks for further processing.
Regex Filtering of Logical Chunks

Apply regex patterns to the logical chunks to filter out sections containing specific financial keywords.
Extract chunks that match the keywords such as “Management Fee,” “Compensation,” and other relevant terms.
AI Filtering of Extracts Using Azure OpenAI

Use Azure OpenAI models to further refine the filtered logical chunks.
Input: Filtered logical chunks.
Output: AI-filtered extracts containing relevant paragraphs that answer predefined financial questions.
Key Terms Identification

Identify key financial and legal terms within the AI-filtered extracts using the AI model.
Extract a list of key terms that are significant for understanding the financial context of the document.
Regex Filtering of Key Term Definitions

Apply regex patterns to further filter the identified key terms to pinpoint exact definitions within the document.
Extract text snippets that provide definitions for the key terms.
AI Filtered Key Term Definitions

Use Azure OpenAI to refine and enhance the definitions of the key terms.
Input: Regex-filtered key term definitions.
Output: Contextualized, detailed definitions of each key term.
AI MFees Interpretations

Generate interpretations of the management fee-related extracts using AI models.
Simplify complex financial language into concise, understandable summaries.
Provide interpretations that highlight critical insights about management fees and their calculations.
AI Key Term Definitions Interpretations

Use AI models to provide clear and comprehensive interpretations of the key term definitions.
Cross-reference key term definitions with relevant document content to ensure accuracy.
Generate summarized interpretations that facilitate easy understanding of complex financial and legal terms.
Output Generation

Compile the results, including page dimensions, document layout, logical chunks, filtered extracts, key terms, definitions, and interpretations.
Format the output data for presentation, reporting, or further analysis by end-users.
Error Handling and Validation

Implement error handling for data retrieval, model processing, and interpretation stages.
Validate outputs at each stage to ensure consistency and accuracy before final output generation.
End of Process

Save the processed data and results back to Azure Blob Storage or an alternative storage solution as required.
Log the process flow and outputs for auditing and troubleshooting purposes.
Key Points
The algorithm leverages a combination of regex filtering and AI-driven extraction techniques to ensure precise identification and interpretation of key financial terms within complex documents.
Each step is validated and cross-referenced to ensure data integrity and accuracy, providing reliable outputs that enhance decision-making.
The integration of Azure OpenAI models adds contextual understanding to the extracted information, making it easier to navigate complex legal and financial language.
This algorithm provides a structured approach to processing and analyzing financial documents, enabling automated insights that are critical for legal, financial, and compliance needs. Let me know if you need further details or specific code implementations for each step!










ChatGPT can make mistakes. Check









1. Input Document Acquisition
Description: This stage involves fetching the input documents from Azure Blob Storage using a managed identity for secure access. The system connects to specific Azure Blob containers using predefined connection settings to locate and retrieve the document files needed for processing.
Purpose: To provide the initial data required for analysis, ensuring that the documents are securely accessed and correctly identified.
2. Artifact Generation and Initialization
Description: Once the documents are retrieved, the system initializes various artifacts that are essential for further processing. This includes setting up data structures like page dimensions, document layout, logical chunks, key term lists, and more. These artifacts help in organizing and structuring the content of the document for subsequent analysis.
Components:
Page Dimensions: Captures the size and layout of each page.
Document Layout: Defines the structure of the document.
Logical Chunks: Divides the document into meaningful sections for processing.
3. Document Parsing
Description: In this stage, Azure Document Intelligence is used to analyze the document's layout and content. It identifies various elements such as text, tables, images, and other structural components. This process is crucial for extracting meaningful chunks of data that can be further analyzed.
Purpose: To break down the document into its fundamental parts, making it easier to target specific information during subsequent analysis.
4. Regex Filtering of Logical Chunks
Description: The logical chunks of the document are filtered using predefined regex patterns to identify specific keywords related to financial and management terms, such as "Management Fee," "Compensation," and others. This stage narrows down the content to the most relevant sections based on the target terms.
Purpose: To refine the document content by focusing on relevant sections that contain specific key terms, allowing for more efficient and targeted analysis.
5. AI Filtering of Extracts
Description: This stage uses Azure OpenAI models to further refine the extracts identified by the regex filtering. The AI models are prompted to answer specific financial questions or highlight relevant content related to management fees and other terms.
Purpose: To enhance the filtering process by leveraging AI's ability to understand context and provide insights beyond simple keyword matching.
6. Key Terms Identification
Description: AI models analyze the filtered content to identify key financial and legal terms. This process involves scanning the text to detect significant terms that are essential for understanding the document's content.
Purpose: To create a comprehensive list of key terms that are critical for interpreting the financial information presented in the document.
7. Regex Filtering of Key Term Definitions
Description: Identified key terms are subjected to another round of filtering using regex patterns to pinpoint the precise definitions within the document. This helps in associating each key term with its most relevant and accurate description.
Purpose: To ensure that each key term is clearly defined within the context of the document, enhancing comprehension for further analysis.
8. AI Filtered Key Term Definitions
Description: Azure OpenAI models are used to refine the definitions of the key terms identified in the previous stage. The models are prompted to provide detailed, contextual definitions based on the document content.
Purpose: To deliver clear, context-specific definitions that help in understanding the financial and legal nuances of the key terms.
9. AI MFees Interpretations
Description: The AI models interpret and summarize the management fee-related content extracted from the document. This involves breaking down complex financial language into simpler, more digestible summaries.
Purpose: To provide actionable insights and interpretations of management fee structures, calculations, and related financial information.
10. AI Key Term Definitions Interpretations
Description: In the final stage, AI models provide interpretations of the key term definitions, summarizing the financial and legal implications in a clear and concise manner. This includes cross-referencing the definitions with the document's content to ensure accuracy.
Purpose: To synthesize the key term definitions into understandable interpretations that can be easily used by non-experts, supporting decision-making and compliance.
These descriptions outline the specific functionality and purpose of each stage in the document processing workflow, highlighting how Azure and AI services are integrated to achieve comprehensive document analysis. Let me know if you need further details on any of these stages!






You said:
Overall description about the project 
ChatGPT said:
ChatGPT
Overall Description of the Project
The project is a comprehensive document processing and analysis system designed to handle complex financial and legal documents using Azure and AI technologies. The primary goal of the system is to extract, interpret, and summarize critical information related to management fees and other key financial terms found within these documents. This automated process aims to provide accurate, relevant insights, reducing manual effort and enhancing the efficiency of document analysis for financial institutions, legal firms, and other stakeholders.

Key Objectives
Automate Document Parsing and Analysis: Utilize Azure Document Intelligence and OpenAI models to automate the extraction and analysis of complex documents, thereby reducing the time and effort required for manual review.
Identify and Interpret Key Financial Terms: Use AI and regex-based filtering to identify key terms related to financial management, such as management fees, compensation, and other expenses, ensuring that relevant information is accurately highlighted.
Provide Clear, Actionable Insights: Interpret the extracted content to deliver concise, understandable summaries of complex financial terms, enabling better decision-making and compliance review.
Technical Components
Azure Blob Storage: Serves as the secure storage location for all input documents, enabling easy access and retrieval through managed identity connections.
Azure Cognitive Services (Document Intelligence): Analyzes the document structure, extracting logical chunks, page layouts, and other relevant information necessary for detailed processing.
Azure OpenAI Models: Plays a central role in refining the extracted content, identifying key terms, and generating interpretations and definitions that are contextually accurate.
Regex Filtering: Combines traditional pattern matching techniques with AI models to ensure precise extraction of financial and legal terms.
Workflow Overview
Document Acquisition: The process begins with retrieving the document from Azure Blob Storage, followed by artifact initialization to prepare the document for analysis.
Document Parsing and Chunking: Azure Document Intelligence parses the document into logical sections, enabling targeted analysis of specific content areas.
Filtering and AI-Driven Extraction: Key terms and relevant paragraphs are identified using regex filtering and AI models, focusing on specific financial terms related to management fees and other expenses.
Interpretation and Summarization: AI models provide detailed interpretations of the identified content, breaking down complex legal and financial information into manageable insights.
Output Generation: The system produces clear, organized summaries and interpretations that can be used directly by stakeholders for financial analysis, legal compliance, or reporting purposes.
Benefits
Enhanced Accuracy and Efficiency: Automates the extraction and interpretation of critical information, significantly reducing the risk of human error.
Time Savings: Speeds up the document review process, allowing analysts and legal professionals to focus on higher-value tasks rather than manual content extraction.
Improved Decision-Making: Provides actionable insights by breaking down complex financial terms into understandable summaries, facilitating better decision-making and strategic planning.
Scalability: The system is designed to handle large volumes of documents, making it suitable for organizations with extensive document processing needs.
Use Cases
Financial Institutions: For processing investment fund documents, understanding fee structures, and ensuring compliance with regulatory requirements.
Legal Firms: Assisting in the review of complex contracts, identifying critical terms, and providing clear interpretations for legal advice.
Corporate Finance Teams: Analyzing management fees, compensation terms, and other expenses to optimize financial management and strategy.
Conclusion
This project leverages state-of-the-art AI and cloud technologies to transform how complex financial documents are processed and interpreted. By automating key aspects of document analysis, the system not only enhances accuracy and efficiency but also empowers users with valuable insights that drive better business outcomes.

Let me know if you need further customization or additional information!









ChatGPT can make mistakes. Check important








ChatGPT 4o







You said:
{

"version": 1,

"vars": {

"pm_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "pvt-markets"

},

"di_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "npeedevgtsd@1txtmrm-strgcont-txtmrm-01"

}, "input_document_path": {

"source": "AzureBlob",

I

params": { "

}

}

"connection": "{vars.pm_azure_blob_connection)",

"storage_location": "(payload.documents[0].storagePath.absolute_path}"

},

"artifacts": {

"pageDimensions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},
"documentLayout": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"logicalChunks": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}, "extractsUsingPrescribed Language": {

"source": "Transient",

I

"data": [],

"params": {},

"extant": false

},

"aiFilteredExtracts": {

"source": "Transient",

"data": [],

"params": {}, "extant": false
},

"keyTerms": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

}, "regexFilteredDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"keyTermDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"keyTermMapping":{

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"mFees Interpretations": {

"source": "AzureBlob",

data": [], "

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",
"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

},

"keyTermInterpretations": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}

}, "output": {

"pageDimensions": "(artifacts.pageDimensions.data}", "documentLayout": "(artifacts.documentLayout.data}",

"logicalChunks": "{artifacts.logicalChunks.data}",

"extractsUsing PrescribedLanguage": "{artifacts.extractsUsing PrescribedLanguage.data}",

"keyTerms": "{artifacts.keyTerms.data)",

"keyTermDefinitions": "{artifacts.keyTermDefinitions.data)",

"keyTermMapping": "{artifacts.keyTermMapping.data)",

"aiFilteredExtracts": "{artifacts.aiFilteredExtracts.data}", "mFees Interpretations": "{artifacts.mFees Interpretations.data)",

"keyTermInterpretations": "{artifacts.keyTermInterpretations.data}"

},
"stages": [

{

"capability": "DocumentParsing",

"method": "AzureDocument Intelligence",

"label": "DocumentParsing",

"processing_config": {

"processing_engine": "Azure-openai",

"model": "prebuilt-layout",

"endpoint": "https://npee-use2-dev-fr-01.cognitiveservices.azure.com/",

"key": "bacfb986c0874ace9211551852829213"

},

"input": {

"document_path": "{vars.input_document_path}"

}, "output": {

"(artifacts.logicalChunks.data}": "logicalChunks",

"{artifacts.documentLayout.data)": "document Layout",

"(artifacts.pageDimensions.data)": "pageDimensions"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Logical Chunks Regex Filtering",

"input": {

"logicalChunks": "(artifacts.logicalChunks.data}",

"keywords": [

"Management Fee",

"Management +Fee",

I

}

"Servicing Fee", "Servicing +Fee",

"Investment Advisory Fee",
"Investment +Advisory +Fee",

"Compensation",

"Remuneration",

"Expenses",

"AIFM Fee",

"AIFM +Fee"

},

]

"{artifacts.extractsUsing Prescribed Language.data}": "extractsUsing Prescribed Language"

}

"output": {

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered MFees Related Extracts",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

}, "temperature": 0,

"seed": 17

"prompt": {

"template":" Objective \nYou will be given a list of paragraphs. You need to identify the paragraphs that helpenser one or more of exes which helped answer one or more of the questions. Ind. Create a field 'questions: List of the question Indexes that was wered by the paragra (

"input variables"

"questions":{

"indexing required": true,

"data":[

"Can you explain the calculation, payment timing, and due dates of the Management Fees?"

"How are the Management Fees affected by the Fund's Investment in any Primary Investee fund or Secondary Investee Fund, "How is the Management Fee calculated and reduced in relation to various Investments and fees

"paragraphs": {

"indexing required": true,

"date": "(artifacts.extractsUsing PrescribedLanguage.data)"

"post process required": true

"output" (

"(artifacts.eiFilteredExtracts.data)": "aifilteredExtracts"

"capability": "GenAZExtraction",

"method": "Prompting",

"label": "Keyteres Identification",

"processing config":{

model connection: (

"model provider_type": "AzureüpenAI",

"openai api type": "azurM
"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 17

},

"prompt": {

"template": "\n# Objective\nYou will be given a list of paragraphs. Your task is to identify the keyterms present in eac

ing company, financial services company and partnership.\n7. Respond back with the list of keyterms against each Paragraph in title case

"input_variables": {

"paragraphs": {

"indexing_required": true,

"data": "{artifacts.aiFilteredExtracts.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.keyTerms.data}": "keyTerms"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Regex Filtered Keyterm Definitions",

input": { "

"logicalChunks": "fartifacts.logicalChunks.data}",

"keyTerms": "{artifacts.keyTerms.data}"

},
"output": {

"(artifacts.regexFilteredDefinitions.data)": "regexFilteredDefinitions"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered Keyterm Definitions",

processing_config": { "

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou will be provided with a numbered list of Paragraphs and Keyterms. For each of the provided Keyter

hat provides the definition of the Keyterm, return -1 within a list as the index response to the Keyterm.\n\n\n# Output Instructions:\n\n## Outpu

"input_variables": { "paragraphs ": {

"indexing_required": true,

"data": "(artifacts.regexFilteredDefinitions.data)"

}, "keyterms": {

"indexing_required": true,

"data": "{artifacts.keyTerms.data)"
}

}, "post_process_required": true

}, "output": {

"(artifacts.keyTermDefinitions.data)": "keyTermDefinitions",

"(artifacts.keyTermMapping.data)": "keyTermMapping"

}

"capability": "GenAIExtraction",

"method" : "Prompting",

"label": "AI MFees Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment funds and manag in title case or capital letters are not changed in the interpretation.\n6. Break down the interpretation into small sentences.\n7. If you are not able terpretations for the paragraph using the reference of 'Keyterm Definition Extracts' if required to understand the content of the paragraph for interpr

input_variables": { " "

mFees InterpretationsExtracts": {

"indexing_required": true,
"data": "(artifacts.aiFilteredExtracts.data}" },

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

}

},

"post_process_required": false

}, "output": {

"{artifacts.mFees Interpretations.data)": "mFees Interpretations"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Keyterm Definitions Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview", "openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment f wotation of the keyterm definition.\n\n# Understanding the layout of 'Keyterm Definition Extracts' provided in the i

The contents of 'Keyterm Definition Extracts' are bullet listed paragraphs, with each paragraph containing the mapping of s.",

"input_variables": {

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

},

"keyTermMapping": {

"indexing_required": true,

"data": "{artifacts.keyTermMapping.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.key TermInterpretations.data}": "key Term Interpretations"

}

}

]

}



""Create a high level flowchat of work flow and create a detailed technical stage wise documentation and create service wise functinolity write up
ChatGPT said:
ChatGPT
To create a comprehensive understanding of the provided JSON configuration, we'll break it down into a detailed flowchart of the workflow and a stage-wise documentation that includes service-wise functionality descriptions. Here's the approach:

High-Level Flowchart of Workflow
Input Document Acquisition

Source: Azure Blob Storage.
Connections: pm_azure_blob_connection and di_azure_blob_connection.
Data: Input document path fetched from Azure Blob.
Artifact Generation and Initialization

Artifacts Created:
Page Dimensions
Document Layout
Logical Chunks
Extracts Using Prescribed Language
AI Filtered Extracts
Key Terms
Key Term Definitions
Key Term Mapping
MFees Interpretations
Key Term Interpretations
Document Parsing Stage

Capability: DocumentParsing
Method: AzureDocument Intelligence (prebuilt-layout model).
Output: Logical chunks, document layout, and page dimensions.
Regex Filtering Stage

Capability: GenAIExtraction
Method: RegexFiltering
Input: Logical chunks filtered with specified keywords (Management Fees, Compensation, etc.).
Output: Extracts using Prescribed Language.
AI Filtering of Extracts

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: AI Filtered MFees Related Extracts.
Key Terms Identification

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Terms.
Regex Filtering of Key Term Definitions

Capability: GenAIExtraction
Method: RegexFiltering
Output: Regex Filtered Key Term Definitions.
AI Filtered Key Term Definitions

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Definitions and Mapping.
AI MFees Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: MFees Interpretations.
AI Key Term Definitions Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Interpretations.
Stage-wise Documentation
Input Document Acquisition

Description: Fetches documents from Azure Blob Storage using managed identity connections (pm_azure_blob_connection and di_azure_blob_connection).
Purpose: To provide input data for processing through the Azure OpenAI models.
Document Parsing

Description: Parses the document to identify layout and logical chunks using Azure Cognitive Services.
Functionality:
Extracts structural information like page dimensions and layout.
Identifies logical sections of the document for further analysis.
Regex Filtering of Logical Chunks

Description: Uses regex patterns to filter logical chunks for specific keywords related to fees and financial terms.
Functionality:
Focuses on terms like "Management Fee," "Compensation," and other financial keywords.
Extracts text segments relevant to fee calculations and descriptions.
AI Filtering of Extracts

Description: Applies Azure OpenAI models to refine filtered chunks by identifying relevant fee-related content.
Functionality:
Uses prompting techniques to focus on answering specific financial questions.
Outputs filtered extracts that provide relevant answers.
Key Terms Identification

Description: Identifies key financial and legal terms within the filtered extracts.
Functionality:
Uses AI models to detect and list key terms.
Provides context for understanding financial terminology within the document.
Regex Filtering of Key Term Definitions

Description: Further filters the key terms to refine their definitions using regex patterns.
Functionality:
Matches and extracts text that defines the identified key terms.
AI Filtered Key Term Definitions

Description: AI models provide detailed definitions and mapping for identified key terms.
Functionality:
Outputs refined definitions that are contextually accurate for financial interpretations.
MFees Interpretations

Description: Interprets the filtered extracts related to management fees and other financial terms.
Functionality:
Breaks down complex financial text into manageable summaries.
Provides interpretative insights for legal and financial understanding.
Key Term Definitions Interpretations

Description: Provides a clear, summarized interpretation of key term definitions with contextual mapping.
Functionality:
Offers a simplified, accurate understanding of complex legal and financial definitions.
Supports clearer comprehension for non-technical stakeholders.
Service-Wise Functionality Write-up
Azure Blob Storage

Manages document storage and retrieval securely using managed identity for access.
Azure Cognitive Services - Document Intelligence

Parses and analyzes document structure, enabling logical division and understanding of document layouts.
Azure OpenAI Services

Implements AI-driven extraction, filtering, and interpretation of financial and legal text.
Models configured with specific prompting strategies to ensure accurate, relevant outputs.
Regex Filtering and AI Extraction Methods

Ensures precise identification of terms and extracts based on defined patterns and AI model capabilities.
Enhances the accuracy of document analysis by combining deterministic regex and probabilistic AI techniques.
This documentation provides a detailed overview of each step, outlining how Azure and OpenAI services are used to process, extract, and interpret complex financial documents effectively. Let me know if you need any specific adjustments or further details!






You said:
I need flow chat diagram 
ChatGPT said:
ChatGPT

Create a flowchart diagram illustrating a document processing workflow using Azure and AI services. The flowchart should include the following stages: 1) Input Document Acquisition from Azure Blob Storage, 2) Artifact Generation and Initialization with components like Page Dimensions, Document Layout, Logical Chunks, etc., 3) Document Parsing using Azure Document Intelligence, 4) Regex Filtering of Logical Chunks with specific keywords, 5) AI Filtering of Extracts using Azure OpenAI, 6) Key Terms Identification using AI, 7) Regex Filtering of Key Term Definitions, 8) AI Filtered Key Term Definitions, 9) AI MFees Interpretations, and 10) AI Key Term Definitions Interpretations. The flowchart should have clear directional arrows showing the flow of data between each stage, with each stage represented by a distinct box labeled accordingly.
Here is the flowchart diagram illustrating the document processing workflow using Azure and AI services. It includes all the stages from document acquisition to AI interpretations as described. You can review and use it for your reference!












ChatGPT

You said:
Ai reletad taks like working on documents ""i need to send the information to my officer please give me content 
ChatGPT said:
ChatGPT
To assist you in preparing content to send to your officer, I will need more details about the specific task or type of information you're working on. Could you clarify:

What kind of document are you preparing (e.g., report, memo, email)?
What specific information needs to be included (e.g., project updates, data analysis, etc.)?
Any particular format or tone you prefer (formal, brief, detailed)?
Feel free to provide more details, and I’ll help craft the content for you.






You said:
Al related tasks:

Step 1: loading pdf document

Step 2: using pdf miner or ADI we are generating the text(Prescribed language)

Step 3: Extracts management fees related extracts from prescribed language(step 2 output we are giving as input)

Step 4: Identify the keyterms from management fees related extracts(step 3 output we are giving as input)

Step 5: extracts keyterm defination from keyterms identification ((step 4 output we are giving as input)

I

Search

Inibss
ChatGPT said:
ChatGPT
It sounds like you're describing an AI workflow for processing documents, specifically for extracting and analyzing management fees-related terms from PDF documents. Here’s a refined summary based on your steps:

AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!






You said:
Can u give me objective of the program. And whatvis your role ofvthe program " they are asking my role is building the models and also evalutingthe the model i
ChatGPT said:
ChatGPT
Here’s a potential objective for the program and a description of your role based on the tasks you’ve outlined:

Objective of the Program:
The objective of this program is to automate the extraction, identification, and analysis of management fee-related information from PDF documents using AI and Natural Language Processing (NLP) techniques. The system aims to streamline the process of reading complex documents, isolating specific fee-related sections, identifying key financial terms, and extracting their definitions. This will help in making fee structures and related information more accessible and manageable for further analysis or reporting.

Your Role in the Program:
As a Model Developer and Evaluator, your role involves:

Building NLP Models:

Designing and implementing AI models that process PDF documents, extract relevant text, and identify key terms related to management fees.
Utilizing tools like PDFMiner for document processing and NLP libraries (e.g., NLTK, spaCy) for extracting and analyzing text.
Developing customized models for key term extraction and definition identification, ensuring they are aligned with the prescribed language and specific requirements of the task.
Evaluating Models:

Testing and validating the accuracy and efficiency of the models to ensure the extracted information is relevant, correct, and in line with expected outputs.
Evaluating the performance of the models using metrics like precision, recall, and F1-score to measure the quality of extraction and identification processes.
Continuously improving models based on evaluation results, feedback, and new data to enhance overall performance.
Optimizing the Workflow:

Ensuring the entire process, from loading documents to extracting definitions, is seamless and optimized for scalability.
Collaborating with other team members to integrate the AI models into the broader system and refine them based on user needs or new use cases.
This should give you a solid overview of both the program’s objective and your critical role within it. Let me know if you'd like further adjustments or clarifications!






Is this conversation helpful so far?












AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!











import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples or numpy arrays
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and contains the necessary keys
    expected_keys = {"accuracy", "precision", "recall", "F1 score"}
    if not isinstance(scores, dict):
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', 'recall', 'F1 score'.")
    missing_keys = expected_keys - scores.keys()
    if missing_keys:
        raise ValueError(f"The following keys are missing in the 'scores' dictionary: {missing_keys}")

    # Populate the DataFrame with the scores in the first row
    df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
    df.loc[0, "Precision"] = scores.get("precision", np.nan)
    df.loc[0, "Recall"] = scores.get("recall", np.nan)
    df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df










import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
        df.loc[0, "Precision"] = scores.get("precision", np.nan)
        df.loc[0, "Recall"] = scores.get("recall", np.nan)
        df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(item)
        elif isinstance(item, dict):
            return tuple(sorted(item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # If actual or predicted contain dictionaries, convert them to tuples
    actual_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in actual)
    predicted_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Create sets for comparison
    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):

    def equalize_lengths(actual, predicted):
        len_actual = len(actual)
        len_predicted = len(predicted)

        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    all_items = list(actual_set.union(predicted_set))

    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df





import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    # Convert lists of dictionaries to lists of strings (or tuples) to make them hashable
    actual_set = set([str(item) for item in actual])
    predicted_set = set([str(item) for item in predicted])

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available")

    def connect_to_azure_blob(self, input_document_path):
        """
        Connect to Azure Blob Storage using Managed Identity.
        """
        try:
            connection = input_document_path['params']['connection_string']
            mi_client_id = connection["mi_client_id"]

            credential = ManagedIdentityCredential(client_id=mi_client_id)

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        absolute_path = params["absolute_path"]
        blob_name = os.path.basename(absolute_path)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        try:
            with open(pdf_file_path, 'wb') as pdf_file:
                download_stream = blob_client.download_blob()
                pdf_file.write(download_stream.readall())
        except Exception as e:
            logger.error(f"Failed to download blob {blob_name} from Azure Blob Storage: {e}")
            return None

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        """
        Run ADI processing on the downloaded PDF.
        """
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key
            )
            return merged_content
        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processingConfig = {
    "processing_engine": "ADI",
    "model": "prebuilt-layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c0c7c3a9a4371b7dc983ac7b618b7"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://eventhubstorage919.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "5da5a4c-edb2-4f9f-8ad8-d11f0214f557"
        },
        "absolute_path": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_1p_-_2nd_er_lpa_march_16_20211.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processingConfig, input_document_path)
print(merged_content)








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available.")

    def connect_to_azure_blob(self, input_document_path):
        # Connect to Azure Blob Storage using Managed Identity.
        connection_string = input_document_path["params"]["connection_string"]

        try:
            credential = ManagedIdentityCredential(client_id=connection_string["mi_client_id"])

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection_string["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection_string["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        storage_location = params["storage_location"]
        blob_name = os.path.basename(storage_location)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        with open(pdf_file_path, 'wb') as pdf_file:
            download_stream = blob_client.download_blob()
            pdf_file.write(download_stream.readall())

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        # Download PDF from Azure Blob
        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            # Example method calls assuming run_prescribed_extracts_azure exists
            page_dim_json_file_path, _doc_layout_json_file_path, doc_layout = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            return merged_content

        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processing_config = {
    "processing_engine": "ADI",
    "model": "Layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://<your-account-name>.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "<your-managed-identity-client-id>"
        },
        "storage_location": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_ip_-_2nd_ar_lpa_march_16_2021.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processing_config, input_document_path)

print(merged_content)







import os
import json
import re

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        if merged_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    Simulate the extraction of definitions based on indexed key terms.
    """
    # This is a placeholder for the actual definition extraction logic.
    return json.dumps({"definitions": quotation_marks_paragraphs})

def load_json(file_path):
    """
    Load JSON data from a file.
    :param file_path: Path to the JSON file.
    :return: Parsed JSON data.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    """
    Process a PDF and associated logical chunks file to extract key terms and their definitions.
    :param pdf_path: Path to the input PDF.
    :param logical_chunks_file: Path to the JSON file containing logical chunks.
    :param extracted_keyterms_json_path: Path to the JSON file containing key terms.
    :param output_dir: Directory where the output JSON will be saved.
    :return: JSON output and the path to the saved JSON file.
    """
    # Check if the PDF name matches the logical chunks file name
    pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    logical_chunks_base_name = os.path.splitext(os.path.basename(logical_chunks_file))[0].replace("logicalchunks", "").strip()

    if pdf_base_name != logical_chunks_base_name:
        raise ValueError(f"Mismatch: PDF base name '{pdf_base_name}' does not match logical chunks base name '{logical_chunks_base_name}'.")

    extracted_definition_json_file_name = pdf_base_name + "_generated_definitions.json"
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms
    with open(extracted_keyterms_json_path, "r") as json_file:
        data = json.load(json_file)

    keyterm_index = 0
    list_indexed_keyterms = []

    search_strings = data["keyTerms"]
    for search_string in search_strings:
        indexed_keyterm = f"{keyterm_index} {search_string}"
        keyterm_index += 1
        list_indexed_keyterms.append(indexed_keyterm)

    # Load logical chunks
    file_data = load_json(logical_chunks_file)

    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            # For chunks that need merging of content
            if len(items) >= 2:
                merged_para = ""
                for i in range(len(items)):
                    para = items[i]["content"]
                    merged_para += " " + para
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para.strip(), quotation_marks_paragraphs, count)
            # For chunks having only one content
            elif len(items) == 1:
                for contents in items:
                    content = contents["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions based on the filtered paragraphs
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)
    json_output = json.loads(output)

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(json_output, ff, ensure_ascii=False, indent=4)

    print(f"Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}")

    return json_output, extracted_definition_json_path

# Testing the function
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H://management_fee_extraction//extracts_output//output7//"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)







import os
import json
import re

def load_json(file_path):
    """Utility function to load JSON data from a file."""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        indexing_para = merged_para
        if indexing_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    This is a placeholder for the actual implementation of extracting definitions.
    It would process the filtered paragraphs and indexed keyterms to produce output.
    """
    # Assuming this function returns a JSON-serializable object as output.
    return {"definitions": [{"keyterm": keyterm, "paragraph": para} for keyterm, para in zip(list_indexed_keyterms, quotation_marks_paragraphs)]}

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    extracted_definition_json_file_name = os.path.basename(pdf_path)
    extracted_definition_json_file_name = extracted_definition_json_file_name.replace(".pdf", "_generated_definitions.json").replace(".PDF", "_generated_definitions.json")
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms from the JSON file
    data = load_json(extracted_keyterms_json_path)
    search_strings = data["keyTerms"]

    # Index key terms
    list_indexed_keyterms = [str(i) + " " + search_string for i, search_string in enumerate(search_strings)]

    # Load logical chunks
    file_data = load_json(logical_chunks_file)
    
    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            if len(items) >= 2:
                merged_para = ""
                for item in items:
                    merged_para += item["content"] + " "
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count)
            elif len(items) == 1:
                for item in items:
                    content = item["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(output, ff, ensure_ascii=False, indent=4)

    return output, extracted_definition_json_path

# Testing
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H:\management_fee_extraction\extracts_output\output7"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)
import re

def get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param count: The current count of filtered paragraphs.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :return: Updated list of filtered paragraphs and the count.
    """
    # Correct the pattern for regex search
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'

    # Check if the pattern is found in the merged paragraph
    if re.search(pattern, merged_para):
        indexing_para = merged_para

        # Ensure the paragraph is not already in the list
        if indexing_para not in quotation_marks_paragraphs:
            # Correct string concatenation and append the paragraph
            filtered_para = str(count) + " " + merged_para
            print(f"filtered_para: {filtered_para}")

            quotation_marks_paragraphs.append(filtered_para)
            count += 1

    return quotation_marks_paragraphs, count

# Example usage:
search_string = "Management Fee"
merged_para = "This paragraph talks about Management Fee and other details."
count = 1
quotation_marks_paragraphs = []

result, updated_count = get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs)

print("Filtered Paragraphs:", result)
print("Updated Count:", updated_count)





import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of search terms
        :return: List of compiled regex patterns
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content to search within
        :param patterns: List of compiled regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :param patterns: List of compiled regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        for item in data['logicalchunks']:
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge content from the provided JSON data
        :param json_data: Input JSON data as a dictionary
        :return: Dictionary with merged content
        """
        data = json.loads(json.dumps(json_data))
        combined_strings = []

        for item in data['extractsUsingPrescribedLanguage']:
            combined_string = ' '.join(item['content'] for item in item)
            combined_strings.append(combined_string)

        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data to a file
    :param json_file_path: Path to the JSON file
    :param json_data: Data to write to the file
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file)
    except Exception as e:
        print(f"Error dumping JSON file: {e}")

# File paths
data_file_path = "H:/management_fee_extraction/extracts_output/output5/LEGAL-#148874-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
output_dest_dir = "H:/management_fee_extraction/extracts_output/output5"

with open(data_file_path, "r") as json_file:
    data = json.load(json_file)

obj = KeywordFilterMethod()
paragraphs = obj.extract(data)

print(paragraphs)

paragraphs = {"extractsUsingPrescribedLanguage": paragraphs}

file_name = os.path.splitext(os.path.basename(data_file_path))[0]

filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
dump_json_file(filtered_images_json_file_path, paragraphs)

merged_content = obj.merge_content(paragraphs)
merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

print(merged_content_json_file_path)
print(merged_content)

dump_json_file(merged_content_json_file_path, merged_content)






import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        return self.extract_data_with_terms(data, patterns)

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of keywords
        :return: List of compiled regex patterns
        """
        return [re.compile(re.escape(term)) for term in terms]

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content of the node
        :param patterns: List of regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :param patterns: List of regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        logical_chunks = data.get('logicalChunks', [])  # Ensure it's a list
        for item in logical_chunks:
            if isinstance(item, list):  # Check if item is a list
                for node in item:
                    if isinstance(node, dict):  # Check if node is a dictionary
                        content = node.get('content', '')  # Safely get content
                        if self.search_terms_in_content(content, patterns):
                            extracted_data.append(item)
                            break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data: JSON data containing extractsUsingPrescribedLanguage
        :return: Merged content dictionary
        """
        extracts = json_data.get('extractsUsingPrescribedLanguage', [])  # Ensure it's a list
        combined_strings = []
        for item in extracts:
            if isinstance(item, list):  # Check if item is a list
                combined_string = ' '.join(node.get('content', '') for node in item if isinstance(node, dict))
                combined_strings.append(combined_string)
        return {'extractsUsingPrescribedLanguage': combined_strings}

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path: Path to save the JSON file
    :param json_data: Data to save
    :return: None
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))

