import os
import json
import re

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        if merged_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    Simulate the extraction of definitions based on indexed key terms.
    """
    # This is a placeholder for the actual definition extraction logic.
    return json.dumps({"definitions": quotation_marks_paragraphs})

def load_json(file_path):
    """
    Load JSON data from a file.
    :param file_path: Path to the JSON file.
    :return: Parsed JSON data.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    """
    Process a PDF and associated logical chunks file to extract key terms and their definitions.
    :param pdf_path: Path to the input PDF.
    :param logical_chunks_file: Path to the JSON file containing logical chunks.
    :param extracted_keyterms_json_path: Path to the JSON file containing key terms.
    :param output_dir: Directory where the output JSON will be saved.
    :return: JSON output and the path to the saved JSON file.
    """
    # Check if the PDF name matches the logical chunks file name
    pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    logical_chunks_base_name = os.path.splitext(os.path.basename(logical_chunks_file))[0].replace("logicalchunks", "").strip()

    if pdf_base_name != logical_chunks_base_name:
        raise ValueError(f"Mismatch: PDF base name '{pdf_base_name}' does not match logical chunks base name '{logical_chunks_base_name}'.")

    extracted_definition_json_file_name = pdf_base_name + "_generated_definitions.json"
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms
    with open(extracted_keyterms_json_path, "r") as json_file:
        data = json.load(json_file)

    keyterm_index = 0
    list_indexed_keyterms = []

    search_strings = data["keyTerms"]
    for search_string in search_strings:
        indexed_keyterm = f"{keyterm_index} {search_string}"
        keyterm_index += 1
        list_indexed_keyterms.append(indexed_keyterm)

    # Load logical chunks
    file_data = load_json(logical_chunks_file)

    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            # For chunks that need merging of content
            if len(items) >= 2:
                merged_para = ""
                for i in range(len(items)):
                    para = items[i]["content"]
                    merged_para += " " + para
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para.strip(), quotation_marks_paragraphs, count)
            # For chunks having only one content
            elif len(items) == 1:
                for contents in items:
                    content = contents["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions based on the filtered paragraphs
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)
    json_output = json.loads(output)

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(json_output, ff, ensure_ascii=False, indent=4)

    print(f"Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}")

    return json_output, extracted_definition_json_path

# Testing the function
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H://management_fee_extraction//extracts_output//output7//"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)







import os
import json
import re

def load_json(file_path):
    """Utility function to load JSON data from a file."""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        indexing_para = merged_para
        if indexing_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    This is a placeholder for the actual implementation of extracting definitions.
    It would process the filtered paragraphs and indexed keyterms to produce output.
    """
    # Assuming this function returns a JSON-serializable object as output.
    return {"definitions": [{"keyterm": keyterm, "paragraph": para} for keyterm, para in zip(list_indexed_keyterms, quotation_marks_paragraphs)]}

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    extracted_definition_json_file_name = os.path.basename(pdf_path)
    extracted_definition_json_file_name = extracted_definition_json_file_name.replace(".pdf", "_generated_definitions.json").replace(".PDF", "_generated_definitions.json")
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms from the JSON file
    data = load_json(extracted_keyterms_json_path)
    search_strings = data["keyTerms"]

    # Index key terms
    list_indexed_keyterms = [str(i) + " " + search_string for i, search_string in enumerate(search_strings)]

    # Load logical chunks
    file_data = load_json(logical_chunks_file)
    
    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            if len(items) >= 2:
                merged_para = ""
                for item in items:
                    merged_para += item["content"] + " "
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count)
            elif len(items) == 1:
                for item in items:
                    content = item["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(output, ff, ensure_ascii=False, indent=4)

    return output, extracted_definition_json_path

# Testing
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H:\management_fee_extraction\extracts_output\output7"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)
import re

def get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param count: The current count of filtered paragraphs.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :return: Updated list of filtered paragraphs and the count.
    """
    # Correct the pattern for regex search
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'

    # Check if the pattern is found in the merged paragraph
    if re.search(pattern, merged_para):
        indexing_para = merged_para

        # Ensure the paragraph is not already in the list
        if indexing_para not in quotation_marks_paragraphs:
            # Correct string concatenation and append the paragraph
            filtered_para = str(count) + " " + merged_para
            print(f"filtered_para: {filtered_para}")

            quotation_marks_paragraphs.append(filtered_para)
            count += 1

    return quotation_marks_paragraphs, count

# Example usage:
search_string = "Management Fee"
merged_para = "This paragraph talks about Management Fee and other details."
count = 1
quotation_marks_paragraphs = []

result, updated_count = get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs)

print("Filtered Paragraphs:", result)
print("Updated Count:", updated_count)





import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of search terms
        :return: List of compiled regex patterns
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content to search within
        :param patterns: List of compiled regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :param patterns: List of compiled regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        for item in data['logicalchunks']:
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge content from the provided JSON data
        :param json_data: Input JSON data as a dictionary
        :return: Dictionary with merged content
        """
        data = json.loads(json.dumps(json_data))
        combined_strings = []

        for item in data['extractsUsingPrescribedLanguage']:
            combined_string = ' '.join(item['content'] for item in item)
            combined_strings.append(combined_string)

        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data to a file
    :param json_file_path: Path to the JSON file
    :param json_data: Data to write to the file
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file)
    except Exception as e:
        print(f"Error dumping JSON file: {e}")

# File paths
data_file_path = "H:/management_fee_extraction/extracts_output/output5/LEGAL-#148874-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
output_dest_dir = "H:/management_fee_extraction/extracts_output/output5"

with open(data_file_path, "r") as json_file:
    data = json.load(json_file)

obj = KeywordFilterMethod()
paragraphs = obj.extract(data)

print(paragraphs)

paragraphs = {"extractsUsingPrescribedLanguage": paragraphs}

file_name = os.path.splitext(os.path.basename(data_file_path))[0]

filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
dump_json_file(filtered_images_json_file_path, paragraphs)

merged_content = obj.merge_content(paragraphs)
merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

print(merged_content_json_file_path)
print(merged_content)

dump_json_file(merged_content_json_file_path, merged_content)






import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        return self.extract_data_with_terms(data, patterns)

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of keywords
        :return: List of compiled regex patterns
        """
        return [re.compile(re.escape(term)) for term in terms]

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content of the node
        :param patterns: List of regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :param patterns: List of regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        logical_chunks = data.get('logicalChunks', [])  # Ensure it's a list
        for item in logical_chunks:
            if isinstance(item, list):  # Check if item is a list
                for node in item:
                    if isinstance(node, dict):  # Check if node is a dictionary
                        content = node.get('content', '')  # Safely get content
                        if self.search_terms_in_content(content, patterns):
                            extracted_data.append(item)
                            break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data: JSON data containing extractsUsingPrescribedLanguage
        :return: Merged content dictionary
        """
        extracts = json_data.get('extractsUsingPrescribedLanguage', [])  # Ensure it's a list
        combined_strings = []
        for item in extracts:
            if isinstance(item, list):  # Check if item is a list
                combined_string = ' '.join(node.get('content', '') for node in item if isinstance(node, dict))
                combined_strings.append(combined_string)
        return {'extractsUsingPrescribedLanguage': combined_strings}

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path: Path to save the JSON file
    :param json_data: Data to save
    :return: None
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))

