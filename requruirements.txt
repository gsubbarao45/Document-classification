
import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(item)
        elif isinstance(item, dict):
            return tuple(sorted(item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # If actual or predicted contain dictionaries, convert them to tuples
    actual_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in actual)
    predicted_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Create sets for comparison
    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):

    def equalize_lengths(actual, predicted):
        len_actual = len(actual)
        len_predicted = len(predicted)

        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    all_items = list(actual_set.union(predicted_set))

    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df





import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    # Convert lists of dictionaries to lists of strings (or tuples) to make them hashable
    actual_set = set([str(item) for item in actual])
    predicted_set = set([str(item) for item in predicted])

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available")

    def connect_to_azure_blob(self, input_document_path):
        """
        Connect to Azure Blob Storage using Managed Identity.
        """
        try:
            connection = input_document_path['params']['connection_string']
            mi_client_id = connection["mi_client_id"]

            credential = ManagedIdentityCredential(client_id=mi_client_id)

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        absolute_path = params["absolute_path"]
        blob_name = os.path.basename(absolute_path)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        try:
            with open(pdf_file_path, 'wb') as pdf_file:
                download_stream = blob_client.download_blob()
                pdf_file.write(download_stream.readall())
        except Exception as e:
            logger.error(f"Failed to download blob {blob_name} from Azure Blob Storage: {e}")
            return None

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        """
        Run ADI processing on the downloaded PDF.
        """
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key
            )
            return merged_content
        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processingConfig = {
    "processing_engine": "ADI",
    "model": "prebuilt-layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c0c7c3a9a4371b7dc983ac7b618b7"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://eventhubstorage919.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "5da5a4c-edb2-4f9f-8ad8-d11f0214f557"
        },
        "absolute_path": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_1p_-_2nd_er_lpa_march_16_20211.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processingConfig, input_document_path)
print(merged_content)








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available.")

    def connect_to_azure_blob(self, input_document_path):
        # Connect to Azure Blob Storage using Managed Identity.
        connection_string = input_document_path["params"]["connection_string"]

        try:
            credential = ManagedIdentityCredential(client_id=connection_string["mi_client_id"])

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection_string["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection_string["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        storage_location = params["storage_location"]
        blob_name = os.path.basename(storage_location)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        with open(pdf_file_path, 'wb') as pdf_file:
            download_stream = blob_client.download_blob()
            pdf_file.write(download_stream.readall())

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        # Download PDF from Azure Blob
        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            # Example method calls assuming run_prescribed_extracts_azure exists
            page_dim_json_file_path, _doc_layout_json_file_path, doc_layout = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            return merged_content

        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processing_config = {
    "processing_engine": "ADI",
    "model": "Layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://<your-account-name>.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "<your-managed-identity-client-id>"
        },
        "storage_location": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_ip_-_2nd_ar_lpa_march_16_2021.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processing_config, input_document_path)

print(merged_content)







import os
import json
import re

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        if merged_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    Simulate the extraction of definitions based on indexed key terms.
    """
    # This is a placeholder for the actual definition extraction logic.
    return json.dumps({"definitions": quotation_marks_paragraphs})

def load_json(file_path):
    """
    Load JSON data from a file.
    :param file_path: Path to the JSON file.
    :return: Parsed JSON data.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    """
    Process a PDF and associated logical chunks file to extract key terms and their definitions.
    :param pdf_path: Path to the input PDF.
    :param logical_chunks_file: Path to the JSON file containing logical chunks.
    :param extracted_keyterms_json_path: Path to the JSON file containing key terms.
    :param output_dir: Directory where the output JSON will be saved.
    :return: JSON output and the path to the saved JSON file.
    """
    # Check if the PDF name matches the logical chunks file name
    pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    logical_chunks_base_name = os.path.splitext(os.path.basename(logical_chunks_file))[0].replace("logicalchunks", "").strip()

    if pdf_base_name != logical_chunks_base_name:
        raise ValueError(f"Mismatch: PDF base name '{pdf_base_name}' does not match logical chunks base name '{logical_chunks_base_name}'.")

    extracted_definition_json_file_name = pdf_base_name + "_generated_definitions.json"
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms
    with open(extracted_keyterms_json_path, "r") as json_file:
        data = json.load(json_file)

    keyterm_index = 0
    list_indexed_keyterms = []

    search_strings = data["keyTerms"]
    for search_string in search_strings:
        indexed_keyterm = f"{keyterm_index} {search_string}"
        keyterm_index += 1
        list_indexed_keyterms.append(indexed_keyterm)

    # Load logical chunks
    file_data = load_json(logical_chunks_file)

    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            # For chunks that need merging of content
            if len(items) >= 2:
                merged_para = ""
                for i in range(len(items)):
                    para = items[i]["content"]
                    merged_para += " " + para
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para.strip(), quotation_marks_paragraphs, count)
            # For chunks having only one content
            elif len(items) == 1:
                for contents in items:
                    content = contents["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions based on the filtered paragraphs
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)
    json_output = json.loads(output)

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(json_output, ff, ensure_ascii=False, indent=4)

    print(f"Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}")

    return json_output, extracted_definition_json_path

# Testing the function
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H://management_fee_extraction//extracts_output//output7//"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)







import os
import json
import re

def load_json(file_path):
    """Utility function to load JSON data from a file."""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        indexing_para = merged_para
        if indexing_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    This is a placeholder for the actual implementation of extracting definitions.
    It would process the filtered paragraphs and indexed keyterms to produce output.
    """
    # Assuming this function returns a JSON-serializable object as output.
    return {"definitions": [{"keyterm": keyterm, "paragraph": para} for keyterm, para in zip(list_indexed_keyterms, quotation_marks_paragraphs)]}

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    extracted_definition_json_file_name = os.path.basename(pdf_path)
    extracted_definition_json_file_name = extracted_definition_json_file_name.replace(".pdf", "_generated_definitions.json").replace(".PDF", "_generated_definitions.json")
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms from the JSON file
    data = load_json(extracted_keyterms_json_path)
    search_strings = data["keyTerms"]

    # Index key terms
    list_indexed_keyterms = [str(i) + " " + search_string for i, search_string in enumerate(search_strings)]

    # Load logical chunks
    file_data = load_json(logical_chunks_file)
    
    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            if len(items) >= 2:
                merged_para = ""
                for item in items:
                    merged_para += item["content"] + " "
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count)
            elif len(items) == 1:
                for item in items:
                    content = item["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(output, ff, ensure_ascii=False, indent=4)

    return output, extracted_definition_json_path

# Testing
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H:\management_fee_extraction\extracts_output\output7"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)
import re

def get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param count: The current count of filtered paragraphs.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :return: Updated list of filtered paragraphs and the count.
    """
    # Correct the pattern for regex search
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'

    # Check if the pattern is found in the merged paragraph
    if re.search(pattern, merged_para):
        indexing_para = merged_para

        # Ensure the paragraph is not already in the list
        if indexing_para not in quotation_marks_paragraphs:
            # Correct string concatenation and append the paragraph
            filtered_para = str(count) + " " + merged_para
            print(f"filtered_para: {filtered_para}")

            quotation_marks_paragraphs.append(filtered_para)
            count += 1

    return quotation_marks_paragraphs, count

# Example usage:
search_string = "Management Fee"
merged_para = "This paragraph talks about Management Fee and other details."
count = 1
quotation_marks_paragraphs = []

result, updated_count = get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs)

print("Filtered Paragraphs:", result)
print("Updated Count:", updated_count)





import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of search terms
        :return: List of compiled regex patterns
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content to search within
        :param patterns: List of compiled regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :param patterns: List of compiled regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        for item in data['logicalchunks']:
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge content from the provided JSON data
        :param json_data: Input JSON data as a dictionary
        :return: Dictionary with merged content
        """
        data = json.loads(json.dumps(json_data))
        combined_strings = []

        for item in data['extractsUsingPrescribedLanguage']:
            combined_string = ' '.join(item['content'] for item in item)
            combined_strings.append(combined_string)

        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data to a file
    :param json_file_path: Path to the JSON file
    :param json_data: Data to write to the file
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file)
    except Exception as e:
        print(f"Error dumping JSON file: {e}")

# File paths
data_file_path = "H:/management_fee_extraction/extracts_output/output5/LEGAL-#148874-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
output_dest_dir = "H:/management_fee_extraction/extracts_output/output5"

with open(data_file_path, "r") as json_file:
    data = json.load(json_file)

obj = KeywordFilterMethod()
paragraphs = obj.extract(data)

print(paragraphs)

paragraphs = {"extractsUsingPrescribedLanguage": paragraphs}

file_name = os.path.splitext(os.path.basename(data_file_path))[0]

filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
dump_json_file(filtered_images_json_file_path, paragraphs)

merged_content = obj.merge_content(paragraphs)
merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

print(merged_content_json_file_path)
print(merged_content)

dump_json_file(merged_content_json_file_path, merged_content)






import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        return self.extract_data_with_terms(data, patterns)

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of keywords
        :return: List of compiled regex patterns
        """
        return [re.compile(re.escape(term)) for term in terms]

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content of the node
        :param patterns: List of regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :param patterns: List of regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        logical_chunks = data.get('logicalChunks', [])  # Ensure it's a list
        for item in logical_chunks:
            if isinstance(item, list):  # Check if item is a list
                for node in item:
                    if isinstance(node, dict):  # Check if node is a dictionary
                        content = node.get('content', '')  # Safely get content
                        if self.search_terms_in_content(content, patterns):
                            extracted_data.append(item)
                            break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data: JSON data containing extractsUsingPrescribedLanguage
        :return: Merged content dictionary
        """
        extracts = json_data.get('extractsUsingPrescribedLanguage', [])  # Ensure it's a list
        combined_strings = []
        for item in extracts:
            if isinstance(item, list):  # Check if item is a list
                combined_string = ' '.join(node.get('content', '') for node in item if isinstance(node, dict))
                combined_strings.append(combined_string)
        return {'extractsUsingPrescribedLanguage': combined_strings}

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path: Path to save the JSON file
    :param json_data: Data to save
    :return: None
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))

