

import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data:
        :return:
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms:
        :return:
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content:
        :param patterns:
        :return:
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data:
        :param patterns:
        :return:
        """
        extracted_data = []
        for item in data.get('logicalChunks', []):
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data:
        :return:
        """
        combined_strings = []
        for item in json_data.get('extractsUsingPrescribedLanguage', []):
            combined_string = ' '.join(node['content'] for node in item)
            combined_strings.append(combined_string)
        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path:
    :param json_data:
    :return:
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))












import os
import json

def run_keyterm_definition_extraction(extracted_keyTermsJson_path, pdf_path, output_path):
    def make_output_dir(dir_name, pdf_path):
        output_folder = os.path.join(output_path, dir_name)
        os.makedirs(output_folder, exist_ok=True)
        file_name = os.path.basename(pdf_path).replace(".pdf", ".json").replace(".PDF", ".json")
        output_file_path = os.path.join(output_folder, file_name)
        return output_file_path

    # Initialize variables
    extracted_text = None
    new_pages = ""
    new_keyword_output = ""
    extracted_definition_json_file_name = ""
    extracted_definitions_folder_path = ""
    extracted_definitions = ""

    try:
        # Load the key terms JSON file with error handling
        with open(extracted_keyTermsJson_path, "r") as file:
            try:
                kt_extract = json.load(file)
                print("Successfully loaded the JSON file")
            except json.JSONDecodeError as json_err:
                print(f"JSONDecodeError: {json_err}")
                return None, None

        # Process the search strings
        search_strings = kt_extract.get('keyTerms', [])
        print(f"search_strings: {search_strings}")

        # Extract pages containing the search strings
        pages = find_pages_with_string_in_quotes(pdf_path, search_strings)
        pages_chunked = chunk_data(pages)
        output_final = []

        # Extract definitions for each chunk
        for chunk in pages_chunked:
            output = extract_definitions(chunk, search_strings)
            json_output = json.loads(output)  # Convert string to JSON
            output_final.extend(json_output)

        print(f'output_final: {output_final}')

        # Determine the output file path
        extracted_definitions_folder_path = os.path.join(output_path, "extracted_definitions")
        if not os.path.exists(extracted_definitions_folder_path):
            os.makedirs(extracted_definitions_folder_path)

        input_subfolder_path = os.path.dirname(pdf_path).replace("\\", "/")
        if not os.path.exists(input_subfolder_path):
            os.makedirs(input_subfolder_path)

        extracted_definitions = os.path.join(extracted_definitions_folder_path, input_subfolder_path)
        if not os.path.exists(extracted_definitions):
            os.makedirs(extracted_definitions)

        extracted_definition_json_file_name = os.path.basename(pdf_path).replace(".pdf", "_definitions.json").replace(".PDF", "_definitions.json")
        extracted_definition_json_file_path = os.path.join(extracted_definitions, extracted_definition_json_file_name)
        extracted_definition_json_file_path = extracted_definition_json_file_path.replace("/", "\\")
        print(f'extracted_definition_json_file_path: {extracted_definition_json_file_path}')

        # Write the final output to a JSON file
        with open(extracted_definition_json_file_path, 'w', encoding='utf-8') as ff:
            json.dump(output_final, ff, ensure_ascii=False, indent=4)

        print(f'Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}')

        # Update the index file
        ready_output_files = [{
            "subFolderPath": input_subfolder_path,
            "keyTermDefinitionsJson": extracted_definition_json_file_name
        }]
        index_data = {
            "rootPath": extracted_definitions_folder_path,
            "readyOutputFiles": ready_output_files
        }

        index_json = json.dumps(index_data, indent=4)
        kt_def_output_index_path = os.path.join(extracted_definitions_folder_path, "index.json")
        with open(kt_def_output_index_path, "w") as json_file:
            json_file.write(index_json)

    except Exception as e:
        print(f"Exception: {e}")
        errored_out_files = []
        errored_out_files.append({
            'input_file_name': os.path.basename(pdf_path),
            'input_file_path': pdf_path,
            'error': str(e)
        })
        raise Exception(e)

    return extracted_definition_json_file_path, output_final

# Example usage:
run_keyterm_definition_extraction("path_to_keyterms_json.json", "path_to_pdf.pdf", "output_directory_path")





import os
import json

def run_keyterm_definition_extraction(extracted_keyTermsJson_path, pdf_path, output_path):
    def make_output_dir(dir_name, pdf_path):
        output_folder = os.path.join(output_path, dir_name)
        os.makedirs(output_folder, exist_ok=True)
        file_name = os.path.basename(pdf_path).replace(".pdf", ".json").replace(".PDF", ".json")
        output_file_path = os.path.join(output_folder, file_name)
        return output_file_path

    # Initialize variables
    extracted_text = None
    new_pages = ""
    new_keyword_output = ""
    extracted_definition_json_file_name = ""
    extracted_definitions_folder_path = ""
    extracted_definitions = ""

    try:
        # Load the key terms JSON file
        with open(extracted_keyTermsJson_path, "r") as file:
            kt_extract = json.load(file)
            print("Successfully loaded the JSON file")

        # Process the search strings
        search_strings = kt_extract['keyTerms']
        print(f"search_strings: {search_strings}")

        # Extract pages containing the search strings
        pages = find_pages_with_string_in_quotes(pdf_path, search_strings)
        pages_chunked = chunk_data(pages)
        output_final = []

        # Extract definitions for each chunk
        for chunk in pages_chunked:
            output = extract_definitions(chunk, search_strings)
            json_output = json.loads(output)  # Convert string to JSON
            output_final.extend(json_output)

        print(f'output_final: {output_final}')

        # Determine the output file path
        extracted_definitions_folder_path = os.path.join(output_path, "extracted_definitions")
        if not os.path.exists(extracted_definitions_folder_path):
            os.makedirs(extracted_definitions_folder_path)

        input_subfolder_path = os.path.dirname(pdf_path).replace("\\", "/")
        if not os.path.exists(input_subfolder_path):
            os.makedirs(input_subfolder_path)

        extracted_definitions = os.path.join(extracted_definitions_folder_path, input_subfolder_path)
        if not os.path.exists(extracted_definitions):
            os.makedirs(extracted_definitions)

        extracted_definition_json_file_name = os.path.basename(pdf_path).replace(".pdf", "_definitions.json").replace(".PDF", "_definitions.json")
        extracted_definition_json_file_path = os.path.join(extracted_definitions, extracted_definition_json_file_name)
        extracted_definition_json_file_path = extracted_definition_json_file_path.replace("/", "\\")
        print(f'extracted_definition_json_file_path: {extracted_definition_json_file_path}')

        # Write the final output to a JSON file
        with open(extracted_definition_json_file_path, 'w', encoding='utf-8') as ff:
            json.dump(output_final, ff, ensure_ascii=False, indent=4)

        print(f'Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}')

        # Update the index file
        ready_output_files = [{
            "subFolderPath": input_subfolder_path,
            "keyTermDefinitionsJson": extracted_definition_json_file_name
        }]
        index_data = {
            "rootPath": extracted_definitions_folder_path,
            "readyOutputFiles": ready_output_files
        }

        index_json = json.dumps(index_data, indent=4)
        kt_def_output_index_path = os.path.join(extracted_definitions_folder_path, "index.json")
        with open(kt_def_output_index_path, "w") as json_file:
            json_file.write(index_json)

    except Exception as e:
        print(f"Exception: {e}")
        errored_out_files = []
        errored_out_files.append({
            'input_file_name': os.path.basename(pdf_path),
            'input_file_path': pdf_path,
            'error': str(e)
        })
        raise Exception(e)

    return extracted_definition_json_file_path, output_final

# Example usage:
run_keyterm_definition_extraction("path_to_keyterms_json.json", "path_to_pdf.pdf", "output_directory_path")








import os
import json

def run_keyterm_definition_extraction(extracted_keyTermsJson_path, pdf_path, output_path):

    def make_output_dir(dir_name, pdf_path):
        output_folder = os.path.join(output_path, dir_name)
        os.makedirs(output_folder, exist_ok=True)
        file_name = os.path.basename(pdf_path).replace(".pdf", ".json").replace(".PDF", ".json")
        output_file_path = os.path.join(output_folder, file_name)
        return output_file_path

    # Initialize variables
    extracted_text = None
    new_pages = ""
    new_keyword_output = ""

    try:
        with open(extracted_keyTermsJson_path, "r") as file:
            kt_extract = json.load(file)
            print("Successfully loaded the JSON file")

        # Process the search strings
        search_strings = kt_extract.get("keyTerms", [])

        # Extract pages with strings
        pages = find_pages_with_string_in_quotes(pdf_path, search_strings)

        # Combine pages into a single string
        for i, text in enumerate(pages):
            new_pages += f"{i}: " + " ".join(text.splitlines()) + "\n"

        # Process keywords and generate output
        for i, term in enumerate(kt_extract.get("keyTerms", [])):
            new_keyword_output += f"{i}: {term}\n"

        # Extract definitions based on the processed pages and keywords
        extracted_KTD = extract_definitions(new_pages, new_keyword_output)

        # Convert the result to JSON
        if isinstance(extracted_KTD, str):
            extracted_KTD = json.loads(extracted_KTD)

        # Generate the output file path
        extracted_KTD_json_path = make_output_dir("extracted_key_terms_definitions", pdf_path)

        # Write the JSON output to a file
        with open(extracted_KTD_json_path, 'w', encoding='utf-8') as json_file:
            json.dump(extracted_KTD, json_file, ensure_ascii=False, indent=4)

        print(f"Successfully dumped the extracted definitions JSON to: {extracted_KTD_json_path}")
        return extracted_KTD_json_path, extracted_KTD

    except Exception as e:
        print(f"Exception occurred: {e}")
        return None, None

# Example usage:
# run_keyterm_definition_extraction("path_to_keyterms_json.json", "path_to_pdf.pdf", "output_directory_path")







from pydantic import BaseModel, Field, HttpUrl
from typing import Any, Dict, List, Optional

class AzureBlobConnection(BaseModel):
    auth_type: str = Field("", description="Authentication type for Azure Blob connection")
    mi_client_id: str = Field("", description="Managed Identity client ID")
    account_url: HttpUrl = Field("https://npeedevgtsd01txtmrm.blob.core.windows.net", description="URL of the Azure Blob account")
    doc_container: str = Field("", description="Document container in Azure Blob")

class DocumentPath(BaseModel):
    source: str = Field("", description="Source of the document")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the document path")

class Artifact(BaseModel):
    source: str = Field("", description="Source of the Artifact")
    data: Any = Field(None, description="Data of the artifact")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the artifact")
    extant: bool = Field(False, description="Whether the artifact is extant")

class Question(BaseModel):
    data: List[str] = Field(default_factory=list, description="List of questions")
    source: str = Field("", description="Source of the questions")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the questions")
    extant: bool = Field(False, description="Whether the questions are extant")

class Keyword(BaseModel):
    data: List[str] = Field(default_factory=list, description="List of keywords")
    source: str = Field("", description="Source of the keywords")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the keywords")
    extant: bool = Field(False, description="Whether the keywords are extant")

class Output(BaseModel):
    pageDimensions: Optional[Artifact] = Field(default_factory=Artifact, description="Page dimensions data")
    documentLayout: Optional[Artifact] = Field(default_factory=Artifact, description="Document layout data")
    logicalChunks: Optional[Artifact] = Field(default_factory=Artifact, description="Logical chunks data")
    extractsUsingPrescribedLanguage: Optional[Artifact] = Field(default_factory=Artifact, description="Extracts using prescribed language data")
    keyTerms: Optional[Artifact] = Field(default_factory=Artifact, description="Key terms data")
    keyTermDefinitions: Optional[Artifact] = Field(default_factory=Artifact, description="Key term definitions data")
    aiFilteredExtracts: Optional[Artifact] = Field(default_factory=Artifact, description="AI filtered extracts data")
    mfeesInterpretations: Optional[Artifact] = Field(default_factory=Artifact, description="Management fees interpretations data")
    keyTermInterpretations: Optional[Artifact] = Field(default_factory=Artifact, description="Key term interpretations data")

class Prompt(BaseModel):
    template: str = Field("", description="Prompt template")
    input_variables: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Input variables for the prompt")

class Stage(BaseModel):
    capability: str = Field("", description="Stage capability")
    method: str = Field("", description="Stage method")
    processing_config: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Processing configuration")
    input: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Stage input")
    output: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Stage output")
    prompt: Prompt = Field(default_factory=Prompt, description="Prompt configuration")

class Vars(BaseModel):
    pm_azure_blob_connection: AzureBlobConnection = Field(default_factory=AzureBlobConnection, alias="pm.azure.blob.connection", description="PM Azure Blob connection")
    di_azure_blob_connection: AzureBlobConnection = Field(default_factory=AzureBlobConnection, alias="di.azure.blob.connection", description="DI Azure Blob connection")
    input_document_path: DocumentPath = Field(default_factory=DocumentPath, alias="input.document_path", description="The path of the input document")

class Artifacts(BaseModel):
    pageDimensions: Artifact = Field(default_factory=Artifact, description="Page dimensions artifact")
    documentLayout: Artifact = Field(default_factory=Artifact, description="Document layout artifact")
    logicalChunks: Artifact = Field(default_factory=Artifact, description="Logical chunks artifact")
    extractsUsingPrescribedLanguage: Artifact = Field(default_factory=Artifact, description="Extracts using prescribed language artifact")
    aiFilteredExtracts: Artifact = Field(default_factory=Artifact, description="AI filtered extracts artifact")
    keyTerms: Artifact = Field(default_factory=Artifact, description="Key terms artifact")
    regexFilteredDefinitions: Artifact = Field(default_factory=Artifact, description="Definitions filtered using regex")
    keyTermDefinitions: Artifact = Field(default_factory=Artifact, description="Key term definitions artifact")
    mfeesInterpretations: Artifact = Field(default_factory=Artifact, description="Management fees interpretations artifact")
    keyTermInterpretations: Artifact = Field(default_factory=Artifact, description="Key term interpretations artifact")

class Model(BaseModel):
    version: int = Field(1, description="Model version")
    vars: Vars = Field(..., description="Variables for the model")
    artifacts: Artifacts = Field(..., description="Artifacts for the model")
    stages: List[Stage] = Field(default_factory=list, description="Stages of the model")
    output: Output = Field(default_factory=Output, description="Output of the model")

# Example usage
model = Model(
    version=1,
    vars=Vars(),
    artifacts=Artifacts(),
    stages=[Stage()],
    output=Output()
)
print(model)









from pydantic import BaseModel, Field, HttpUrl
from typing import Any, Dict, List, Optional

class AzureBlobConnection(BaseModel):
    auth_type: str = Field("", description="Authentication type for Azure Blob connection")
    mi_client_id: str = Field("", description="Managed Identity client ID")
    account_url: HttpUrl = Field("https://npeedevgtsd01txtmrm.blob.core.windows.net", description="URL of the Azure Blob account")
    doc_container: str = Field("", description="Document container in Azure Blob")

class DocumentPath(BaseModel):
    source: str = Field("", description="Source of the document")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the document path")

class Artifact(BaseModel):
    source: str = Field("", description="Source of the Artifact")
    data: Any = Field(None, description="Data of the artifact")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the artifact")
    extant: bool = Field(False, description="Whether the artifact is extant")

class Question(BaseModel):
    data: List[str] = Field(default_factory=list, description="List of questions")
    source: str = Field("", description="Source of the questions")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the questions")
    extant: bool = Field(False, description="Whether the questions are extant")

class Keyword(BaseModel):
    data: List[str] = Field(default_factory=list, description="List of keywords")
    source: str = Field("", description="Source of the keywords")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Parameters for the keywords")
    extant: bool = Field(False, description="Whether the keywords are extant")

class Output(BaseModel):
    pageDimensions: Optional[Artifact] = Field(default_factory=Artifact, description="Page dimensions data")
    documentLayout: Optional[Artifact] = Field(default_factory=Artifact, description="Document layout data")
    logicalChunks: Optional[Artifact] = Field(default_factory=Artifact, description="Logical chunks data")
    extractsUsingPrescribedLanguage: Optional[Artifact] = Field(default_factory=Artifact, description="Extracts using prescribed language data")
    keyTerms: Optional[Artifact] = Field(default_factory=Artifact, description="Key terms data")
    keyTermDefinitions: Optional[Artifact] = Field(default_factory=Artifact, description="Key term definitions data")
    aiFilteredExtracts: Optional[Artifact] = Field(default_factory=Artifact, description="AI filtered extracts data")
    mfeesInterpretations: Optional[Artifact] = Field(default_factory=Artifact, description="Management fees interpretations data")
    keyTermInterpretations: Optional[Artifact] = Field(default_factory=Artifact, description="Key term interpretations data")

class Prompt(BaseModel):
    template: str = Field("", description="Prompt template")
    input_variables: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Input variables for the prompt")

class Stage(BaseModel):
    capability: str = Field("", description="Stage capability")
    method: str = Field("", description="Stage method")
    processing_config: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Processing configuration")
    input: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Stage input")
    output: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Stage output")
    prompt: Prompt = Field(default_factory=Prompt, description="Prompt configuration")

class Vars(BaseModel):
    pm_azure_blob_connection: AzureBlobConnection = Field(default_factory=AzureBlobConnection, alias="pm.azure.blob.connection", description="PM Azure Blob connection")
    di_azure_blob_connection: AzureBlobConnection = Field(default_factory=AzureBlobConnection, alias="di.azure.blob.connection", description="DI Azure Blob connection")
    input_document_path: DocumentPath = Field(default_factory=DocumentPath, alias="input.document_path", description="The path of the input document")

class Artifacts(BaseModel):
    pageDimensions: Artifact = Field(default_factory=Artifact, description="Page dimensions artifact")
    documentLayout: Artifact = Field(default_factory=Artifact, description="Document layout artifact")
    logicalChunks: Artifact = Field(default_factory=Artifact, description="Logical chunks artifact")
    extractsUsingPrescribedLanguage: Artifact = Field(default_factory=Artifact, description="Extracts using prescribed language artifact")
    aiFilteredExtracts: Artifact = Field(default_factory=Artifact, description="AI filtered extracts artifact")
    keyTerms: Artifact = Field(default_factory=Artifact, description="Key terms artifact")
    regexFilteredDefinitions: Artifact = Field(default_factory=Artifact, description="Definitions filtered using regex")
    keyTermDefinitions: Artifact = Field(default_factory=Artifact, description="Key term definitions artifact")
    mfeesInterpretations: Artifact = Field(default_factory=Artifact, description="Management fees interpretations artifact")
    keyTermInterpretations: Artifact = Field(default_factory=Artifact, description="Key term interpretations artifact")

class Model(BaseModel):
    version: int = Field(1, description="Model version")
    vars: Vars = Field(..., description="Variables for the model")
    artifacts: Artifacts = Field(..., description="Artifacts for the model")
    stages: List[Stage] = Field(default_factory=list, description="Stages of the model")
    output: Output = Field(default_factory=Output, description="Output of the model")

# Example usage
model = Model(
    version=1,
    vars=Vars(),
    artifacts=Artifacts(),
    stages=[Stage()],
    output=Output()
)
print(model)










{
  "version": 1,
  "vars": {
    "pm.azure.blob.connection": {
      "auth_type": "MANAGED",
      "mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",
      "account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",
      "doc_container": "npeedevgtsd01txtmrm-strgcont-txtmrm-01"
    },
    "di.azure.blob.connection": {
      "auth_type": "MANAGED",
      "mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",
      "account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",
      "doc_container": "npeedevgtsd01txtmrm-strgcont-txtmrm-01"
    },
    "input.document_path": {
      "source": "AzureBlob",
      "params": {
        "connection": "{vars.pm.azure.blob.connection}",
        "storage_location": "{payload.documents[0].storagePath.absolute_path}"
      }
    }
  },
  "artifacts": {
    "pageDimensions": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "documentLayout": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "logicalChunks": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "extractsUsingPrescribedLanguage": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "aiFilteredExtracts": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "keyTerms": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "regexFilteredDefinitions": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "keyTermDefinitions": {
      "source": "Transient",
      "data": {
        "KeyTerm": [],
        "KeyTerm2": []
      },
      "params": {},
      "extant": true
    },
    "mFeesInterpretations": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "keyTermInterpretations": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "questions": {
      "data": [
        "Can you explain the structure, calculation, and reduction of the Management Fee for each Limited Partner and Series?",
        "How does the Management Fee relate to Direct Investments, other fees, and the Fund's investments in Primary and Secondary Investee Funds?",
        "Can you provide details on the Lead Investor's agreement, commitment, and understanding of Management Fees?",
        "How are the Advisor Compensation, Management Fee, and Incentive Fee determined as per the agreement?",
        "How are the Management Fees affected by the Fund's investment in any Primary Investee Fund or Secondary Investee Fund?",
        "Can you explain the calculation, payment timing, and due dates of the Management Fees?",
        "How are the Excess Start-Up Costs Reduction applied and how do they affect the Management Fees?",
        "How are the payments of Management Fees handled by the General Partner for a Limited Partner's share with respect to a Class?",
        "What are the details of the Management Fee as per different sections and programs?",
        "How is the Management Fee calculated and reduced in relation to various investments and fees?",
        "What types of fees will the Fund pay in connection with its investments?",
        "How is the Management Fee calculated for each Limited Partner after specific dates or events?",
        "What happens if one or more interest holders in a Designated Partner has a certain Capital Commitment?",
        "How is the US dollar amount determined for a Portfolio Investment in a non-US currency?",
        "Who is responsible for the expenses and out-of-pocket costs related to the organization, admission, and maintenance of interest in the Fund?"
      ],
      "source": "Transient",
      "params": {},
      "extant": false
    },
    "keywords": {
      "data": [
        "Management Fee",
        "Management fee",
        "Servicing Fee",
        "Servicing Fee",
        "Investment Advisory Fee",
        "Investment Advisory Fee",
        "Compensation",
        "Remuneration",
        "Expenses",
        "AIFM Fee",
        "AIFM Fee"
      ],
      "source": "Transient",
      "params": {},
      "extant": false
    },
    "output": {
      "pageDimensions": "{artifacts.pageDimensions.data}",
      "documentLayout": "{artifacts.documentLayout.data}",
      "logicalChunks": "{artifacts.logicalChunks.data}",
      "extractsUsingPrescribedLanguage": "{artifacts.extractsUsingPrescribedLanguage.data}",
      "keyTerms": "{artifacts.keyTerms.data}",
      "aifilteredExtracts": "{artifacts.aiFilteredExtracts.data}",
      "mfeesInterpretations": "{artifacts.mFeesInterpretations.data}",
      "keyTermInterpretations": "{artifacts.keyTermInterpretations.data}"
    }
  },
  "stages": [
    {
      "capability": "DocumentParsing",
      "method": "AzureDocumentIntelligence",
      "processing_config": {
        "processing_engine": "ADI",
        "model": "prebuilt_layout",
        "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
        "key": "dc7c0c7c3a9a4371b7dc983ac7b618b"
      },
      "input": {
        "document_path": "{vars.input.document_path}"
      },
      "output": {
        "(artifacts.logicalChunks.data)": "logicalChunks",
        "(artifacts.documentLayout.data)": "documentLayout",
        "(artifacts.pageDimensions.data)": "pageDimensions"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "RegexFiltering",
      "input": {
        "logicalChunks": "{artifacts.logicalChunks.data}",
        "keywords": "{artifacts.keywords.data}"
      },
      "output": {
        "(artifacts.extractsUsingPrescribedLanguage.data)": "extractsUsingPrescribedLanguage"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "Prompting",
      "processing_config": {
        "processing_engine": "Azure_openai",
        "openai_api_type": "azure",
        "azure_endpoint": "https://ssgpt_predev.openai.azure.com/",
        "openai_api_version": "2023_07_01_preview",
        "openai_api_key": "43d6ab1448834574807eff7a195f76f3",
        "deployment_name": "ssgpt_predev_1_eastus",
        "model": "gpt_4_32k"
      },
      "prompt": {
        "template": "# Objective You will be given a list of paragraphs. You need to identify the paragraphs that help answer one or more of the questions. # Questions {questions} # Paragraphs {paragraphs} # Steps Step 1: Read through each paragraph and determine if it helps answer one or many of the questions. Step 2: If the paragraph helps answer the question, include the paragraph index in the response. If the paragraph index is already present in the response, do not repeat. If the paragraph does not help answer any of the questions, don't include the paragraph index in the response. Step 3: Respond back with the indexes of the paragraphs and indexes of the questions which were answered in the paragraph. # Output Instructions 1. Generate a JSON response only. Do not give any text apart from the JSON (No prose). 2. The response should be a list of objects containing the fields for each selected paragraph index. 3. Create a field 'indexes': Paragraph index which helped answer one or more of the questions. 4. Create a field 'questions': List of the question indexes that was answered by the paragraph. 5. Don't add anything like json or 'output' in the output.",
        "input_variables": {
          "questions": {
            "indexing_required": true,
            "data": "{artifacts.questions.data}"
          },
          "paragraphs": {
            "indexing_required": true,
            "data": "{artifacts.aiFilteredExtracts.data}"
          }
        }
      },
      "output": {
        "(artifacts.keyTerms.data)": "keyTerms"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "RegexFiltering",
      "input": {
        "logicalChunks": "{artifacts.logicalChunks.data}",
        "keyTerms": "{artifacts.keyTerms.data}"
      },
      "output": {
        "(artifacts.regexFilteredDefinitions.data)": "regexFilteredDefinitions"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "Prompting",
      "processing_config": {
        "processing_engine": "Azure_openai",
        "openai_api_type": "azure",
        "azure_endpoint": "https://ssgpt_predev.openai.azure.com/",
        "openai_api_version": "2023_07_01_preview",
        "openai_api_key": "43d6ab1448834574807eff7a195f76f3",
        "deployment_name": "ssgpt_predev_1_eastus",
        "model": "gpt_4_32k"
      },
      "prompt": {
        "template": "# Objective You will be given a list of paragraphs. Your task is to identify the key terms present in each of the provided paragraphs. There can be one or more key terms in each of the paragraphs. Paragraphs {paragraphs} # Steps to identify the key terms from the Paragraphs: 1. Read through each provided paragraph and identify the key terms which would need some definition to understand the context of the paragraph. 2. Title Case is when only the initial letter of every word is in uppercase. The key terms are always written in Title Case. 3. Identify and extract all the key terms that may possibly have their definitions or more information explained in other parts of the document which are not provided in the Paragraphs. 4. These key terms should be significant to the context. These words are not common and are unique. 5. The key terms should not include the terms that are related to 'fees'. 6. The key terms should not include the name of organization, fund, or partnership. 7. Respond back with the list of key terms against each paragraph. 8. If no key terms are identified against a paragraph, respond back with an empty list. # Output Instructions ##Output Format 1. Generate a JSON response only. Do not give any text apart from the JSON (No prose). If no key terms are identified, then provide an empty list against the specific paragraph. 2. Extract the key terms exactly as they are from the paragraph without making any changes to the case of the words. ##Output Structure 1. The response should be a list of objects corresponding to each paragraph with the below provided fields. 2. Create a field 'index': Should be a paragraph index from the input list of paragraphs for which the key terms are extracted. 3. Create a field 'keyTerms': Contains a list of extracted key terms from the specific paragraph.",
        "input_variables": {
          "paragraphs": {
            "indexing_required": true,
            "data": "{artifacts.aiFilteredExtracts.data}"
          }
        }
      },
      "output": {
        "(artifacts.keyTerms.data)": "keyTerms"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "RegexFiltering",
      "input": {
        "logicalChunks": "{artifacts.logicalChunks.data}",
        "keyTerms": "{artifacts.keyTerms.data}"
      },
      "output": {
        "(artifacts.regexFilteredDefinitions.data)": "regexFilteredDefinitions"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "Prompting",
      "processing_config": {
        "processing_engine": "Azure_openai",
        "openai_api_type": "azure",
        "azure_endpoint": "https://ssgpt_predev.openai.azure.com/",
        "openai_api_version": "2023_07_01_preview",
        "openai_api_key": "43d6ab1448834574807eff7a195f76f3",
        "deployment_name": "ssgpt-40",
        "model": "gpt-4-32k"
      },
      "prompt": {
        "template": "# Objective You are tasked to interpret and summarize legal and financial content related to investment funds and management fees. # Steps to Write Interpretations 1. Read through each paragraph provided in the 'Management Fees Related Extracts' and wherever required use the paragraphs in the 'Keyterm Definition Extracts' as reference to understand the content of the paragraph for interpretation. 2. From the understanding of each paragraph in the 'Management Fees Related Extracts' create easily understandable interpretations related to the calculation of Management Fees and its duration. 3. Read through each paragraph provided in the 'Keyterm Definition Extracts' and wherever required use the paragraphs in the 'Management Fees Related Extracts' as reference to understand the content of the paragraph for interpretation. 4. From the understanding of each paragraph in the 'Keyterm Definition Extracts' create easily understandable interpretations related to the definitions of key terms. 5. If the paragraphs in 'Keyterm Definition Extracts' are defining the dates, the values of those might be found in one of the paragraphs in the 'Management Fees Related Extracts'. 6. Break down the interpretation into small sentences. 7. Respond back with the separate interpretations against each paragraph in 'Management Fees Related Extracts' and 'Keyterm Definition Extracts'. # Understanding Indexes The contents of 'Management Fees Related Extracts' and 'Keyterm Definition Extracts' are a numbered list of paragraphs with index starting from 0. # Management Fees Related Extracts {mFeesInterpretationsExtracts} # Keyterm Definition Extracts {KeyTermDefinitionExtracts} # Output Instructions ## Output Structure 1. The response should be a list of objects containing the below provided fields. 2. Create a field 'mFeesInterpretations': Contains the list of fields 'index' and 'interpretation' from 'Management Fees Related Extracts' for which the interpretation is created. 3. Create a field 'keyTermInterpretations': Contains the list of fields 'index' and 'interpretation' from 'Keyterm Definition Extracts' for which the interpretation is created. 4. Each of the above created fields should have the list of objects containing the following subfields: 1. Create a field 'index': Paragraph index from 'Management Fees Related Extracts' for which the interpretation is created. 2. Create a field 'interpretation': List of easily understandable interpretations for the paragraphs in the index. ## Output Format 1. Generate a JSON response only. 2. Do not generate markdown code snippets in the output. Do not give any text apart from the JSON (No prose). 3. Use only ASCII characters to write the interpretations.",
        "input_variables": {
          "KeyTermDefinitionExtracts": {
            "indexing_required": true,
            "data": "{artifacts.keyTermDefinitions.data}"
          },
          "mFeesInterpretationsExtracts": {
            "indexing_required": true,
            "data": "{artifacts.aiFilteredExtracts.data}"
          }
        }
      },
      "output": {
        "(artifacts.mFeesInterpretations.data)": "mFeesInterpretations",
        "(artifacts.keyTermInterpretations.data)": "keyTermInterpretations"
      }
    }


  ]
}







{
  "version": 1,
  "vars": {
    "pm.azure.blob.connection": {
      "auth_type": "MANAGED",
      "mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",
      "account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",
      "doc_container": "npeedevgtsd01txtmrm-strgcont-txtmrm-01"
    },
    "di.azure.blob.connection": {
      "auth_type": "MANAGED",
      "mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",
      "account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",
      "doc_container": "npeedevgtsd01txtmrm-strgcont-txtmrm-01"
    },
    "input.document_path": {
      "source": "AzureBlob",
      "params": {
        "connection": "{vars.pm.azure.blob.connection}",
        "storage_location": "{payload.documents[0].storagePath.absolute_path}"
      }
    }
  },
  "artifacts": {
    "pageDimensions": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "documentLayout": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "logicalChunks": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "extractsUsingPrescribedLanguage": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "aiFilteredExtracts": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "keyTerms": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "regexFilteredDefinitions": {
      "source": "Transient",
      "data": [],
      "params": {},
      "extant": false
    },
    "keyTermDefinitions": {
      "source": "Transient",
      "data": {
        "KeyTerm1": [],
        "KeyTerm2": []
      },
      "params": {},
      "extant": true
    },
    "feesInterpretations": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "keyTermInterpretations": {
      "source": "AzureBlob",
      "data": [],
      "params": {
        "connection": "{vars.di.azure.blob.connection}",
        "absolute_location": "<path_to_file>",
        "encoding": "utf_8"
      },
      "extant": false
    },
    "questions": {
      "data": [
        "Can you explain the structure, calculation, and reduction of the Management Fee for each Limited Partner and Series?",
        "How does the Management Fee relate to Direct Investments, other fees, and the Fund's investments in Primary and Secondary Investee Funds?",
        "Can you provide details on the Lead Investor's agreement, commitment, and understanding of Management Fees?",
        "How are the Advisor Compensation, Management Fee, and Incentive Fee determined as per the agreement?",
        "How are the Management Fees affected by the Fund's investment in any Primary Investee Fund or Secondary Investee Fund?",
        "Can you explain the calculation, payment timing, and due dates of the Management Fees?",
        "How are the Excess Start-Up Costs Reduction applied and how do they affect the Management Fees?",
        "How are the payments of Management Fees handled by the General Partner for a Limited Partner's share with respect to a Class?",
        "What are the details of the Management Fee as per different sections and programs?",
        "How is the Management Fee calculated and reduced in relation to various investments and fees?",
        "What types of fees will the Fund pay in connection with its investments?",
        "How is the Management Fee calculated for each Limited Partner after specific dates or events?",
        "What happens if one or more interest holders in a Designated Partner has a certain Capital Commitment?",
        "How is the US dollar amount determined for a Portfolio Investment in a non-US currency?",
        "Who is responsible for the expenses and out-of-pocket costs related to the organization, admission, and maintenance of interest in the Fund?"
      ],
      "source": "Transient",
      "params": {},
      "extant": false
    },
    "keywords": {
      "data": [
        "Management Fee",
        "Servicing Fee",
        "Investment Advisory Fee",
        "Compensation",
        "Remuneration",
        "Expenses",
        "AIFM Fee"
      ],
      "source": "Transient",
      "params": {},
      "extant": false
    }
  },
  "output": {
    "pageDimensions": "{artifacts.pageDimensions.data}",
    "documentLayout": "{artifacts.documentLayout.data}",
    "logicalChunks": "{artifacts.logicalChunks.data}",
    "extractsUsingPrescribedLanguage": "{artifacts.extractsUsingPrescribedLanguage.data}",
    "keyTerms": "{artifacts.keyTerms.data}",
    "aiFilteredExtracts": "{artifacts.aiFilteredExtracts.data}",
    "feesInterpretations": "{artifacts.feesInterpretations.data}",
    "keyTermInterpretations": "{artifacts.keyTermInterpretations.data}"
  },
  "stages": [
    {
      "capability": "DocumentParsing",
      "method": "AzureDocumentIntelligence",
      "processing_config": {
        "processing_engine": "ADI",
        "model": "prebuilt_layout",
        "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
        "key": "dc7c0c7c3a9a4371b7dc983ac7b618b"
      },
      "input": {
        "document_path": "{vars.input.document_path}"
      },
      "output": {
        "logicalChunks": "{artifacts.logicalChunks.data}",
        "documentLayout": "{artifacts.documentLayout.data}",
        "pageDimensions": "{artifacts.pageDimensions.data}"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "RegexFiltering",
      "input": {
        "logicalChunks": "{artifacts.logicalChunks.data}",
        "keywords": "{artifacts.keywords.data}"
      },
      "output": {
        "extractsUsingPrescribedLanguage": "{artifacts.extractsUsingPrescribedLanguage.data}"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "Prompting",
      "processing_config": {
        "processing_engine": "Azure_openai",
        "openai_api_type": "azure",
        "azure_endpoint": "https://ssgpt_predev.openai.azure.com/",
        "openai_api_version": "2023_07_01_preview",
        "openai_api_key": "43d6ab1448834574807eff7a195f76f3",
        "deployment_name": "ssgpt_predev_1_eastus",
        "model": "gpt_4_32k"
      },
      "prompt": {
        "template": "# Objective\nYou will be given a list of paragraphs. You need to identify the paragraphs that help answer one or more of the questions.\n\n# Questions\n(questions)\n\n# Paragraphs\n(paragraphs)\n\n# Steps\nStep 1: Read through each paragraph and determine if it helps answer one or many of the questions.\nStep 2: If the paragraph helps answer the question, include the paragraph index in the response. If the paragraph index is already present in the response, do not repeat. If the paragraph does not help answer any of the questions, don't include the paragraph index in the response.\nStep 3: Respond back with the indexes of the paragraphs and indexes of the questions which were answered in the paragraph.\n\n# Output Instructions\n1. Generate a JSON response only. Do not give any text apart from the JSON (No prose).\n2. The response should be a list of objects containing the fields for each selected paragraph index.\n3. Create a field 'indexes': Paragraph index which helped answer one or more of the questions.\n4. Create a field 'questions': List of the question indexes that was answered by the paragraph.\n5. Don't add anything like 'json' or any other text in the output.",
        "input_variables": {
          "questions": {
            "indexing_required": true,
            "data": "{artifacts.questions.data}"
          }
        }
      },
      "output": {
        "keyTerms": "{artifacts.keyTerms.data}"
      }
    },
    {
      "capability": "GenAIExtraction",
      "method": "Prompting",
      "processing_config": {
        "processing_engine": "Azure_openai",
        "openai_api_ty








Hi team,

From the start, I have faced issues with Zoom meetings that are still not resolved. Occasionally, the problem recurs. Additionally, I have encountered VDI issues where typing is delayed. This month, my VDI (DaaS DC3) stopped working, and I raised a ticket. They assigned me another DC3 PRI2, which worked until Saturday evening, but now I am facing VDI issues again.

They have assigned me three VDIs: DaaS DC3 PRI (1), DaaS DC3 PRI (2), and DaaS GDC PRI. Instead of DC3 PRI (1), they provided DC3 PRI (2), which was working, but now it has been removed. I have also checked DC3 PRI (1) and DaaS GDC PRI, but they are not working either. To resolve this issue, I called the desktop support team at 08045248999 and explained my situation. They raised a ticket (ticket number: INC29179773). Previously, I faced VDI issues and raised different tickets as well. I can provide those ticket numbers if needed.

How can we complete our work if we keep facing these issues? Please treat this as a high priority and provide a dedicated VDI for me.

Thank you.






Is this conversation helpful so far?






ChatGPT can make mistakes. Check important






import pandas as pd
import json

# Assuming ple_json_file_path and ground_truth_rouge_score are already defined
# Replace ".json" with "Prescribed_language_score.json" in the file path
ground_truth_rouge_score_path = ple_json_file_path.replace(".json", "Prescribed_language_score.json")

print(ground_truth_rouge_score_path)

# Dump the ground_truth_rouge_score to a JSON file
with open(ground_truth_rouge_score_path, 'w') as fjson:
    json.dump(ground_truth_rouge_score, fjson)

print('Successfully dumped ground_truth_rouge_score json file')

# Check the structure of ground_truth_rouge_score
print(f"Structure of ground_truth_rouge_score: {type(ground_truth_rouge_score)}")
print(f"Example entry: {ground_truth_rouge_score[0] if isinstance(ground_truth_rouge_score, list) else 'Not a list'}")

# Create a DataFrame from the ground_truth_rouge_score dictionary
ground_truth_rouge_score_df = pd.DataFrame(ground_truth_rouge_score)

# Check the structure of the DataFrame
print(f"DataFrame structure:\n{ground_truth_rouge_score_df.head()}")

# Ensure the 'results' column exists and is a list
if 'results' in ground_truth_rouge_score_df.columns and isinstance(ground_truth_rouge_score_df['results'].iloc[0], list):
    # Explode the 'results' column
    ground_truth_rouge_score_df = ground_truth_rouge_score_df.explode('results')

    # Create a new DataFrame from the exploded 'results' column
    score_df = pd.DataFrame(list(ground_truth_rouge_score_df["results"]))

    # Concatenate the original DataFrame (without 'results' column) with the new DataFrame
    final_score_df = pd.concat([ground_truth_rouge_score_df.drop(columns=["results"]), score_df], axis=1)

    # Generate the CSV file name by replacing ".json" with ".csv" in the original path
    csv_file_name = ground_truth_rouge_score_path.replace(".json", ".csv")

    # Save the final DataFrame to a CSV file
    final_score_df.to_csv(csv_file_name, index=False)
    print(f"{csv_file_name} groundtruth scores saved successfully!")
else:
    print("Error: 'results' column not found or not in expected format.")






import json

def clean_json_data(data):
    """
    Recursively remove newline characters from JSON data.
    
    :param data: JSON data (dict, list, or str)
    :return: Cleaned JSON data
    """
    if isinstance(data, dict):
        return {key: clean_json_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [clean_json_data(item) for item in data]
    elif isinstance(data, str):
        return data.replace('\n', ' ')
    else:
        return data

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON file after removing newline characters.
    
    :param json_file_path: Path to the JSON file
    :param json_data: JSON data to be dumped
    :return: None
    """
    try:
        cleaned_data = clean_json_data(json_data)
        with open(json_file_path, "w") as json_file:
            json.dump(cleaned_data, json_file, indent=4)
        print(f"{json_file_path} created successfully!")
    except Exception as e:
        print(f"Failed to create {json_file_path}: {e}")

# Example usage
json_data = {
    "keyTerms": [
        "Affiliate",
        "Closing Date",
        "Direct Investment",
        "General Partner"
    ],
    "description": "This is a description\nwith newlines\nthat should be removed."
}

dump_json_file("output.json", json_data)






Please generate a JSON object based on the following delimited input "#####". The JSON object must include the keys "management_fee", "paragraph_1", and "paragraph_2", with the corresponding values extracted from the input text. Ensure the JSON object is properly formatted, starting and ending correctly with curly braces and following proper JSON syntax.

Input:
#####
management_fee: 2.5%
paragraph_1: This is the first paragraph of the input text. It provides an introduction and context.
paragraph_2: This is the second paragraph of the input text. It provides additional details and information.
#####

Expected JSON output:
{
  "management_fee": "2.5%",
  "paragraph_1": "This is the first paragraph of the input text. It provides an introduction and context.",
  "paragraph_2": "This is the second paragraph of the input text. It provides additional details and information."
}







import json

# Step 1: Original string response
response = '{"paragraph_1": "31 55075046_36 65777438_1 (b) The Management Fee lu de any related profits) paragraph_3": "and similar fees paid on behalf of the Fund, includ ing such expenses", "paragraph_4": "u al to (i) the Limited Partner\'s total Capital Contributions in respect of Realized"}'

# Step 2: Parse the string into a JSON object
try:
    json_object = json.loads(response)
except json.JSONDecodeError as e:
    print("JSON decoding failed:", e)
else:
    # Step 3: Save the JSON object to a file
    with open('response.json', 'w') as json_file:
        json.dump(json_object, json_file, indent=4)
    
    print("JSON file created successfully.")




import json

# The given string response
response = """ "paragraph_1": "31 55075046_36 65777438_1 (b) The Management Fee lu de any related profits)
paragraph_3": "and similar fees paid on behalf of the Fund, includ ing such expenses

paragraph_4": u al to (i) the Limited Partner's total Capital Contributions in respect of Realized """

# Splitting the response into lines
lines = response.strip().split('\n')

# Initialize an empty dictionary to hold the JSON data
json_data = {}

# Iterate over each line and process it
for line in lines:
    # Splitting the key and value
    if ': ' in line:
        key, value = line.split(': ', 1)
        # Removing any leading or trailing quotes and whitespace from key and value
        key = key.strip().strip('"')
        value = value.strip().strip('"')
        # Assigning the key-value pair to the dictionary
        json_data[key] = value

# Convert the dictionary to a JSON string
json_response = json.dumps(json_data, indent=4)

# Printing the JSON response
print(json_response)









import json

# Define the input string
input_string = """
(b) The Management Fee lu de any related profits)
" paragraph_3": "and similar fees paid on behalf of the Fund, includ ing such expenses
"paragraph_4": u al to (i) the Limited Partner's total Capital Contributions in respect of Realized
"""

# Clean and prepare the input string
input_string = input_string.strip()

# Create a dictionary to hold the key-value pairs
data = {}

# Split the input string by lines
lines = input_string.split('\n')

# Process the first line which doesn't have a key
data["Paragraph_1"] = lines[0].strip()

# Process the remaining lines to extract key-value pairs
for line in lines[1:]:
    key_value = line.split(':', 1)  # Split based on the first occurrence of ':'
    if len(key_value) == 2:
        key = key_value[0].strip().strip('" ')  # Remove leading/trailing spaces and quotes
        value = key_value[1].strip().strip('" ')  # Remove leading/trailing spaces and quotes
        data[key] = value

# Define the output JSON file path
json_file_path = "snippets.json"

# Write the dictionary to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(data, json_file, indent=4)

print(f"JSON file '{json_file_path}' has been created successfully.")







import json

# Define the input string
input_string = """
(b) The Management Fee lu de any related profits)
" paragraph_3": "and similar fees paid on behalf of the Fund, includ ing such expenses
"paragraph_4": u al to (i) the Limited Partner's total Capital Contributions in respect of Realized
"""

# Clean and prepare the input string
input_string = input_string.strip()

# Create a dictionary to hold the key-value pairs
data = {}

# Manually parse the string to extract key-value pairs
lines = input_string.split('\n')

# First line which doesn't have a key
data["Paragraph_1"] = lines[0].strip()

# Process the rest of the lines
for line in lines[1:]:
    key_value = line.split(':', 1)  # Split based on the first occurrence of ':'
    if len(key_value) == 2:
        key = key_value[0].strip().strip('" ')
        value = key_value[1].strip().strip('" ')
        data[key] = value

# Define the output JSON file path
json_file_path = "snippets.json"

# Write the dictionary to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(data, json_file, indent=4)

print(f"JSON file '{json_file_path}' has been created successfully.")





import json

# Define the input string
input_string = """
(b) The Management Fee lu de any related profits)

" paragraph_3": "and similar fees paid on behalf of the Fund, includ ing such expenses

"paragraph_4": u al to (i) the Limited Partner's total Capital Contributions in respect of Realized
"""

# Split the input string by lines
lines = input_string.strip().split('\n')

# Initialize an empty dictionary to hold the key-value pairs
data = {}

# Process each line to extract key-value pairs
for line in lines:
    # Split the line into key and value based on the first occurrence of ':'
    key_value = line.split(':', 1)
    if len(key_value) == 2:
        key = key_value[0].strip().strip('" ')  # Remove leading/trailing spaces and quotes
        value = key_value[1].strip().strip('" ')  # Remove leading/trailing spaces and quotes
        data[key] = value

# Define the output JSON file path
json_file_path = "snippets.json"

# Write the dictionary to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(data, json_file, indent=4)

print(f"JSON file '{json_file_path}' has been created successfully.")








import re
import json

# Noisy text containing embedded JSON-like structure
text = '''
json\n{\n

"paragraph_1": the Fund the capital subscription set (a)\n

"paragraph_2": any right to pa Related Firm) from the conduct ofthat of (and then only to the extent contemplated by this Agreement).

"paragraph_3": The Limited Partners acknowledge and 10.7.2. Fund),

any Portfolio Direct 35 55075046_36 65777438_1"\n}\n```'

# Regular expression to extract the JSON-like content
pattern = r'\{(?:[^{}]|[\n])*?\}'

# Extracting the JSON-like content
match = re.search(pattern, text)

if match:
    json_like_content = match.group()
    # Clean up the JSON-like content
    json_like_content = json_like_content.replace('json', '').replace('```', '').strip()
    
    # Attempt to parse as JSON object
    try:
        # Correct the format of the JSON-like content
        cleaned_content = re.sub(r'[\n\r]+', '', json_like_content)
        cleaned_content = re.sub(r'\s*([a-zA-Z0-9_]+)\s*:', r'"\1":', cleaned_content)
        
        # Load as JSON object
        json_data = json.loads(cleaned_content)
        print(json_data)
        
        # Write to a JSON file
        with open('output.json', 'w') as f:
            json.dump(json_data, f, indent=4)
        
        print("JSON file 'output.json' has been created successfully.")
    
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    
else:
    print("No JSON-like structure found in the text.")









s

import re
import json

# Noisy text containing embedded JSON-like structure
text = '''
json\n{\n

"paragraph_1": the Fund the capital subscription set (a)\n

"paragraph_2": any right to pa Related Firm) from the conduct ofthat of (and then only to the extent contemplated by this Agreement).

"paragraph_3": The Limited Partners acknowledge and 10.7.2. Fund),

any Portfolio Direct 35 55075046_36 65777438_1"\n}\n```'

# Regular expression to extract the JSON-like content
pattern = r'\{(?:[^{}]|[\n])*?\}'

# Extracting the JSON-like content
match = re.search(pattern, text)

if match:
    json_like_content = match.group()
    # Clean up the JSON-like content
    json_like_content = json_like_content.replace('json', '').replace('```', '').strip()
    
    # Attempt to parse as JSON object
    try:
        # Correct the format of the JSON-like content
        # Remove newlines and ensure proper quoting of keys
        cleaned_content = re.sub(r'[\n\r]+', '', json_like_content)
        cleaned_content = re.sub(r'(?<!\\)"', r'\\"', cleaned_content)
        
        # Add double quotes around keys
        cleaned_content = re.sub(r'([^\s:]+)', r'"\1"', cleaned_content)
        
        # Load as JSON object
        json_data = json.loads(cleaned_content)
        print(json_data)
        
        # Write to a JSON file
        with open('output.json', 'w') as f:
            json.dump(json_data, f, indent=4)
        
        print("JSON file 'output.json' has been created successfully.")
    
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    
else:
    print("No JSON-like structure found in the text.")




import re
import json

# Noisy text containing embedded JSON-like structure
text = '''
json\n{\n

"paragraph_1": the Fund the capital subscription set (a)\n

"paragraph_2": any right to pa Related Firm) from the conduct ofthat of (and then only to the extent contemplated by this Agreement).

"paragraph_3": The Limited Partners acknowledge and 10.7.2. Fund),

any Portfolio Direct 35 55075046_36 65777438_1"\n}\n```'

# Regular expression to extract the JSON-like content
pattern = r'\{(?:[^{}]|[\n])*?\}'

# Extracting the JSON-like content
match = re.search(pattern, text)

if match:
    json_like_content = match.group()
    # Clean up the JSON-like content
    json_like_content = json_like_content.replace('json', '').replace('```', '').strip()
    
    # Attempt to load as JSON object
    try:
        # Correcting the JSON-like content format
        json_like_content = re.sub(r'\n+', ' ', json_like_content)  # Replace multiple newlines with a single space
        json_like_content = json_like_content.replace('\\"', '"')  # Replace escaped double quotes
        
        # Ensure the structure is valid JSON by wrapping it with curly braces
        json_data = json.loads('{' + json_like_content + '}')
        print(json_data)
        
        # Write to a JSON file
        with open('output.json', 'w') as f:
            json.dump(json_data, f, indent=4)
        
        print("JSON file 'output.json' has been created successfully.")
    
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    
else:
    print("No JSON-like structure found in the text.")








import re
import json

# Noisy text containing embedded JSON-like structure
text = '''
json\n{\n

"paragraph_1": the Fund the capital subscription set (a)\n

"paragraph_2": any right to pa Related Firm) from the conduct ofthat of (and then only to the extent contemplated by this Agreement).

"paragraph_3": The Limited Partners acknowledge and 10.7.2. Fund),

any Portfolio Direct 35 55075046_36 65777438_1"\n}\n```'

# Regular expression to extract the JSON-like content
pattern = r'\{(?:[^{}]|[\n])*?\}'

# Extracting the JSON-like content
match = re.search(pattern, text)

if match:
    json_like_content = match.group()
    # Clean up the JSON-like content
    json_like_content = json_like_content.replace('json', '').replace('```', '').strip()
    
    # Attempt to load as JSON object
    try:
        # Correcting the JSON-like content format
        json_like_content = json_like_content.replace('\n', ' ').replace('\\"', '"')
        
        json_data = json.loads(json_like_content)
        print(json_data)
        
        # Write to a JSON file
        with open('output.json', 'w') as f:
            json.dump(json_data, f, indent=4)
        
        print("JSON file 'output.json' has been created successfully.")
    
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    
else:
    print("No JSON-like structure found in the text.")


import re
import json

# Noisy text containing embedded JSON-like structure
text = '''
json\n{\n

"paragraph_1": the Fund the capital subscription set (a)\n

"paragraph_2": any right to pa Related Firm) from the conduct ofthat of (and then only to the extent contemplated by this Agreement).

"paragraph_3": The Limited Partners acknowledge and 10.7.2. Fund),

any Portfolio Direct 35 55075046_36 65777438_1"\n}\n```'

# Regular expression to extract the JSON-like content
pattern = r'\{(?:[^{}]|(?R))*\}'

# Extracting the JSON-like content
match = re.search(pattern, text, re.DOTALL)

if match:
    json_like_content = match.group()
    # Clean up the JSON-like content
    json_like_content = re.sub(r'\\n', '\n', json_like_content)
    json_like_content = json_like_content.replace('json', '').replace('```', '').strip()
    
    # Load as JSON object
    try:
        json_data = json.loads(json_like_content)
        print(json_data)
        
        # Write to a JSON file
        with open('output.json', 'w') as f:
            json.dump(json_data, f, indent=4)
        
        print("JSON file 'output.json' has been created successfully.")
    
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    
else:
    print("No JSON-like structure found in the text.")












_prompt_template = """
Objective: Extract distinct paragraphs related to the provided question from the document.

Scope: Only consider the main body of the document. Exclude the Table of Contents, headers, and footers.

Steps:
1. Read through the entire document, starting from the first section in the given text after the Table of Contents and ending before any appendices or endnotes.
2. Identify and count the distinct paragraphs where the content is relevant to the question: "{question}".
3. Filter and extract these paragraphs, ensuring they match the context of the question.
4. Provide the filtered content in the following JSON format:
{
    "FilteredContent": [
        {
            "Paragraph": "Relevant paragraph extracted from the document"
        }
    ]
}

The question is: {question}

The text to analyze is:
{paragraphs}
"""










_prompt_template = """
Objective: Extract and filter relevant content related to the management fee calculation based on the provided question using the given text.

Scope: Only consider the main body of the document. Exclude the Table of Contents, headers, and footers.

Steps:
1. Read through the entire document, starting from the first section in the given text after the Table of Contents and ending before any appendices or endnotes.
2. Identify the relevant sections that address the question: "{question}".
3. Filter and extract the content from these sections based on the question, ensuring clarity and relevance.
4. For each relevant section, provide the paragraph content and its corresponding page number.
5. Return the response in the following JSON format:
{
    "FilteredContent": [
        {
            "PageNumber": "Page number where the content is found",
            "Paragraph": "Relevant paragraph extracted from the document"
        }
    ]
}

The question is: {question}

The text to analyze is:
{paragraphs}
"""



import os
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def extract_filtered_content(pdf_text, _prompt_template, question):
    # Initialize the AzureChatOpenAI model
    llm = AzureChatOpenAI(temperature=0, deployment_name=os.environ["DEPLOYMENT_NAME"])
    
    # Create the prompt template
    prompt_template = ChatPromptTemplate(
        messages=[HumanMessagePromptTemplate.from_template(_prompt_template)],
        input_variables=["paragraphs", "question"]
    )
    
    # Initialize the LLMChain with the model and prompt template
    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt_template
    )
    
    # Predict the output using the given paragraphs and question
    output = llm_chain.predict(paragraphs=pdf_text, question=question)
    
    return output

# Example usage
pdf_text = """Your PDF text here. This should be a string containing the text extracted from your PDF document."""
question = "Can you explain the structure, calculation, and reduction of the Management Fee for each Limited Partner and Series?"

result = extract_filtered_content(pdf_text, _prompt_template, question)
print("Final Result:", result)










_prompt_template = """
Objective: Extract and filter relevant content based on the provided question using the given text.

Scope: Only consider the main body of the document. Exclude the Table of Contents, headers, and footers.

Steps:
1. Read through the entire document, starting from the first section in {paragraphs} after the Table of Contents and ending before any appendices or endnotes.
2. Identify the relevant sections that address the question: "{question}".
3. Filter and extract the content from these sections based on the question, ensuring clarity and relevance.
4. Return the response in the following JSON format:
{
    "FilteredContent": [
        {
            "Section": "<Section Name or Number>",
            "Content": "<Relevant content extracted from the document>"
        }
    ]
}

The question is: {question}
"""
import os
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def extract_filtered_content(pdf_text, _prompt_template, question):
    # Initialize the AzureChatOpenAI model
    llm = AzureChatOpenAI(temperature=0, deployment_name=os.environ["DEPLOYMENT_NAME"])
    
    # Create the prompt template
    prompt_template = ChatPromptTemplate(
        messages=[HumanMessagePromptTemplate.from_template(_prompt_template)],
        input_variables=["paragraphs", "question"]
    )
    
    # Initialize the LLMChain with the model and prompt template
    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt_template
    )
    
    # Predict the output using the given paragraphs and question
    output = llm_chain.predict(paragraphs=pdf_text, question=question)
    
    return output

# Example usage
pdf_text = """Your PDF text here. This should be a string containing the text extracted from your PDF document."""
question = "Can you explain the structure, calculation, and reduction of the Management Fee for each Limited Partner and Series?"

result = extract_filtered_content(pdf_text, _prompt_template, question)
print("Final Result:", result)















import os
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def extract_filtered_content(pdf_text, _prompt_template, question):
    # Initialize the AzureChatOpenAI model
    llm = AzureChatOpenAI(temperature=0, deployment_name=os.environ["DEPLOYMENT_NAME"])
    
    # Create the prompt template
    prompt_template = ChatPromptTemplate(
        messages=[HumanMessagePromptTemplate.from_template(_prompt_template)],
        input_variables=["paragraphs", "question"]
    )
    
    # Initialize the LLMChain with the model and prompt template
    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt_template
    )
    
    # Predict the output using the given paragraphs and question
    output = llm_chain.predict(paragraphs=pdf_text, question=question)
    
    return output

# Example usage
pdf_text = """Your PDF text here. This should be a string containing the text extracted from your PDF document."""
_prompt_template = """
Objective: Extract and filter relevant content based on the provided question using the given text.

Scope: Only consider the main body of the document. Exclude the Table of Contents, headers, and footers.

Steps:
1. Read through the entire document, starting from the first section in {paragraphs} after the Table of Contents and ending before any appendices or endnotes.
2. Identify the relevant sections that address the question: "{question}".
3. Filter and extract the content from these sections based on the question, ensuring clarity and relevance.
4. Return the response in the following JSON format:
{
    "FilteredContent": [
        {
            "Section": "Section Name or Number",
            "Content": "Relevant content extracted from the document"
        },
        ...
    ]
}

The question is: {question}
"""
question = "Can you explain the structure, calculation, and reduction of the Management Fee for each Limited Partner and Series?"

result = extract_filtered_content(pdf_text, _prompt_template, question)
print("Final Result:", result)









import json
import re

def save_as_json(self, split_text, file_path, heading):
    # Initialize an empty list to hold the chunks
    chunks = []
    
    for i, text_chunk in enumerate(split_text):
        # Clean the text chunk from unwanted characters
        text_chunk = re.sub(r'[\x80-\x1F]+', '', str(text_chunk))
        
        # Create a dictionary with the chunk details
        chunk_data = {
            'heading': f'{heading} - Chunk {i + 1}',
            'text': text_chunk
        }
        
        # Append the chunk data to the chunks list
        chunks.append(chunk_data)
    
    # Create a dictionary to hold the overall structure
    data = {
        'document_heading': heading,
        'chunks': chunks
    }
    
    # Save the data to a JSON file
    with open(file_path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, ensure_ascii=False, indent=4)
    
    print("Output saved as JSON successfully")

# Example usage
# Assuming `split_text` is a list of text chunks and `heading` is a string
# save_as_json(split_text, 'output.json', 'Document Heading')


absl-py==2.1.0
anyio==4.3.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
astunparse==1.6.3
async-lru==2.0.4
attrs==23.2.0
Babel==2.14.0
beautifulsoup4==4.12.3
bleach==6.1.0
certifi==2024.2.2
cffi==1.16.0
charset-normalizer==3.3.2
colorama==0.4.6
comm==0.2.2
debugpy==1.8.1
decorator==5.1.1
defusedxml==0.7.1
executing==2.0.1
fastjsonschema==2.19.1
filelock==3.13.4
flatbuffers==24.3.25
fqdn==1.5.1
fsspec==2024.3.1
gast==0.5.4
google-pasta==0.2.0
grpcio==1.62.2
h11==0.14.0
h5py==3.11.0
httpcore==1.0.5
httpx==0.27.0
huggingface-hub==0.22.2
idna==3.7
ipykernel==6.29.4
ipython==8.23.0
ipython-genutils==0.2.0
isoduration==20.11.0
jedi==0.19.1
Jinja2==3.1.3
json5==0.9.25
jsonpointer==2.4
jsonschema==4.21.1
jsonschema-specifications==2023.12.1
jupyter-events==0.10.0
jupyter-lsp==2.2.5
jupyter_client==8.6.1
jupyter_core==5.7.2
jupyter_server==2.14.0
jupyter_server_terminals==0.5.3
jupyterlab==4.1.6
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.0
keras==3.3.2
libclang==18.1.1
Markdown==3.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib-inline==0.1.7
mdurl==0.1.2
mistune==3.0.2
ml-dtypes==0.3.2
mpmath==1.3.0
namex==0.0.8
nbclient==0.10.0
nbconvert==7.16.3
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.3
notebook==6.1.4
notebook_shim==0.2.4
numpy==1.26.4
opt-einsum==3.3.0
optree==0.11.0
overrides==7.7.0
packaging==24.0
pandas==2.2.2
pandocfilters==1.5.1
parso==0.8.4
platformdirs==4.2.0
prometheus_client==0.20.0
prompt-toolkit==3.0.43
protobuf==4.25.3
psutil==5.9.8
pure-eval==0.2.2
pycparser==2.22
Pygments==2.17.2
PyPDF2==3.0.1
python-dateutil==2.9.0.post0
python-json-logger==2.0.7
pytz==2024.1
pywin32==306
pywinpty==2.0.13
PyYAML==6.0.1
pyzmq==26.0.2
referencing==0.34.0
regex==2024.4.16
requests==2.31.0
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rich==13.7.1
rpds-py==0.18.0
safetensors==0.4.3
Send2Trash==1.8.3
six==1.16.0
sniffio==1.3.1
soupsieve==2.5
stack-data==0.6.3
sympy==1.12
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-intel==2.16.1
tensorflow-io-gcs-filesystem==0.31.0
termcolor==2.4.0
terminado==0.18.1
tf_keras==2.16.0
tinycss2==1.2.1
tokenizers==0.19.1
torch==2.2.2
tornado==6.4
tqdm==4.66.2
traitlets==5.14.3
transformers==4.40.0
types-python-dateutil==2.9.0.20240316
typing_extensions==4.11.0
tzdata==2024.1
uri-template==1.3.0
urllib3==2.2.1
wcwidth==0.2.13
webcolors==1.13
webencodings==0.5.1
websocket-client==1.7.0
Werkzeug==3.0.2
wrapt==1.16.0
