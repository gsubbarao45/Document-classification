
from langchain.chains import LLMChain

def process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm):
    formatted_ocr_by_page = {}

    # Process each page's OCR text with GPT-4
    for page_num, ocr_text in ocr_text_by_page.items():
        if not ocr_text.strip():  # Skip empty OCR text
            continue

        try:
            # Create the prompt template
            prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

            # Initialize the LLMChain with the LLM and the prompt
            llm_chain = LLMChain(llm=llm, prompt=prompt)

            # Format the OCR text using GPT-4
            formatted_ocr_text = llm_chain.predict(content=ocr_text)

            # Save the formatted text by page
            formatted_ocr_by_page[page_num] = formatted_ocr_text

        except Exception as e:
            print(f"Error formatting OCR text for Page {page_num} with GPT-4: {e}")
            formatted_ocr_by_page[page_num] = f"Error formatting OCR text: {e}"

    return formatted_ocr_by_page









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io

def extract_text_from_pdf(pdf_path):
    final_text = ""  # Combined text for all pages
    ocr_text_by_page = {}  # Store OCR-specific text by page number

    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page_text = ""

                # Step 1: Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # Step 2: Extract text and tables using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data
                        tables = page_plumber.extract_tables()
                        if tables:
                            for table in tables:
                                for row in table:
                                    row_text = " | ".join(
                                        str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                        for cell in row
                                    )
                                    page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # Step 3: Perform OCR extraction for images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_page_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_page_text  # Add OCR text to the current page
                            if page_num + 1 not in ocr_text_by_page:
                                ocr_text_by_page[page_num + 1] = ""
                            ocr_text_by_page[page_num + 1] += ocr_page_text  # Store OCR text by page
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the combined content of the current page
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text, ocr_text_by_page  # Return combined text and OCR-specific text by page



from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

def process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm):
    formatted_ocr_by_page = {}

    # Process each page's OCR text with GPT-4
    for page_num, ocr_text in ocr_text_by_page.items():
        if not ocr_text.strip():
            continue

        # Create the prompt template
        prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

        # Initialize the LLMChain with the LLM and the prompt
        llm_chain = LLMChain(llm=llm, prompt=prompt)

        # Use GPT-4 to format the OCR text
        try:
            formatted_ocr_text = llm_chain.predict(content=ocr_text)
            formatted_ocr_by_page[page_num] = formatted_ocr_text
        except Exception as e:
            print(f"Error formatting OCR text for Page {page_num} with GPT-4: {e}")

    return formatted_ocr_by_page  # Return formatted OCR text by page




def combine_text_with_formatting(full_text, formatted_ocr_by_page):
    combined_text = ""  # Start with an empty string
    pages = full_text.split("\n--- Page ")  # Split full text by pages

    for page in pages:
        if not page.strip():
            continue

        # Extract the page number and content
        header, page_content = page.split("---\n", 1)
        page_num = int(header.strip())

        # Add the formatted OCR content for the current page
        if page_num in formatted_ocr_by_page:
            page_content += f"\n\n--- Formatted OCR Content ---\n{formatted_ocr_by_page[page_num]}"

        # Append the processed page back to the combined text
        combined_text += f"\n--- Page {page_num} ---\n{page_content}"

    return combined_text

# Define the PDF path
pdf_path = "path/to/your/pdf/document.pdf"

# Define the GPT-4 prompt template
prompt_template = """
You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format.

Output the formatted content:
{content}
"""

# Initialize the LLM (GPT-4)
llm = OpenAI(
    temperature=0.7,
    api_version="your_api_version",
    azure_endpoint="your_azure_endpoint"
)

# Step 1: Extract text
full_text, ocr_text_by_page = extract_text_from_pdf(pdf_path)

# Step 2: Format OCR text with GPT-4
formatted_ocr_by_page = process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm)

# Step 3: Combine all content
final_output = combine_text_with_formatting(full_text, formatted_ocr_by_page)









from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import OpenAI

# Initialize the OpenAI model (GPT-4) with appropriate parameters
llm = OpenAI(temperature=0.7, api_version="your_api_version", azure_endpoint="your_azure_endpoint")

# Define the prompt template
prompt_template = """
You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format.

Output the formatted content:
{content}
"""

# Define the function to generate questions or organize content
def generate_questions(content, prompt_template):
    # Check if the content is a string
    if not isinstance(content, str):
        raise ValueError("The content should be a string.")

    # Create the prompt template using LangChain's PromptTemplate
    prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

    # Initialize the LLMChain with the LLM and the prompt
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Ensure that the content is passed correctly
        response = llm_chain.predict(content=content)
        return response  # return the formatted content or response
    except Exception as e:
        print(f"Error formatting with GPT-4: {e}")
        return None

# Example usage
content = "This is the OCR-extracted content from your document."
formatted_content = generate_questions(content, prompt_template)
print(formatted_content)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import openai

# Azure OpenAI credentials
AZURE_OPENAI_API_KEY = "your_azure_openai_api_key"  # Replace with your Azure API key
AZURE_OPENAI_ENDPOINT = "your_azure_endpoint"       # Replace with your Azure endpoint
AZURE_OPENAI_DEPLOYMENT_ID = "gpt4-deployment-id"   # Replace with your GPT-4 deployment ID

# Set the path to Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup


def extract_text_from_pdf(pdf_path):
    """
    Extract text, tables, and images from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
    
    Returns:
        str: Extracted content from the PDF.
    """
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""

                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                row_text = " | ".join(
                                    str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                    for cell in row
                                )
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text


def format_with_gpt4(content):
    """
    Format extracted content using GPT-4.
    
    Args:
        content (str): The raw text content to format.
    
    Returns:
        str: The formatted content.
    """
    openai.api_key = AZURE_OPENAI_API_KEY
    openai.api_type = "azure"
    openai.api_base = AZURE_OPENAI_ENDPOINT
    openai.api_version = "2023-03-15-preview"  # Use the appropriate version for your setup

    # GPT-4 prompt to format content
    prompt = f"""
    You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format with proper headings, sections, and tables if applicable:

    {content}

    Output the formatted content:
    """

    try:
        response = openai.ChatCompletion.create(
            engine=AZURE_OPENAI_DEPLOYMENT_ID,
            messages=[
                {"role": "system", "content": "You are an expert formatter."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3000,
            temperature=0.3
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error formatting with GPT-4: {e}")
        return None















import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                # Convert each cell in the row to string and join with " | "
                                row_text = " | ".join(
                                    str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                    for cell in row
                                )
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc, pdfplumber.open(pdf_path) as pdf_plumber:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    page_plumber = pdf_plumber.pages[page_num]
                    pdfplumber_text = page_plumber.extract_text()
                    page_text += pdfplumber_text or ""

                    # Extract table data if available
                    table = page_plumber.extract_tables()
                    if table:
                        for row in table:
                            row_text = " | ".join(row)
                            page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc, pdfplumber.open(pdf_path) as pdf_plumber:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    page_plumber = pdf_plumber.pages[page_num]
                    pdfplumber_text = page_plumber.extract_text()
                    page_text += pdfplumber_text or ""

                    # Extract table data if available
                    table = page_plumber.extract_tables()
                    if table:
                        for row in table:
                            row_text = " | ".join(row)
                            page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)








import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                row_text = " | ".join(row)
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image  # Ensure this is included
import pytesseract
import io
import sys

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text using PyMuPDF
    print("Starting PyMuPDF extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text or ""
                print(f"PyMuPDF Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"PyMuPDF Error: {e}")

    # 2. Extract text using pdfplumber
    print("Starting pdfplumber extraction...")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text or ""
                print(f"pdfplumber Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"pdfplumber Error: {e}")

    # 3. Perform OCR extraction
    print("Starting OCR extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    try:
                        image = Image.open(io.BytesIO(image_bytes))
                        extracted_text = pytesseract.image_to_string(image)
                        ocr_text += extracted_text or ""
                        print(f"OCR Page {page_num + 1}, Image {img_index + 1}:\n{extracted_text}")
                    except Exception as e:
                        print(f"OCR Error: {e}")
    except Exception as e:
        print(f"OCR Extraction Error: {e}")

    return pymupdf_text + pdfplumber_text + ocr_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Update with your file path
result = extract_text_from_pdf(pdf_path)
print("Final Extracted Text:\n", result)









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text using PyMuPDF
    print("Starting PyMuPDF extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text or ""
                print(f"PyMuPDF Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"PyMuPDF Error: {e}")

    # 2. Extract text using pdfplumber
    print("Starting pdfplumber extraction...")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text or ""
                print(f"pdfplumber Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"pdfplumber Error: {e}")

    # 3. Perform OCR extraction
    print("Starting OCR extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    try:
                        image = Image.open(io.BytesIO(image_bytes))
                        extracted_text = pytesseract.image_to_string(image)
                        ocr_text += extracted_text or ""
                        print(f"OCR Page {page_num + 1}, Image {img_index + 1}:\n{extracted_text}")
                    except Exception as e:
                        print(f"OCR Error: {e}")
    except Exception as e:
        print(f"OCR Extraction Error: {e}")

    return pymupdf_text + pdfplumber_text + ocr_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Update with your file path
result = extract_text_from_pdf(pdf_path)
print("Final Extracted Text:\n", result)













import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text if page_text else ""
                if page_text.strip():  # Print extracted text from pdfplumber
                    print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                    sys.stdout.flush()

                # Extract table data if available
                table = page.extract_tables()
                if table:
                    print(f"Extracted table from page {page_num + 1}:")
                    for row in table:
                        row_text = " | ".join(row)
                        print(row_text)
                        pdfplumber_text += "\n" + row_text  # Append table data to full text
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with pdfplumber: {e}")
        sys.stdout.flush()

    # 3. Perform OCR extraction from images
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    # Combine all extracted text
    full_text = pymupdf_text + pdfplumber_text + ocr_text
    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)






import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. If no text found, extract OCR text from images (using pytesseract)
    if not full_text:
        print("No regular text found, attempting OCR extraction from images...")
        sys.stdout.flush()
        ocr_text = ""
        try:
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        image = Image.open(io.BytesIO(image_bytes))
                        
                        # Perform OCR on the image and collect the extracted text
                        extracted_text = pytesseract.image_to_string(image)
                        if extracted_text.strip():  # Check if OCR text was found
                            ocr_text += extracted_text
                            print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                        else:
                            print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                        sys.stdout.flush()

            # Append OCR text to the main text variable
            if ocr_text.strip():
                full_text += ocr_text
            else:
                print("No OCR text extracted.")
                sys.stdout.flush()

        except Exception as e:
            print(f"Error extracting text using OCR: {e}")
            sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)











https://github.com/UB-Mannheim/tesseract/wiki



import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
import sys

# Function to extract OCR text from images in the PDF
def extract_ocr_from_pdf(pdf_path):
    ocr_text = ""
    print("Attempting OCR extraction from images...")

    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                # Extract images
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if any OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        if ocr_text.strip():
            print("\nFinal OCR extracted text:\n")
            print(ocr_text)  # Print the full OCR text if any was extracted
        else:
            print("No OCR text extracted from any images.")
        
    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
extract_ocr_from_pdf(pdf_path)










import os
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import openpyxl

def extract_text_from_pdf(pdf_path):
    text = ""
    
    # Try to extract text from the PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                text += page.get_text("text")
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
    
    # If PyMuPDF doesn't work, try pdfplumber for better handling of tables and text
    if not text:
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    text += page.extract_text()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
    
    # If text is still empty, use OCR (pytesseract) to extract text from images within the PDF
    if not text:
        try:
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    # Extract images
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        image = Image.open(io.BytesIO(image_bytes))
                        text += pytesseract.image_to_string(image)
        except Exception as e:
            print(f"Error extracting text using OCR: {e}")
    
    return text

def generate_questions_by_all_types(text, total_questions, chunk_size):
    # Dummy function for generating questions
    # Replace with your own logic
    return [{"question": f"Question {i}", "answer": "Sample answer"} for i in range(1, total_questions+1)]

def save_to_excel(questions_and_answers, output_file):
    # Save the questions and answers to an Excel file
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.append(["Question", "Answer"])

    for qa in questions_and_answers:
        ws.append([qa["question"], qa["answer"]])
    
    wb.save(output_file)

def process_pdfs(root_directory, output_folder, total_questions=5, chunk_size=500):
    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")

# Example usage
root_directory = "path_to_pdf_folder"
output_folder = "path_to_output_folder"
process_pdfs(root_directory, output_folder)












import os
import time
import signal
import sys
import PyPDF2
import pandas as pd
import json
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import logging

# Configure logging
logging.basicConfig(
    filename="batch_processing.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

def log_info(message):
    logging.info(message)
    print(message)

def log_error(message):
    logging.error(message)
    print(message)

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-4"):
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")
        return len(encoding.encode(text))
    except Exception as e:
        log_error(f"Error calculating tokens: {e}")
        return 0

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page in reader.pages:
                text += page.extract_text()
    except Exception as e:
        log_error(f"Error extracting text from {pdf_path}: {e}")
    return text

# Function to divide items into batches
def divide_into_batches(items, batch_size):
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

# Prompts for question generation
FACTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

# Function to generate questions for a batch of paragraphs
def generate_questions_for_batch(paragraphs, total_questions_per_paragraph, chunk_size=10):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        for i, paragraph in enumerate(paragraphs):
            log_info(f"Processing {question_type} questions for paragraph {i + 1}/{len(paragraphs)}...")
            try:
                llm = AzureChatOpenAI(
                    temperature=0,
                    azure_openai_api_base="https://<your-endpoint>.openai.azure.com/",
                    deployment_name="gpt4-0",
                    api_version="2023-06-01-preview"
                )

                prompt_template = PromptTemplate(
                    input_variables=["paragraph", "num_questions"],
                    template=prompt
                )

                llm_chain = LLMChain(llm=llm, prompt=prompt_template)
                response = llm_chain.predict(paragraph=paragraph, num_questions=str(total_questions_per_paragraph))
                questions = json.loads(response)

                for qa in questions:
                    question_text = qa["question"]
                    answer_text = qa["answer"]
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": question_text,
                        "Answer": answer_text,
                        "Question Length (Chars)": len(question_text),
                        "Answer Length (Chars)": len(str(answer_text)),
                        "Question Tokens": calculate_tokens(question_text),
                        "Answer Tokens": calculate_tokens(answer_text)
                    })
            except Exception as e:
                log_error(f"Error processing {question_type} questions for paragraph {i + 1}: {e}")
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        log_info(f"Questions and answers saved to {output_file}.")
    else:
        log_info(f"No data to save for {output_file}.")

# Function to continuously monitor and process PDFs
def monitor_and_process(root_directory, batch_size=5, total_questions=10, chunk_size=10, output_folder="output", interval=300):
    log_info("Starting continuous monitoring...")
    processed_files = set()

    while True:
        try:
            pdf_files = []
            for dirpath, _, filenames in os.walk(root_directory):
                for filename in filenames:
                    if filename.endswith('.pdf'):
                        pdf_files.append(os.path.join(dirpath, filename))

            new_files = [f for f in pdf_files if f not in processed_files]

            if new_files:
                log_info(f"Found {len(new_files)} new files. Processing...")
                pdf_batches = list(divide_into_batches(new_files, batch_size))

                for batch_index, batch in enumerate(pdf_batches):
                    log_info(f"Processing batch {batch_index + 1}/{len(pdf_batches)}...")
                    paragraphs = [extract_text_from_pdf(pdf_path) for pdf_path in batch]
                    batch_questions = generate_questions_for_batch(paragraphs, total_questions, chunk_size)
                    batch_output_file = os.path.join(output_folder, f"batch_{batch_index + 1}_questions.xlsx")
                    save_to_excel(batch_questions, batch_output_file)

                processed_files.update(new_files)
            else:
                log_info("No new files found.")

        except Exception as e:
            log_error(f"Error during monitoring: {e}")

        time.sleep(interval)

# Graceful shutdown
def graceful_shutdown(signal_received, frame):
    log_info("Shutting down gracefully...")
    sys.exit(0)

signal.signal(signal.SIGINT, graceful_shutdown)
signal.signal(signal.SIGTERM, graceful_shutdown)

# Main execution
if __name__ == "__main__":
    root_directory = "path/to/pdf/folder"
    output_folder = "path/to/output/folder"
    batch_size = 5
    total_questions = 10
    chunk_size = 10
    interval = 300

    monitor_and_process(root_directory, batch_size, total_questions, chunk_size, output_folder, interval)
















import json

def json_to_text(json_obj):
    # Convert JSON object to a human-readable text format
    text = ""
    for key, value in json_obj.items():
        if isinstance(value, dict):  # Handle nested dictionaries
            text += f"{key}:\n"
            text += json_to_text(value)  # Recursive call for nested dictionaries
        elif isinstance(value, list):  # Handle lists
            text += f"{key}: {', '.join(map(str, value))}\n"
        else:
            text += f"{key}: {value}\n"
    return text

# Read JSON data from a file
file_path = "data.json"  # Replace with the path to your JSON file

try:
    with open(file_path, "r") as json_file:
        json_data = json.load(json_file)  # Load JSON data from the file

    # Convert JSON to plain text
    text_output = json_to_text(json_data)

    # Print or save the text output
    print(text_output)

    # Optionally save the text to another file
    with open("output.txt", "w") as text_file:
        text_file.write(text_output)

except FileNotFoundError:
    print(f"Error: File '{file_path}' not found.")
except json.JSONDecodeError:
    print(f"Error: File '{file_path}' is not a valid JSON file.")







import json

def json_to_text(json_obj):
    # Convert JSON object to a human-readable text format
    text = ""
    for key, value in json_obj.items():
        if isinstance(value, dict):  # Handle nested dictionaries
            text += f"{key}:\n"
            text += json_to_text(value)  # Recursive call for nested dictionaries
        elif isinstance(value, list):  # Handle lists
            text += f"{key}: {', '.join(map(str, value))}\n"
        else:
            text += f"{key}: {value}\n"
    return text

# Example JSON object
json_data = {
    "name": "John Doe",
    "age": 30,
    "address": {
        "street": "123 Main St",
        "city": "New York",
        "zipcode": "10001"
    },
    "hobbies": ["reading", "traveling", "coding"]
}

# Convert JSON to plain text
text_output = json_to_text(json_data)

# Print the result
print(text_output)














import os
import PyPDF2
import pandas as pd
import json
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        return len(encoding.encode(text))
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return 0

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.

    Args:
        response (str): Response string.

    Returns:
        list: Parsed JSON list.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.

    Args:
        questions (list): List of question-answer dictionaries.

    Returns:
        list: Unique question-answer pairs.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions by all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.

    Args:
        paragraph (str): Input text.
        total_questions (int): Total number of questions to generate.
        chunk_size (int): Number of questions to generate per chunk.

    Returns:
        list: List of question-answer dictionaries with metadata.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions and answers to an Excel file.

    Args:
        questions_and_answers (list): List of question-answer dictionaries.
        output_file (str): Path to the output Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.

    Args:
        root_directory (str): Root directory containing PDF files.
        total_questions (int): Number of questions to generate per file.
        chunk_size (int): Number of questions to generate per chunk.
        output_folder (str): Directory to save the output Excel files.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")













import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require drawing conclusions or making interpretations beyond the direct information in the text.
    - Answers justify the inference with clear logical reasoning and include the reasoning process explicitly.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic} based on the input text?",
            "answer": "The inference that can be drawn is {logical explanation with reasoning}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or abstract ideas mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle, including why it is important.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation and reasoning about its importance}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")













import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")


















import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        all_questions.extend(questions)

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")
















import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = qa["answer"]

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        all_questions.extend(questions)

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")














# Prompts for Factual, Procedural, Inferential, Conceptual, and Reasoning-based questions

FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Answers are precise and concise.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""



















import os
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Tokenizer setup
tokenizer = tiktoken.get_encoding("cl100k_base")  # Use the appropriate encoding for your model

# Prompts remain the same as before

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to calculate token counts
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text.
    """
    return len(tokenizer.encode(text))

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, including token and character counts.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]

            # Calculate token and character counts
            question_tokens = calculate_tokens(question_text)
            answer_tokens = calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            answer_length = len(answer_text) if isinstance(answer_text, str) else sum(len(str(ans)) for ans in answer_text)

            # Append question type, question, answer, and their counts
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": answer_length,
                "Question Tokens": question_tokens,
                "Answer Tokens": answer_tokens
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")



















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Updated Factual Prompt
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions are focused on names, titles, or specific entities mentioned in the input text.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Procedural Prompt
PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Reasoning-Based Prompt
REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Updated Factual Prompt
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions are focused on names, titles, or specific entities mentioned in the input text.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Procedural Prompt
PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Describe the first action or decision clearly.",
                "Step 2: Provide the next step with clarity and details.",
                "Step 3: Add any additional steps needed for completion.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Reasoning-Based Prompt
REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to sav

















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Prompts remain the same as earlier

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, num_questions=3, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")


















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Prompts remain the same as provided in earlier examples

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory
def process_pdfs_in_directory(directory_path, num_questions=3, output_folder="output"):
    """
    Processes all PDF files in the given directory and generates categorized questions.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]
    for pdf_file in pdf_files:
        pdf_path = os.path.join(directory_path, pdf_file)
        try:
            print(f"Processing: {pdf_path}")
            # Extract text from the PDF
            pdf_text = extract_text_from_pdf(pdf_path)

            # Generate questions
            questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

            # Save results to Excel
            output_file = os.path.join(output_folder, pdf_file.replace(".pdf", "_questions.xlsx"))
            save_to_excel(questions_and_answers, output_file)
        except Exception as e:
            print(f"Error processing {pdf_file}: {e}")



















from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Define prompts for each question type
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, definitions, or data.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your factual question here",
            "answer": "Your factual answer here"
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step explanations.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your procedural question here",
            "answer": [
                "Step 1: Description of the first step.",
                "Step 2: Description of the second step.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

EXPLANATORY_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} explanatory question-answer pairs based on the input text.

    Ensure:
    - Questions aim to clarify concepts or ideas in the text.
    - Answers are comprehensive, including step-by-step reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your explanatory question here",
            "answer": [
                "Step 1: Introduction to the concept or idea.",
                "Step 2: Explanation of key elements.",
                "Step 3: How these elements interact or apply in practice.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} inferential question-answer pairs based on the input text.

    Ensure:
    - Questions encourage interpretation or conclusions based on the data.
    - Answers justify inferences with logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your inferential question here",
            "answer": "Your inference-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or ideas.
    - Answers provide detailed explanations of the underlying concepts.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your conceptual question here",
            "answer": "Your conceptual answer here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - Questions require logical argumentation or critical thinking.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your reasoning-based question here",
            "answer": "Your reasoning-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Process multiple PDFs in a loop
def process_multiple_pdfs(pdf_paths, num_questions=3, output_folder="output"):
    """
    Process multiple PDFs and generate categorized questions for each.
    """
    for pdf_path in pdf_paths:
        try:
            print(f"Processing PDF: {pdf_path}")
            # Extract text
            pdf_text = extract_text_from_pdf(pdf_path)

            # Generate questions
            questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

            # Save to Excel
            output_file = f"{output_folder}/{pdf_path.split('/')[-1].replace('.pdf', '_questions.xlsx')}"
            save_to_excel(questions_and_answers, output_file)

        except Exception as e:
            print(f"Error processing {pdf_path}: {e}")



















from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Define separate prompts for each question type
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, definitions, or data.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your factual question here",
            "answer": "Your factual answer here"
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step explanations.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your procedural question here",
            "answer": [
                "Step 1: Description of the first step.",
                "Step 2: Description of the second step.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

EXPLANATORY_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} explanatory question-answer pairs based on the input text.

    Ensure:
    - Questions aim to clarify concepts or ideas in the text.
    - Answers are comprehensive, including step-by-step reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your explanatory question here",
            "answer": [
                "Step 1: Introduction to the concept or idea.",
                "Step 2: Explanation of key elements.",
                "Step 3: How these elements interact or apply in practice.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} inferential question-answer pairs based on the input text.

    Ensure:
    - Questions encourage interpretation or conclusions based on the data.
    - Answers justify inferences with logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your inferential question here",
            "answer": "Your inference-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or ideas.
    - Answers provide detailed explanations of the underlying concepts.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your conceptual question here",
            "answer": "Your conceptual answer here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - Questions require logical argumentation or critical thinking.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your reasoning-based question here",
            "answer": "Your reasoning-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    """
    Generates questions for a specific question type.
    """
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Main function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    """
    Generate categorized questions for all types.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main Execution
if __name__ == "__main__":
    pdf_path = "your_pdf_file.pdf"  # Replace with the path to your PDF file

    try:
        # Extract text from the PDF
        pdf_text = extract_text_from_pdf(pdf_path)

        # Generate categorized questions
        questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions=3)

        # Save the output to an Excel file
        if questions_and_answers:
            save_to_excel(questions_and_answers, "questions_by_type.xlsx")
        else:
            print("No questions generated.")
    except Exception as e:
        print(f"Error: {e}")










from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import pandas as pd
import json

def generate_questions_by_type(paragraph, num_questions=3):
    """
    Generates categorized questions and answers (Factual, Procedural, etc.)
    """
    question_types = [
        "Factual",
        "Procedural",
        "Explanatory",
        "Inferential",
        "Conceptual",
        "Reasoning-based"
    ]

    # Define prompts for each question type
    prompt_templates = {
        "Factual": """
            You are a financial expert with a deep understanding of corporate finance, investment strategies, and accounting.
            Generate {num_questions} factual question-answer pairs based on the input text.
            
            Factual questions should:
            - Focus on specific details, definitions, or data.
            - Provide precise answers without unnecessary elaboration.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your factual question here",
                    "answer": "Your factual answer here"
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Procedural": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} procedural question-answer pairs based on the input text.

            Procedural questions should:
            - Focus on processes, methods, or sequences of steps.
            - Provide step-by-step answers that are logical and easy to follow.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your procedural question here",
                    "answer": [
                        "Step 1: Description of the first step.",
                        "Step 2: Description of the second step.",
                        "Step 3: Description of the third step."
                    ]
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Explanatory": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} explanatory question-answer pairs based on the input text.

            Explanatory questions should:
            - Clarify complex concepts or ideas in the text.
            - Provide step-by-step explanations to improve understanding.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your explanatory question here",
                    "answer": [
                        "Step 1: Introduction to the concept or idea.",
                        "Step 2: Explanation of the key elements.",
                        "Step 3: How these elements interact or apply in practice.",
                        "Step 4: Conclusion or summary of the explanation."
                    ]
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Inferential": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} inferential question-answer pairs based on the input text.

            Inferential questions should:
            - Require interpreting information or drawing logical conclusions.
            - Encourage predictions or implications based on the provided data.
            - Include answers that justify the inference with clear reasoning.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your inferential question here",
                    "answer": "Your inference-based answer with justification here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Conceptual": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} conceptual question-answer pairs based on the input text.

            Conceptual questions should:
            - Explore theories, principles, or abstract ideas.
            - Provide answers that are detailed and cover the underlying concepts comprehensively.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your conceptual question here",
                    "answer": "Your conceptual answer here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Reasoning-based": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} reasoning-based question-answer pairs based on the input text.

            Reasoning-based questions should:
            - Require logical argumentation, critical thinking, or evaluation.
            - Include answers that justify conclusions with a logical explanation.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your reasoning-based question here",
                    "answer": "Your reasoning-based answer with justification here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """
    }

    # Initialize the Azure LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    all_questions = []
    for question_type, template in prompt_templates.items():
        try:
            prompt = PromptTemplate(input_variables=["paragraph", "num_questions"], template=template)
            llm_chain = LLMChain(llm=llm, prompt=prompt)

            response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_questions))
            questions_and_answers = json.loads(response)

            for qa in questions_and_answers:
                all_questions.append({
                    "Question Type": question_type,
                    "Question": qa["question"],
                    "Answer": qa["answer"]
                })
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON for {question_type} questions.")
            print("Raw Response:", response)
        except Exception as e:
            print(f"Error generating {question_type} questions: {e}")

    return all_questions

# Save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main execution
if __name__ == "__main__":
    paragraph = """
    The Management Fee is a percentage of the committed capital used to cover operational costs. 
    The term of the fund is 10 years, with an option for two 1-year extensions. 
    Investors are responsible for operational and organizational expenses, including the Investor Servicing Fee.
    """
    try:
        questions_and_answers = generate_questions_by_type(paragraph, num_questions=3)
        if questions_and_answers:
            save_to_excel(questions_and_answers, "questions_by_type.xlsx")
        else:
            print("No questions generated.")
    except Exception as e:
        print(f"Error: {e}")















from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Set Hugging Face cache directory
hf_home_path = r"/Volumes/dev2_catalog_01/default/txt-wtf-pe/hf/all-MiniLM-L6-v2"

# Load the model
model = SentenceTransformer(hf_home_path)
print("Model loaded successfully!")

# Example sentences
sentences = [
    "The weather is sunny and warm.",
    "It's a bright and sunny day.",
    "I love going to the beach when the weather is nice.",
    "Rainy days make me want to stay indoors."
]

# Generate embeddings for sentences
print("\nGenerating embeddings for sentences...")
embeddings = model.encode(sentences)
print("Embeddings generated!")

# Display embeddings
print("\nSentence embeddings:")
for i, embedding in enumerate(embeddings):
    print(f"Sentence {i + 1}: {sentences[i]}")
    print(f"Embedding (first 5 dimensions): {embedding[:5]}\n")

# Compute similarity between two sentences
print("\nCalculating similarity between the first two sentences...")
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Similarity between '{sentences[0]}' and '{sentences[1]}': {similarity:.2f}")

# Perform semantic search
print("\nPerforming semantic search...")
query = "What is the weather like?"
query_embedding = model.encode(query)
similarities = cosine_similarity([query_embedding], embeddings)[0]

# Find the most similar sentence
most_similar_idx = np.argmax(similarities)
print(f"Query: {query}")
print(f"Most similar sentence: {sentences[most_similar_idx]} (Similarity: {similarities[most_similar_idx]:.2f})")








combined_prompt_template = PromptTemplate(
    input_variables=["paragraph", "num_questions"],
    template="""
        You are an expert in finance and accounting. Your task is to generate {num_questions} question-answer pairs covering all the following types:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Requirements:
        - Ensure an even distribution of questions among all types.
        - Questions should be clear, concise, and relevant to the text.
        - Answers should be accurate, comprehensive, and easy to understand.

        Input Text: {paragraph}

        Generate the response in the following JSON format:
        [
            {{
                "question": "Your question here",
                "answer": "Your answer here",
                "type": "Type of the question here (e.g., Factual, Procedural, etc.)"
            }},
            ...
        ]
        Ensure the response is a valid JSON and contains no extra text outside the JSON format.
    """
)







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import pandas as pd
import json

def generate_questions_by_type(paragraph, num_questions=3):
    """
    Generates categorized questions and answers (Procedural, Explanatory, etc.)
    """
    question_types = [
        "Factual",
        "Procedural",
        "Explanatory",
        "Inferential",
        "Conceptual",
        "Reasoning-based"
    ]

    # Template for generating categorized questions
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "question_type", "num_questions"],
        template="""
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting. 
            Your task is to generate {num_questions} {question_type} question-answer pairs based on the input text.

            Ensure:
            - The questions are clear, concise, and aligned with the {question_type} type.
            - Answers are comprehensive yet easy to understand.
            - The response is a valid JSON array in the format:
              [
                  {{
                      "question": "Your question here",
                      "answer": "Your answer here"
                  }},
                  ...
              ]

            Input Text: {paragraph}
        """
    )

    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Generate questions for each type
    all_questions = []
    for question_type in question_types:
        try:
            response = llm_chain.predict(
                paragraph=paragraph,
                question_type=question_type,
                num_questions=str(num_questions)
            )
            questions_and_answers = json.loads(response)

            for qa in questions_and_answers:
                all_questions.append({
                    "Question Type": question_type,
                    "Question": qa["question"],
                    "Answer": qa["answer"]
                })
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON for {question_type} questions.")
            continue
        except Exception as e:
            print(f"Error: {e}")
            continue

    return all_questions

def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Example Usage
paragraph = """
The Management Fee is a percentage of the committed capital used to cover operational costs. 
The term of the fund is 10 years, with an option for two 1-year extensions. 
Investors are responsible for operational and organizational expenses, including the Investor Servicing Fee.
"""

try:
    questions_and_answers = generate_questions_by_type(paragraph, num_questions=3)
    if questions_and_answers:
        save_to_excel(questions_and_answers, "questions_by_type.xlsx")
    else:
        print("No questions generated.")
except Exception as e:
    print(f"Error: {e}")













from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template="""
            You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
            Your task is to generate precise and accurate question-answer pairs based on the input text. 
            Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
            
            Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
            - Factual
            - Procedural
            - Explanatory
            - Inferential
            - Conceptual
            - Reasoning-based

            Input Text: {paragraph}

            Generate {num_questions} questions along with their answers in the following JSON format:
            [
                {
                    "question": "Your question here",
                    "answer": "Your answer here"
                },
                ...
            ]
            Ensure the JSON format is valid and parsable.
        """
    )
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    # Generate the response
    try:
        response = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error: {e}")
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main Function to Process Entire PDF
pdf_path = "your_pdf_file.pdf"  # Replace with the actual PDF file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    if questions_and_answers:
        print("Questions and Answers Generated Successfully.")

        # Step 3: Save to Excel
        save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
    else:
        print("No questions generated.")
except Exception as e:
    print(f"Error: {e}")









from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template="""
            You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
            Your task is to generate precise and accurate question-answer pairs based on the input text. 
            Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
            
            Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
            - Factual
            - Procedural
            - Explanatory
            - Inferential
            - Conceptual
            - Reasoning-based

            Input Text: {paragraph}

            Generate {num_questions} questions along with their answers in the following JSON format:
            [
                {
                    "question": "Your question here",
                    "answer": "Your answer here"
                },
                ...
            ]
            Ensure the JSON format is valid and parsable.
        """
    )
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    # Generate the response
    try:
        response = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error: {e}")
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON. Cleaning the response...")
        # Attempt to clean and parse
        cleaned_response = response.replace("\n", "").strip()
        try:
            questions_and_answers = json.loads(cleaned_response)
            return questions_and_answers
        except Exception as e:
            print("Failed to clean and parse JSON response.")
            print("Raw Response:", response)
            return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")













from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json
import pandas as pd

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Model Response:", response)
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")










from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Model Response:", response)
        return []

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Display the results
    print(json.dumps(questions_and_answers, indent=2))
except Exception as e:
    print(f"Error: {e}")







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        str: Combined text from all pages of the PDF.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    
    Args:
        pdf_text (str): Input text to generate questions from.
        num_questions (int): Number of question-answer pairs to generate.
    
    Returns:
        list: List of question-answer pairs in JSON format.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers. Present the output in JSON format as:
        [
            {
                "question": "Question text here",
                "answer": "Answer text here"
            },
            ...
        ]
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your actual API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    return json.loads(response)  # Parse the JSON output

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")



# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")







from langchain.prompts import PromptTempla



te
from langchain.chains import LLMChain
from langchain.llms import AzureChatOpenAI
import PyPDF2

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        str: Combined text from all pages of the PDF.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    
    Args:
        pdf_text (str): Input text to generate questions from.
        num_questions (int): Number of question-answer pairs to generate.
    
    Returns:
        list: List of question-answer pairs in JSON format.
    """
    # Prompt template for generating questions and answers
    prompt_template = PromptTemplate.from_template("""
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers. Present the output in JSON format as:
        [
            {
                "question": "Question text here",
                "answer": "Answer text here"
            },
            ...
        ]
    """)

    # LLM setup
    llm = AzureChatOpenAI(
        temperature=0, 
        api_version="2023-06-01-preview",  # Replace with your actual API version
        azure_endpoint="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40", 
        seed=42
    )

    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Predict output
    output = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)

    return output


# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    import json
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")













import os
from pathlib import Path
from sentence_transformers import SentenceTransformer

# Set Hugging Face cache directory
hf_home_path = r"C:\Users\subba\OneDrive\Desktop\test"
os.environ["HF_HOME"] = hf_home_path

# Ensure the directory exists
if not os.path.exists(hf_home_path):
    os.makedirs(hf_home_path, exist_ok=True)
print(f"Hugging Face cache directory set to: {hf_home_path}")

# Load model
try:
    print("Downloading model...")
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", cache_folder=hf_home_path)
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

# Verify downloaded files
print(f"Checking downloaded files in: {hf_home_path}")
try:
    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")   "give me example how we load the model use it
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Set Hugging Face cache directory
hf_home_path = r"C:\Users\subba\OneDrive\Desktop\test"

# Load the model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", cache_folder=hf_home_path)
print("Model loaded successfully!")

# Example sentences
sentences = [
    "The weather is sunny and warm.",
    "It’s a bright and sunny day.",
    "I love going to the beach when the weather is nice.",
    "Rainy days make me want to stay indoors."
]

# Generate embeddings for sentences
print("\nGenerating embeddings for sentences...")
embeddings = model.encode(sentences)
print("Embeddings generated!")

# Display embeddings
print("\nSentence embeddings:")
for i, embedding in enumerate(embeddings):
    print(f"Sentence {i+1}: {sentences[i]}")
    print(f"Embedding (first 5 dimensions): {embedding[:5]}\n")

# Compute similarity between two sentences
print("\nCalculating similarity between the first two sentences...")
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Similarity between '{sentences[0]}' and '{sentences[1]}': {similarity:.2f}")

# Perform semantic search
print("\nPerforming semantic search...")
query = "What is the weather like?"
query_embedding = model.encode(query)
similarities = cosine_similarity([query_embedding], embeddings)[0]

# Find the most similar sentence
most_similar_idx = np.argmax(similarities)
print(f"Query: {query}")
print(f"Most similar sentence: {sentences[most_similar_idx]} (Similarity: {similarities[most_similar_idx]:.2f})")








# Step 1: Stop Proxy
import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

# Remove proxy settings if they exist
if "https_proxy" in os.environ:
    del os.environ["https_proxy"]
if "http_proxy" in os.environ:
    del os.environ["http_proxy"]

print("Proxy settings have been removed.")

# Step 2: Set Hugging Face Cache Directory
# Set the Hugging Face home to a writable directory in Databricks
hf_home_path = "/dbfs/tmp/huggingface_cache/"
os.environ["HF_HOME"] = hf_home_path

# Ensure the directory exists
if not os.path.exists(hf_home_path):
    os.makedirs(hf_home_path, exist_ok=True)
print(f"Hugging Face cache directory set to: {hf_home_path}")

# Step 3: Download Model and Tokenizer
model_name = "gpt2"  # Small model for testing

try:
    print("Downloading model...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # Automatically map model to available devices
        trust_remote_code=True  # Allow downloading custom models if needed
    )
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

try:
    print("Downloading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    print("Tokenizer downloaded successfully.")
except Exception as e:
    print(f"Error downloading tokenizer: {e}")

# Step 4: Verify Downloaded Files
print(f"Checking downloaded files in: {hf_home_path}")
try:
    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")







import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# Environment setup
os.environ['CURL_CA_BUNDLE'] = ''
os.environ["HF_HOME"] = "/Volumes/dev2_catalog_01/default/txt-wtf-pe/hf/test-model/"  # Change to a test directory

# Small model for testing
model_name = "gpt2"  # You can also try "bert-base-uncased" or any lightweight model

# Load the model
print("Downloading model...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map='auto',  # Automatically map model to available devices
        trust_remote_code=True  # Allow downloading custom models if needed
    )
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

# Load the tokenizer
print("Downloading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    print("Tokenizer downloaded successfully.")
except Exception as e:
    print(f"Error downloading tokenizer: {e}")

# Check if the model and tokenizer are saved
hf_home_path = os.environ["HF_HOME"]
print(f"Checking model files in: {hf_home_path}")
try:
    from pathlib import Path

    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")








import openai
import pandas as pd

# Set your OpenAI API key
openai.api_key = "your_openai_api_key"

def generate_questions(paragraph, num_questions=5):
    """
    Generate questions based on a paragraph using GPT-4.
    """
    prompt = f"""
    Based on the following paragraph, generate {num_questions} meaningful and relevant questions:
    
    Paragraph:
    {paragraph}
    
    Questions:
    """
    try:
        # Call GPT-4 model
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant for generating questions."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        # Extract and return the questions
        questions = response['choices'][0]['message']['content']
        return questions.strip()
    except Exception as e:
        return f"Error: {e}"

def process_excel(file_path, output_path, num_questions=5):
    """
    Process the Excel file, generate questions for each paragraph, and save the results.
    """
    # Read the Excel file
    df = pd.read_excel(file_path)

    # Check for the required columns
    if 'Paragraph' not in df.columns:
        raise ValueError("The Excel file must contain a 'Paragraph' column.")

    # Generate questions for each paragraph
    questions_list = []
    for index, row in df.iterrows():
        paragraph = row['Paragraph']
        if pd.isna(paragraph):  # Skip empty paragraphs
            questions_list.append("")
            continue
        print(f"Generating questions for S. No {row['S. No']}...")
        questions = generate_questions(paragraph, num_questions=num_questions)
        questions_list.append(questions)

    # Add questions to a new column
    df['Questions'] = questions_list

    # Save the updated DataFrame to a new Excel file
    df.to_excel(output_path, index=False)
    print(f"Questions have been added and saved to {output_path}")

if __name__ == "__main__":
    # Input Excel file path
    input_file = "input_file.xlsx"  # Replace with your input file path
    output_file = "output_with_questions.xlsx"  # Replace with your desired output file path

    # Number of questions to generate per paragraph
    num_questions = 5

    # Process the Excel file
    process_excel(input_file, output_file, num_questions=num_questions)




import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    """Check if a heading is invalid (empty, only special characters, or only numbers)."""
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    """Extract headings or subheadings from a given line."""
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)  # Match section numbers and titles
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)  # Title text (e.g., "Formation and Continuation")

        if not is_invalid_heading(title):  # Check if the heading is valid
            if len(section.split(".")) == 1:  # Top-level heading
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": section.strip() + " " + title.strip(),  # Combine section and title
                }
            elif len(section.split(".")) > 1:  # Subheading
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": section.strip() + " " + title.strip(),  # Combine section and title
                }

    return heading

def extract_headings_from_pdf(pdf_path):
    """Extract headings from a PDF file."""
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        temp = ""

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    # Identify headings based on font size or style
                    if span["size"] >= 10:  # Adjust threshold if needed
                        line_text = span["text"].strip()

                        if line_text != "":
                            if re.match(r'^\d+\.*\s*$', line_text):
                                # Likely a standalone section number
                                temp = line_text
                            else:
                                # Combine with previous line if it's a continuation
                                line_text = temp + " " + line_text if temp else line_text
                                temp = ""  # Reset temp after combining

                                # Extract headings
                                heading = extract_headings(line_text.strip())
                                if heading:
                                    headings.append({
                                        "page_number": page_number + 1,
                                        "type": heading["type"],
                                        "text": heading["text"].strip(),
                                    })

    return headings

def save_headings_to_csv(pdf_path):
    """Extract headings and save them to a CSV file."""
    headings = extract_headings_from_pdf(pdf_path)
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Text"])  # CSV header
        for heading in headings:
            writer.writerow([heading["page_number"], heading["type"], heading["text"]])

    print(f"Results saved to the file: {csv_file}")

# Example usage:
# save_headings_to_csv("example.pdf")













import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    # Check if a heading is invalid (empty, only special characters, or only numbers).
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    # Match patterns like "1.", "1.1", etc., followed by a heading
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)    # Title text
        if not is_invalid_heading(title):
            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": title.strip()
                }
            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": title.strip()
                }
    return heading

def extract_headings_from_pdf(pdf_path):
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            temp_line = ""
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    if span["size"] >= 10:  # Adjust this threshold for font size
                        line_text = span["text"].strip()
                        if re.match(r'^\d+\.*$', line_text):  # Handle cases where section number is on a separate line
                            temp_line = line_text
                        else:
                            if temp_line:
                                line_text = temp_line + " " + line_text
                                temp_line = ""
                            heading = extract_headings(line_text)
                            if heading:
                                headings.append({
                                    "page_number": page_number + 1,
                                    "type": heading["type"],
                                    "text": heading["text"]
                                })
    return headings

# Save results to a CSV file
def save_to_csv(pdf_path, headings):
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading"])
        for heading in headings:
            writer.writerow([heading['page_number'], heading['type'], heading['text']])
    print(f"Results saved to the file: {csv_file}")

# Main Function
pdf_path = "example.pdf"  # Replace with your PDF file path
headings = extract_headings_from_pdf(pdf_path)
save_to_csv(pdf_path, headings)








import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv

# Configure Tesseract path (if not in PATH)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Update with your Tesseract path

# Configure Poppler path (for Windows users)
POPPLER_PATH = r"C:\poppler\bin"  # Update with your Poppler installation path

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_content(text):
    """
    Extract headings, subheadings, and their associated content.
    """
    results = []
    lines = text.splitlines()

    current_heading = None
    current_content = []

    for line in lines:
        line = line.strip()

        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                # If there is a current heading, save it with its content
                if current_heading:
                    results.append({
                        "type": current_heading["type"],
                        "section": current_heading["section"],
                        "text": current_heading["text"],
                        "content": " ".join(current_content).strip()
                    })

                # Start a new heading or subheading
                heading_type = "Heading" if len(section.split(".")) == 1 else "Subheading"
                current_heading = {
                    "type": heading_type,
                    "section": section,
                    "text": title.strip() + "."
                }
                current_content = []  # Reset content for the new heading/subheading
        else:
            # Add non-heading lines as content for the current heading/subheading
            if current_heading:
                current_content.append(line)

    # Save the last heading/subheading with its content
    if current_heading:
        results.append({
            "type": current_heading["type"],
            "section": current_heading["section"],
            "text": current_heading["text"],
            "content": " ".join(current_content).strip()
        })

    return results

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings, subheadings, and their content to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text", "Content"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"], heading["content"]])

def main(pdf_path):
    """
    Main function to extract and save headings, subheadings, and their content from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings_and_content.csv").replace(".PDF", "_headings_and_content.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings, subheadings, and content
    print("Extracting headings, subheadings, and content...")
    headings = extract_headings_and_content(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings with content:")
    for heading in headings:
        print(f"{heading['type']} ({heading['section']}): {heading['text']}")
        print(f"Content: {heading['content']}\n")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)








import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    # Check if a heading is invalid (empty, only special characters, or only numbers).
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)    # Title text
        if not is_invalid_heading(title):
            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": line.strip()
                }
            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": line.strip()
                }
    return heading

def extract_headings_from_pdf(pdf_path):
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    if span["size"] >= 10:  # Adjust this threshold as needed
                        line_text = span["text"].strip()
                        if line_text != "":
                            heading = extract_headings(line_text)
                            if heading:
                                headings.append({
                                    "page_number": page_number + 1,
                                    "type": heading["type"],
                                    "text": heading["text"]
                                })
    return headings

# Save results to a CSV file
def save_to_csv(pdf_path, headings):
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading"])
        for heading in headings:
            writer.writerow([heading['page_number'], heading['type'], heading['text']])
    print(f"Results saved to the file: {csv_file}")

# Main Function
pdf_path = "example.pdf"  # Replace with your PDF file path
headings = extract_headings_from_pdf(pdf_path)
save_to_csv(pdf_path, headings)










import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv

# Configure Tesseract path (if not in PATH)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Update with your Tesseract path

# Configure Poppler path (for Windows users)
POPPLER_PATH = r"C:\poppler\bin"  # Update with your Poppler installation path

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_subheadings(text):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation').
    """
    headings = []
    lines = text.splitlines()

    for line in lines:
        line = line.strip()
        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                    headings.append({
                        "type": "Heading",
                        "section": section,
                        "text": title.strip() + "."
                    })
                elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                    headings.append({
                        "type": "Subheading",
                        "section": section,
                        "text": title.strip() + "."
                    })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"]])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings and subheadings
    print("Extracting headings and subheadings...")
    headings = extract_headings_and_subheadings(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings:")
    for heading in headings:
        print(f"{heading['type']}: {heading['section']} {heading['text']}")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)















POPPLER_PATH = r"C:\poppler\bin"  # Replace with the path to your Poppler installation

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)
Step 3: Install Required Python Libraries
Ensure the following libraries are installed:

bash
Copy code
pip install PyMuPDF pdf2image pytesseract pillow
Step 4: Test Poppler Installation
Run the following command to verify that Poppler is installed correctly:

bash
Copy code
pdftoppm -h
If you see the help message, Poppler is correctly installed.

Step 5: Run the Script
After configuring the Poppler path and installing dependencies, run the script:

bash
Copy code
python your_script.py
Expected Output
If everything is set up correctly, the script will extract headings and subheadings from your PDF and save them to a CSV file.

Let me know if you encounter further issues!












ChatGPT can make mistakes. Check important info.








import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv
import os

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_subheadings(text):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation').
    """
    headings = []
    lines = text.splitlines()

    for line in lines:
        line = line.strip()
        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                    headings.append({
                        "type": "Heading",
                        "section": section,
                        "text": title.strip() + "."
                    })
                elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                    headings.append({
                        "type": "Subheading",
                        "section": section,
                        "text": title.strip() + "."
                    })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"]])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings and subheadings
    print("Extracting headings and subheadings...")
    headings = extract_headings_and_subheadings(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings:")
    for heading in headings:
        print(f"{heading['type']}: {heading['section']} {heading['text']}")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)










import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip() + ".",
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip() + ".",
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)










import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both top-level headings (e.g., '1. Organization') and clean subheadings (e.g., '1.1 Mission Statement') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Mission Statement", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*?)(\.|\s)*$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            # Differentiate between top-level headings and subheadings
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both top-level headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Something') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "2. List", "1.1 Something", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            # Differentiate between top-level headings and subheadings
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)







import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Something') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Something", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if len(section.split(".")) == 1:  # Heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            else:  # Subheading (e.g., "1.1", "1.1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)











import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both headings and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "2. Capital", "3.2.3 Distribution"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "3.2.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if is_upper or "Bold" in font_name or font_size >= 12:  # Heading
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif is_underlined or "Italic" in font_name:  # Subheading
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)









import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings (e.g., '1. Organization') and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Distribution')
                    match = re.match(r"^(\d+\.)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number
                        title = match.group(2)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if is_upper or "Bold" in font_name or font_size >= 12:  # Heading
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif is_underlined:  # Subheading
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_underlined_subheadings(pdf_path):
    """
    Extract headings and underlined subheadings from a PDF.
    Only keeps text before the first sentence-ending punctuation.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Extract only headings and underlined subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            # Keep only the section number and title
                            match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                            if match:
                                section = match.group(1)  # Section number
                                title = match.group(3)  # Title text
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                        elif is_underlined:  # Subheading with underline
                            # Keep only the section number and title
                            match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                            if match:
                                section = match.group(1)  # Section number
                                title = match.group(3)  # Title text
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and underlined subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_underlined_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and underlined subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_underlined_subheadings(pdf_path):
    """
    Extract headings and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Extract only headings and underlined subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading with underline
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and underlined subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_underlined_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']}")
        count += 1

    print(f"Number of headings and underlined subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings and subheadings from a PDF.
    Subheadings are identified by underlining, and normal content is ignored.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Ignore normal content and extract only headings and subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading", "Font_Size", "Font_Name"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"],
                heading["font_size"],
                heading["font_name"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']} (Font: {heading['font_name']}, Size: {heading['font_size']})")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)









import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings and subheadings from a PDF.
    Subheadings are identified by underlining.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Identify headings and subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading", "Font_Size", "Font_Name"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"],
                heading["font_size"],
                heading["font_name"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']} (Font: {heading['font_name']}, Size: {heading['font_size']})")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)






import fitz  # PyMuPDF
import re

def extract_headings_and_subheadings(pdf_path):
    headings = []
    section_pattern = re.compile(r'^\d+(\.\d+)*')  # Matches patterns like 1, 1.1, 1.1.1, etc.
    
    # Open the PDF
    doc = fitz.open(pdf_path)
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]
        
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    # Extract font size and text
                    text = " ".join(span["text"] for span in line["spans"]).strip()
                    font_size = line["spans"][0]["size"]  # Assuming all spans in the line have the same size
                    
                    if section_pattern.match(text):  # Match section numbers
                        headings.append((font_size, text))
    
    # Post-process: Categorize by font size
    # Assuming larger font sizes are main headings
    font_sizes = [size for size, _ in headings]
    threshold = sum(font_sizes) / len(font_sizes)  # Use average font size as a threshold
    
    main_headings = [text for size, text in headings if size > threshold]
    subheadings = [text for size, text in headings if size <= threshold]
    
    return main_headings, subheadings

# Usage
pdf_path = "your_document.pdf"
main_headings, subheadings = extract_headings_and_subheadings(pdf_path)

print("Main Headings:")
print("\n".join(main_headings))

print("\nSubheadings:")
print("\n".join(subheadings))






import fitz  # PyMuPDF
import re

def extract_headings_from_pdf(pdf_path):
    headings = []
    section_pattern = re.compile(r'^\d+(\.\d+)*')  # Matches patterns like 1, 1.1, 1.1.1, etc.
    
    # Open the PDF
    doc = fitz.open(pdf_path)
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]
        
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    text = " ".join(span["text"] for span in line["spans"]).strip()
                    if section_pattern.match(text):  # Match section numbers
                        headings.append(text)
    
    return headings

# Usage
pdf_path = "your_document.pdf"
headings = extract_headings_from_pdf(pdf_path)
print("\n".join(headings))









import fitz  # PyMuPDF
import re





def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    filtering out noisy content like standalone numbers and retaining valid headings.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Skip noisy content: Only keep lines with valid text and patterns
                    if len(text) <= 2 or text.isdigit():
                        continue

                    # Clean the text: Remove dots or unwanted characters
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)  # Remove extra spaces

                    # Regex to identify valid section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.+)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text

                        # Append valid headings only
                        if heading_text:
                            extracted_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")











import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    filtering out noisy content like standalone numbers.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Skip noisy content: Ignore lines that are only numbers
                    if text.isdigit():
                        continue

                    # Clean the text to remove unwanted dots or patterns
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)  # Remove extra spaces

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text

                        # Filter out empty or short headings and numeric-only headings
                        if heading_text and not heading_text.isdigit() and len(heading_text) > 1:
                            extracted_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")









import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        page_data = []

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", text)
                    if match:
                        section_number = match.group(1).strip()
                        heading_text = match.group(3).strip()

                        # Filtering noisy content: Only include lines with proper section numbering
                        if heading_text and len(heading_text) > 1:
                            page_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text,
                                "position": bbox
                            })

        # Add data for this page only if valid headings exist
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")





import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    excluding unwanted characters like '............'.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        page_data = []

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Clean the text to remove unwanted dots or similar patterns
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()  # Remove repeated dots
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)   # Remove extra spaces

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()
                        heading_text = match.group(3).strip()

                        # Filter out invalid or empty headings
                        if heading_text and len(heading_text) > 1:
                            page_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

        # Add data for this page only if valid headings exist
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")






import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        page_data = []  # Temporary storage for headings on this page

        for block in blocks:
            # Check if the block contains lines
            if "lines" not in block:
                continue

            for line in block["lines"]:
                # Check if the line contains spans
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        page_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

        # Add data from this page only if it contains headings/subheadings
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No headings or subheadings found in the document.")







import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            # Check if the block contains lines
            if "lines" not in block:
                continue

            for line in block["lines"]:
                # Check if the line contains spans
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        extracted_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
print_extracted_data(extracted_data)





import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        extracted_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
print_extracted_data(extracted_data)


=SUMPRODUCT(--(ISNUMBER(SEARCH(TRIM(MID(SUBSTITUTE(A1, " ", REPT(" ", 100)), ROW(INDIRECT("1:" & LEN(A1)-LEN(SUBSTITUTE(A1, " ", ""))+1))*100-99, 100)), B1)))) / (LEN(A1)-LEN(SUBSTITUTE(A1, " ", ""))+1) * 100




# Initialize counters and sums
correct_matches = 0  # Total correctly matched paragraphs
matched_completeness_sum = 0  # Sum of completeness scores for matched paragraphs
matched_count = 0  # Count of matched paragraphs
total_gt_paragraphs = len(ground_truth_paragraphs)  # Total ground truth paragraphs
completeness_sum = 0  # Sum of completeness scores for all predictions

# Process predictions and ground truth
for gt_para, predicted_para in zip(ground_truth_paragraphs, predicted_paragraphs):
    # Find best match score and index for each ground truth paragraph
    best_match_score, best_match_idx = get_best_match(gt_para, predicted_para)

    if best_match_score > 0:  # Match found
        correct_matches += 1
        matched_completeness_sum += best_match_score  # Only for matched paragraphs
        matched_count += 1

    completeness_sum += best_match_score  # Sum completeness for all paragraphs

    # Append row-wise results
    result_rows.append({
        "groundTruth": gt_para,
        "predicted": predicted_para if predicted_para else "",
        "extracted": best_match_score > 0,  # True if a match exists
        "matching_ai_index": best_match_idx if best_match_score > 0 else -1,
        "completeness": best_match_score,
        "correctness": 1 if best_match_score > 0 else 0
    })

# Process extra ground truth paragraphs not matched with any prediction
extra_gts = [gt for gt in ground_truth_paragraphs if gt not in [row["groundTruth"] for row in result_rows]]

for pr_gt in extra_gts:
    result_rows.append({
        "groundTruth": pr_gt,
        "predicted": None,
        "extracted": False,
        "matching_ai_index": None,
        "completeness": 0,
        "correctness": 0
    })

# Calculate Overall Metrics
overall_correctness = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
overall_completeness = (
    matched_completeness_sum / matched_count if matched_count > 0 else 0
)

# Calculate Recall and Precision
count_based_recall = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
count_based_precision = correct_matches / len(predicted_paragraphs) if len(predicted_paragraphs) > 0 else 0

# Calculate Business Score
business_score = (0.6 * overall_correctness) + (0.4 * overall_completeness)

# Summary of results
summary = {
    "Overall Correctness": overall_correctness,
    "Overall Completeness": overall_completeness,
    "Recall": count_based_recall,
    "Precision": count_based_precision,
    "Business Score": business_score
}

# Return final result rows and summary
return result_rows, summary






Algorithm for the Document Processing Workflow
Here's a step-by-step algorithm that outlines the complete process for the document processing workflow using Azure and AI services:

Initialize Connections and Load Configurations

Establish connections to Azure Blob Storage using managed identity configurations (pm_azure_blob_connection and di_azure_blob_connection).
Load configurations for AI models, regex patterns, and document processing settings.
Input Document Acquisition

Retrieve the input document from Azure Blob Storage using the specified connection details.
Validate the document's format and accessibility.
Artifact Generation and Initialization

Initialize artifacts to capture essential document metadata:
Page Dimensions: Record the size of each page in the document.
Document Layout: Identify structural elements such as text blocks, tables, and images.
Logical Chunks: Segment the document into manageable sections for targeted analysis.
Document Parsing using Azure Document Intelligence

Parse the document using the Azure Cognitive Services model (prebuilt-layout).
Extract and store the document layout, page dimensions, and logical chunks for further processing.
Regex Filtering of Logical Chunks

Apply regex patterns to the logical chunks to filter out sections containing specific financial keywords.
Extract chunks that match the keywords such as “Management Fee,” “Compensation,” and other relevant terms.
AI Filtering of Extracts Using Azure OpenAI

Use Azure OpenAI models to further refine the filtered logical chunks.
Input: Filtered logical chunks.
Output: AI-filtered extracts containing relevant paragraphs that answer predefined financial questions.
Key Terms Identification

Identify key financial and legal terms within the AI-filtered extracts using the AI model.
Extract a list of key terms that are significant for understanding the financial context of the document.
Regex Filtering of Key Term Definitions

Apply regex patterns to further filter the identified key terms to pinpoint exact definitions within the document.
Extract text snippets that provide definitions for the key terms.
AI Filtered Key Term Definitions

Use Azure OpenAI to refine and enhance the definitions of the key terms.
Input: Regex-filtered key term definitions.
Output: Contextualized, detailed definitions of each key term.
AI MFees Interpretations

Generate interpretations of the management fee-related extracts using AI models.
Simplify complex financial language into concise, understandable summaries.
Provide interpretations that highlight critical insights about management fees and their calculations.
AI Key Term Definitions Interpretations

Use AI models to provide clear and comprehensive interpretations of the key term definitions.
Cross-reference key term definitions with relevant document content to ensure accuracy.
Generate summarized interpretations that facilitate easy understanding of complex financial and legal terms.
Output Generation

Compile the results, including page dimensions, document layout, logical chunks, filtered extracts, key terms, definitions, and interpretations.
Format the output data for presentation, reporting, or further analysis by end-users.
Error Handling and Validation

Implement error handling for data retrieval, model processing, and interpretation stages.
Validate outputs at each stage to ensure consistency and accuracy before final output generation.
End of Process

Save the processed data and results back to Azure Blob Storage or an alternative storage solution as required.
Log the process flow and outputs for auditing and troubleshooting purposes.
Key Points
The algorithm leverages a combination of regex filtering and AI-driven extraction techniques to ensure precise identification and interpretation of key financial terms within complex documents.
Each step is validated and cross-referenced to ensure data integrity and accuracy, providing reliable outputs that enhance decision-making.
The integration of Azure OpenAI models adds contextual understanding to the extracted information, making it easier to navigate complex legal and financial language.
This algorithm provides a structured approach to processing and analyzing financial documents, enabling automated insights that are critical for legal, financial, and compliance needs. Let me know if you need further details or specific code implementations for each step!










ChatGPT can make mistakes. Check









1. Input Document Acquisition
Description: This stage involves fetching the input documents from Azure Blob Storage using a managed identity for secure access. The system connects to specific Azure Blob containers using predefined connection settings to locate and retrieve the document files needed for processing.
Purpose: To provide the initial data required for analysis, ensuring that the documents are securely accessed and correctly identified.
2. Artifact Generation and Initialization
Description: Once the documents are retrieved, the system initializes various artifacts that are essential for further processing. This includes setting up data structures like page dimensions, document layout, logical chunks, key term lists, and more. These artifacts help in organizing and structuring the content of the document for subsequent analysis.
Components:
Page Dimensions: Captures the size and layout of each page.
Document Layout: Defines the structure of the document.
Logical Chunks: Divides the document into meaningful sections for processing.
3. Document Parsing
Description: In this stage, Azure Document Intelligence is used to analyze the document's layout and content. It identifies various elements such as text, tables, images, and other structural components. This process is crucial for extracting meaningful chunks of data that can be further analyzed.
Purpose: To break down the document into its fundamental parts, making it easier to target specific information during subsequent analysis.
4. Regex Filtering of Logical Chunks
Description: The logical chunks of the document are filtered using predefined regex patterns to identify specific keywords related to financial and management terms, such as "Management Fee," "Compensation," and others. This stage narrows down the content to the most relevant sections based on the target terms.
Purpose: To refine the document content by focusing on relevant sections that contain specific key terms, allowing for more efficient and targeted analysis.
5. AI Filtering of Extracts
Description: This stage uses Azure OpenAI models to further refine the extracts identified by the regex filtering. The AI models are prompted to answer specific financial questions or highlight relevant content related to management fees and other terms.
Purpose: To enhance the filtering process by leveraging AI's ability to understand context and provide insights beyond simple keyword matching.
6. Key Terms Identification
Description: AI models analyze the filtered content to identify key financial and legal terms. This process involves scanning the text to detect significant terms that are essential for understanding the document's content.
Purpose: To create a comprehensive list of key terms that are critical for interpreting the financial information presented in the document.
7. Regex Filtering of Key Term Definitions
Description: Identified key terms are subjected to another round of filtering using regex patterns to pinpoint the precise definitions within the document. This helps in associating each key term with its most relevant and accurate description.
Purpose: To ensure that each key term is clearly defined within the context of the document, enhancing comprehension for further analysis.
8. AI Filtered Key Term Definitions
Description: Azure OpenAI models are used to refine the definitions of the key terms identified in the previous stage. The models are prompted to provide detailed, contextual definitions based on the document content.
Purpose: To deliver clear, context-specific definitions that help in understanding the financial and legal nuances of the key terms.
9. AI MFees Interpretations
Description: The AI models interpret and summarize the management fee-related content extracted from the document. This involves breaking down complex financial language into simpler, more digestible summaries.
Purpose: To provide actionable insights and interpretations of management fee structures, calculations, and related financial information.
10. AI Key Term Definitions Interpretations
Description: In the final stage, AI models provide interpretations of the key term definitions, summarizing the financial and legal implications in a clear and concise manner. This includes cross-referencing the definitions with the document's content to ensure accuracy.
Purpose: To synthesize the key term definitions into understandable interpretations that can be easily used by non-experts, supporting decision-making and compliance.
These descriptions outline the specific functionality and purpose of each stage in the document processing workflow, highlighting how Azure and AI services are integrated to achieve comprehensive document analysis. Let me know if you need further details on any of these stages!






You said:
Overall description about the project 
ChatGPT said:
ChatGPT
Overall Description of the Project
The project is a comprehensive document processing and analysis system designed to handle complex financial and legal documents using Azure and AI technologies. The primary goal of the system is to extract, interpret, and summarize critical information related to management fees and other key financial terms found within these documents. This automated process aims to provide accurate, relevant insights, reducing manual effort and enhancing the efficiency of document analysis for financial institutions, legal firms, and other stakeholders.

Key Objectives
Automate Document Parsing and Analysis: Utilize Azure Document Intelligence and OpenAI models to automate the extraction and analysis of complex documents, thereby reducing the time and effort required for manual review.
Identify and Interpret Key Financial Terms: Use AI and regex-based filtering to identify key terms related to financial management, such as management fees, compensation, and other expenses, ensuring that relevant information is accurately highlighted.
Provide Clear, Actionable Insights: Interpret the extracted content to deliver concise, understandable summaries of complex financial terms, enabling better decision-making and compliance review.
Technical Components
Azure Blob Storage: Serves as the secure storage location for all input documents, enabling easy access and retrieval through managed identity connections.
Azure Cognitive Services (Document Intelligence): Analyzes the document structure, extracting logical chunks, page layouts, and other relevant information necessary for detailed processing.
Azure OpenAI Models: Plays a central role in refining the extracted content, identifying key terms, and generating interpretations and definitions that are contextually accurate.
Regex Filtering: Combines traditional pattern matching techniques with AI models to ensure precise extraction of financial and legal terms.
Workflow Overview
Document Acquisition: The process begins with retrieving the document from Azure Blob Storage, followed by artifact initialization to prepare the document for analysis.
Document Parsing and Chunking: Azure Document Intelligence parses the document into logical sections, enabling targeted analysis of specific content areas.
Filtering and AI-Driven Extraction: Key terms and relevant paragraphs are identified using regex filtering and AI models, focusing on specific financial terms related to management fees and other expenses.
Interpretation and Summarization: AI models provide detailed interpretations of the identified content, breaking down complex legal and financial information into manageable insights.
Output Generation: The system produces clear, organized summaries and interpretations that can be used directly by stakeholders for financial analysis, legal compliance, or reporting purposes.
Benefits
Enhanced Accuracy and Efficiency: Automates the extraction and interpretation of critical information, significantly reducing the risk of human error.
Time Savings: Speeds up the document review process, allowing analysts and legal professionals to focus on higher-value tasks rather than manual content extraction.
Improved Decision-Making: Provides actionable insights by breaking down complex financial terms into understandable summaries, facilitating better decision-making and strategic planning.
Scalability: The system is designed to handle large volumes of documents, making it suitable for organizations with extensive document processing needs.
Use Cases
Financial Institutions: For processing investment fund documents, understanding fee structures, and ensuring compliance with regulatory requirements.
Legal Firms: Assisting in the review of complex contracts, identifying critical terms, and providing clear interpretations for legal advice.
Corporate Finance Teams: Analyzing management fees, compensation terms, and other expenses to optimize financial management and strategy.
Conclusion
This project leverages state-of-the-art AI and cloud technologies to transform how complex financial documents are processed and interpreted. By automating key aspects of document analysis, the system not only enhances accuracy and efficiency but also empowers users with valuable insights that drive better business outcomes.

Let me know if you need further customization or additional information!









ChatGPT can make mistakes. Check important








ChatGPT 4o







You said:
{

"version": 1,

"vars": {

"pm_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "pvt-markets"

},

"di_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "npeedevgtsd@1txtmrm-strgcont-txtmrm-01"

}, "input_document_path": {

"source": "AzureBlob",

I

params": { "

}

}

"connection": "{vars.pm_azure_blob_connection)",

"storage_location": "(payload.documents[0].storagePath.absolute_path}"

},

"artifacts": {

"pageDimensions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},
"documentLayout": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"logicalChunks": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}, "extractsUsingPrescribed Language": {

"source": "Transient",

I

"data": [],

"params": {},

"extant": false

},

"aiFilteredExtracts": {

"source": "Transient",

"data": [],

"params": {}, "extant": false
},

"keyTerms": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

}, "regexFilteredDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"keyTermDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"keyTermMapping":{

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"mFees Interpretations": {

"source": "AzureBlob",

data": [], "

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",
"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

},

"keyTermInterpretations": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}

}, "output": {

"pageDimensions": "(artifacts.pageDimensions.data}", "documentLayout": "(artifacts.documentLayout.data}",

"logicalChunks": "{artifacts.logicalChunks.data}",

"extractsUsing PrescribedLanguage": "{artifacts.extractsUsing PrescribedLanguage.data}",

"keyTerms": "{artifacts.keyTerms.data)",

"keyTermDefinitions": "{artifacts.keyTermDefinitions.data)",

"keyTermMapping": "{artifacts.keyTermMapping.data)",

"aiFilteredExtracts": "{artifacts.aiFilteredExtracts.data}", "mFees Interpretations": "{artifacts.mFees Interpretations.data)",

"keyTermInterpretations": "{artifacts.keyTermInterpretations.data}"

},
"stages": [

{

"capability": "DocumentParsing",

"method": "AzureDocument Intelligence",

"label": "DocumentParsing",

"processing_config": {

"processing_engine": "Azure-openai",

"model": "prebuilt-layout",

"endpoint": "https://npee-use2-dev-fr-01.cognitiveservices.azure.com/",

"key": "bacfb986c0874ace9211551852829213"

},

"input": {

"document_path": "{vars.input_document_path}"

}, "output": {

"(artifacts.logicalChunks.data}": "logicalChunks",

"{artifacts.documentLayout.data)": "document Layout",

"(artifacts.pageDimensions.data)": "pageDimensions"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Logical Chunks Regex Filtering",

"input": {

"logicalChunks": "(artifacts.logicalChunks.data}",

"keywords": [

"Management Fee",

"Management +Fee",

I

}

"Servicing Fee", "Servicing +Fee",

"Investment Advisory Fee",
"Investment +Advisory +Fee",

"Compensation",

"Remuneration",

"Expenses",

"AIFM Fee",

"AIFM +Fee"

},

]

"{artifacts.extractsUsing Prescribed Language.data}": "extractsUsing Prescribed Language"

}

"output": {

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered MFees Related Extracts",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

}, "temperature": 0,

"seed": 17

"prompt": {

"template":" Objective \nYou will be given a list of paragraphs. You need to identify the paragraphs that helpenser one or more of exes which helped answer one or more of the questions. Ind. Create a field 'questions: List of the question Indexes that was wered by the paragra (

"input variables"

"questions":{

"indexing required": true,

"data":[

"Can you explain the calculation, payment timing, and due dates of the Management Fees?"

"How are the Management Fees affected by the Fund's Investment in any Primary Investee fund or Secondary Investee Fund, "How is the Management Fee calculated and reduced in relation to various Investments and fees

"paragraphs": {

"indexing required": true,

"date": "(artifacts.extractsUsing PrescribedLanguage.data)"

"post process required": true

"output" (

"(artifacts.eiFilteredExtracts.data)": "aifilteredExtracts"

"capability": "GenAZExtraction",

"method": "Prompting",

"label": "Keyteres Identification",

"processing config":{

model connection: (

"model provider_type": "AzureüpenAI",

"openai api type": "azurM
"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 17

},

"prompt": {

"template": "\n# Objective\nYou will be given a list of paragraphs. Your task is to identify the keyterms present in eac

ing company, financial services company and partnership.\n7. Respond back with the list of keyterms against each Paragraph in title case

"input_variables": {

"paragraphs": {

"indexing_required": true,

"data": "{artifacts.aiFilteredExtracts.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.keyTerms.data}": "keyTerms"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Regex Filtered Keyterm Definitions",

input": { "

"logicalChunks": "fartifacts.logicalChunks.data}",

"keyTerms": "{artifacts.keyTerms.data}"

},
"output": {

"(artifacts.regexFilteredDefinitions.data)": "regexFilteredDefinitions"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered Keyterm Definitions",

processing_config": { "

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou will be provided with a numbered list of Paragraphs and Keyterms. For each of the provided Keyter

hat provides the definition of the Keyterm, return -1 within a list as the index response to the Keyterm.\n\n\n# Output Instructions:\n\n## Outpu

"input_variables": { "paragraphs ": {

"indexing_required": true,

"data": "(artifacts.regexFilteredDefinitions.data)"

}, "keyterms": {

"indexing_required": true,

"data": "{artifacts.keyTerms.data)"
}

}, "post_process_required": true

}, "output": {

"(artifacts.keyTermDefinitions.data)": "keyTermDefinitions",

"(artifacts.keyTermMapping.data)": "keyTermMapping"

}

"capability": "GenAIExtraction",

"method" : "Prompting",

"label": "AI MFees Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment funds and manag in title case or capital letters are not changed in the interpretation.\n6. Break down the interpretation into small sentences.\n7. If you are not able terpretations for the paragraph using the reference of 'Keyterm Definition Extracts' if required to understand the content of the paragraph for interpr

input_variables": { " "

mFees InterpretationsExtracts": {

"indexing_required": true,
"data": "(artifacts.aiFilteredExtracts.data}" },

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

}

},

"post_process_required": false

}, "output": {

"{artifacts.mFees Interpretations.data)": "mFees Interpretations"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Keyterm Definitions Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview", "openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment f wotation of the keyterm definition.\n\n# Understanding the layout of 'Keyterm Definition Extracts' provided in the i

The contents of 'Keyterm Definition Extracts' are bullet listed paragraphs, with each paragraph containing the mapping of s.",

"input_variables": {

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

},

"keyTermMapping": {

"indexing_required": true,

"data": "{artifacts.keyTermMapping.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.key TermInterpretations.data}": "key Term Interpretations"

}

}

]

}



""Create a high level flowchat of work flow and create a detailed technical stage wise documentation and create service wise functinolity write up
ChatGPT said:
ChatGPT
To create a comprehensive understanding of the provided JSON configuration, we'll break it down into a detailed flowchart of the workflow and a stage-wise documentation that includes service-wise functionality descriptions. Here's the approach:

High-Level Flowchart of Workflow
Input Document Acquisition

Source: Azure Blob Storage.
Connections: pm_azure_blob_connection and di_azure_blob_connection.
Data: Input document path fetched from Azure Blob.
Artifact Generation and Initialization

Artifacts Created:
Page Dimensions
Document Layout
Logical Chunks
Extracts Using Prescribed Language
AI Filtered Extracts
Key Terms
Key Term Definitions
Key Term Mapping
MFees Interpretations
Key Term Interpretations
Document Parsing Stage

Capability: DocumentParsing
Method: AzureDocument Intelligence (prebuilt-layout model).
Output: Logical chunks, document layout, and page dimensions.
Regex Filtering Stage

Capability: GenAIExtraction
Method: RegexFiltering
Input: Logical chunks filtered with specified keywords (Management Fees, Compensation, etc.).
Output: Extracts using Prescribed Language.
AI Filtering of Extracts

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: AI Filtered MFees Related Extracts.
Key Terms Identification

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Terms.
Regex Filtering of Key Term Definitions

Capability: GenAIExtraction
Method: RegexFiltering
Output: Regex Filtered Key Term Definitions.
AI Filtered Key Term Definitions

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Definitions and Mapping.
AI MFees Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: MFees Interpretations.
AI Key Term Definitions Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Interpretations.
Stage-wise Documentation
Input Document Acquisition

Description: Fetches documents from Azure Blob Storage using managed identity connections (pm_azure_blob_connection and di_azure_blob_connection).
Purpose: To provide input data for processing through the Azure OpenAI models.
Document Parsing

Description: Parses the document to identify layout and logical chunks using Azure Cognitive Services.
Functionality:
Extracts structural information like page dimensions and layout.
Identifies logical sections of the document for further analysis.
Regex Filtering of Logical Chunks

Description: Uses regex patterns to filter logical chunks for specific keywords related to fees and financial terms.
Functionality:
Focuses on terms like "Management Fee," "Compensation," and other financial keywords.
Extracts text segments relevant to fee calculations and descriptions.
AI Filtering of Extracts

Description: Applies Azure OpenAI models to refine filtered chunks by identifying relevant fee-related content.
Functionality:
Uses prompting techniques to focus on answering specific financial questions.
Outputs filtered extracts that provide relevant answers.
Key Terms Identification

Description: Identifies key financial and legal terms within the filtered extracts.
Functionality:
Uses AI models to detect and list key terms.
Provides context for understanding financial terminology within the document.
Regex Filtering of Key Term Definitions

Description: Further filters the key terms to refine their definitions using regex patterns.
Functionality:
Matches and extracts text that defines the identified key terms.
AI Filtered Key Term Definitions

Description: AI models provide detailed definitions and mapping for identified key terms.
Functionality:
Outputs refined definitions that are contextually accurate for financial interpretations.
MFees Interpretations

Description: Interprets the filtered extracts related to management fees and other financial terms.
Functionality:
Breaks down complex financial text into manageable summaries.
Provides interpretative insights for legal and financial understanding.
Key Term Definitions Interpretations

Description: Provides a clear, summarized interpretation of key term definitions with contextual mapping.
Functionality:
Offers a simplified, accurate understanding of complex legal and financial definitions.
Supports clearer comprehension for non-technical stakeholders.
Service-Wise Functionality Write-up
Azure Blob Storage

Manages document storage and retrieval securely using managed identity for access.
Azure Cognitive Services - Document Intelligence

Parses and analyzes document structure, enabling logical division and understanding of document layouts.
Azure OpenAI Services

Implements AI-driven extraction, filtering, and interpretation of financial and legal text.
Models configured with specific prompting strategies to ensure accurate, relevant outputs.
Regex Filtering and AI Extraction Methods

Ensures precise identification of terms and extracts based on defined patterns and AI model capabilities.
Enhances the accuracy of document analysis by combining deterministic regex and probabilistic AI techniques.
This documentation provides a detailed overview of each step, outlining how Azure and OpenAI services are used to process, extract, and interpret complex financial documents effectively. Let me know if you need any specific adjustments or further details!






You said:
I need flow chat diagram 
ChatGPT said:
ChatGPT

Create a flowchart diagram illustrating a document processing workflow using Azure and AI services. The flowchart should include the following stages: 1) Input Document Acquisition from Azure Blob Storage, 2) Artifact Generation and Initialization with components like Page Dimensions, Document Layout, Logical Chunks, etc., 3) Document Parsing using Azure Document Intelligence, 4) Regex Filtering of Logical Chunks with specific keywords, 5) AI Filtering of Extracts using Azure OpenAI, 6) Key Terms Identification using AI, 7) Regex Filtering of Key Term Definitions, 8) AI Filtered Key Term Definitions, 9) AI MFees Interpretations, and 10) AI Key Term Definitions Interpretations. The flowchart should have clear directional arrows showing the flow of data between each stage, with each stage represented by a distinct box labeled accordingly.
Here is the flowchart diagram illustrating the document processing workflow using Azure and AI services. It includes all the stages from document acquisition to AI interpretations as described. You can review and use it for your reference!












ChatGPT

You said:
Ai reletad taks like working on documents ""i need to send the information to my officer please give me content 
ChatGPT said:
ChatGPT
To assist you in preparing content to send to your officer, I will need more details about the specific task or type of information you're working on. Could you clarify:

What kind of document are you preparing (e.g., report, memo, email)?
What specific information needs to be included (e.g., project updates, data analysis, etc.)?
Any particular format or tone you prefer (formal, brief, detailed)?
Feel free to provide more details, and I’ll help craft the content for you.






You said:
Al related tasks:

Step 1: loading pdf document

Step 2: using pdf miner or ADI we are generating the text(Prescribed language)

Step 3: Extracts management fees related extracts from prescribed language(step 2 output we are giving as input)

Step 4: Identify the keyterms from management fees related extracts(step 3 output we are giving as input)

Step 5: extracts keyterm defination from keyterms identification ((step 4 output we are giving as input)

I

Search

Inibss
ChatGPT said:
ChatGPT
It sounds like you're describing an AI workflow for processing documents, specifically for extracting and analyzing management fees-related terms from PDF documents. Here’s a refined summary based on your steps:

AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!






You said:
Can u give me objective of the program. And whatvis your role ofvthe program " they are asking my role is building the models and also evalutingthe the model i
ChatGPT said:
ChatGPT
Here’s a potential objective for the program and a description of your role based on the tasks you’ve outlined:

Objective of the Program:
The objective of this program is to automate the extraction, identification, and analysis of management fee-related information from PDF documents using AI and Natural Language Processing (NLP) techniques. The system aims to streamline the process of reading complex documents, isolating specific fee-related sections, identifying key financial terms, and extracting their definitions. This will help in making fee structures and related information more accessible and manageable for further analysis or reporting.

Your Role in the Program:
As a Model Developer and Evaluator, your role involves:

Building NLP Models:

Designing and implementing AI models that process PDF documents, extract relevant text, and identify key terms related to management fees.
Utilizing tools like PDFMiner for document processing and NLP libraries (e.g., NLTK, spaCy) for extracting and analyzing text.
Developing customized models for key term extraction and definition identification, ensuring they are aligned with the prescribed language and specific requirements of the task.
Evaluating Models:

Testing and validating the accuracy and efficiency of the models to ensure the extracted information is relevant, correct, and in line with expected outputs.
Evaluating the performance of the models using metrics like precision, recall, and F1-score to measure the quality of extraction and identification processes.
Continuously improving models based on evaluation results, feedback, and new data to enhance overall performance.
Optimizing the Workflow:

Ensuring the entire process, from loading documents to extracting definitions, is seamless and optimized for scalability.
Collaborating with other team members to integrate the AI models into the broader system and refine them based on user needs or new use cases.
This should give you a solid overview of both the program’s objective and your critical role within it. Let me know if you'd like further adjustments or clarifications!






Is this conversation helpful so far?












AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!











import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples or numpy arrays
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and contains the necessary keys
    expected_keys = {"accuracy", "precision", "recall", "F1 score"}
    if not isinstance(scores, dict):
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', 'recall', 'F1 score'.")
    missing_keys = expected_keys - scores.keys()
    if missing_keys:
        raise ValueError(f"The following keys are missing in the 'scores' dictionary: {missing_keys}")

    # Populate the DataFrame with the scores in the first row
    df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
    df.loc[0, "Precision"] = scores.get("precision", np.nan)
    df.loc[0, "Recall"] = scores.get("recall", np.nan)
    df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df










import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
        df.loc[0, "Precision"] = scores.get("precision", np.nan)
        df.loc[0, "Recall"] = scores.get("recall", np.nan)
        df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(item)
        elif isinstance(item, dict):
            return tuple(sorted(item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # If actual or predicted contain dictionaries, convert them to tuples
    actual_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in actual)
    predicted_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Create sets for comparison
    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):

    def equalize_lengths(actual, predicted):
        len_actual = len(actual)
        len_predicted = len(predicted)

        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    all_items = list(actual_set.union(predicted_set))

    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df





import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    # Convert lists of dictionaries to lists of strings (or tuples) to make them hashable
    actual_set = set([str(item) for item in actual])
    predicted_set = set([str(item) for item in predicted])

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available")

    def connect_to_azure_blob(self, input_document_path):
        """
        Connect to Azure Blob Storage using Managed Identity.
        """
        try:
            connection = input_document_path['params']['connection_string']
            mi_client_id = connection["mi_client_id"]

            credential = ManagedIdentityCredential(client_id=mi_client_id)

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        absolute_path = params["absolute_path"]
        blob_name = os.path.basename(absolute_path)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        try:
            with open(pdf_file_path, 'wb') as pdf_file:
                download_stream = blob_client.download_blob()
                pdf_file.write(download_stream.readall())
        except Exception as e:
            logger.error(f"Failed to download blob {blob_name} from Azure Blob Storage: {e}")
            return None

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        """
        Run ADI processing on the downloaded PDF.
        """
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key
            )
            return merged_content
        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processingConfig = {
    "processing_engine": "ADI",
    "model": "prebuilt-layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c0c7c3a9a4371b7dc983ac7b618b7"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://eventhubstorage919.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "5da5a4c-edb2-4f9f-8ad8-d11f0214f557"
        },
        "absolute_path": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_1p_-_2nd_er_lpa_march_16_20211.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processingConfig, input_document_path)
print(merged_content)








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available.")

    def connect_to_azure_blob(self, input_document_path):
        # Connect to Azure Blob Storage using Managed Identity.
        connection_string = input_document_path["params"]["connection_string"]

        try:
            credential = ManagedIdentityCredential(client_id=connection_string["mi_client_id"])

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection_string["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection_string["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        storage_location = params["storage_location"]
        blob_name = os.path.basename(storage_location)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        with open(pdf_file_path, 'wb') as pdf_file:
            download_stream = blob_client.download_blob()
            pdf_file.write(download_stream.readall())

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        # Download PDF from Azure Blob
        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            # Example method calls assuming run_prescribed_extracts_azure exists
            page_dim_json_file_path, _doc_layout_json_file_path, doc_layout = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            return merged_content

        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processing_config = {
    "processing_engine": "ADI",
    "model": "Layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://<your-account-name>.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "<your-managed-identity-client-id>"
        },
        "storage_location": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_ip_-_2nd_ar_lpa_march_16_2021.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processing_config, input_document_path)

print(merged_content)







import os
import json
import re

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        if merged_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    Simulate the extraction of definitions based on indexed key terms.
    """
    # This is a placeholder for the actual definition extraction logic.
    return json.dumps({"definitions": quotation_marks_paragraphs})

def load_json(file_path):
    """
    Load JSON data from a file.
    :param file_path: Path to the JSON file.
    :return: Parsed JSON data.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    """
    Process a PDF and associated logical chunks file to extract key terms and their definitions.
    :param pdf_path: Path to the input PDF.
    :param logical_chunks_file: Path to the JSON file containing logical chunks.
    :param extracted_keyterms_json_path: Path to the JSON file containing key terms.
    :param output_dir: Directory where the output JSON will be saved.
    :return: JSON output and the path to the saved JSON file.
    """
    # Check if the PDF name matches the logical chunks file name
    pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    logical_chunks_base_name = os.path.splitext(os.path.basename(logical_chunks_file))[0].replace("logicalchunks", "").strip()

    if pdf_base_name != logical_chunks_base_name:
        raise ValueError(f"Mismatch: PDF base name '{pdf_base_name}' does not match logical chunks base name '{logical_chunks_base_name}'.")

    extracted_definition_json_file_name = pdf_base_name + "_generated_definitions.json"
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms
    with open(extracted_keyterms_json_path, "r") as json_file:
        data = json.load(json_file)

    keyterm_index = 0
    list_indexed_keyterms = []

    search_strings = data["keyTerms"]
    for search_string in search_strings:
        indexed_keyterm = f"{keyterm_index} {search_string}"
        keyterm_index += 1
        list_indexed_keyterms.append(indexed_keyterm)

    # Load logical chunks
    file_data = load_json(logical_chunks_file)

    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            # For chunks that need merging of content
            if len(items) >= 2:
                merged_para = ""
                for i in range(len(items)):
                    para = items[i]["content"]
                    merged_para += " " + para
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para.strip(), quotation_marks_paragraphs, count)
            # For chunks having only one content
            elif len(items) == 1:
                for contents in items:
                    content = contents["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions based on the filtered paragraphs
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)
    json_output = json.loads(output)

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(json_output, ff, ensure_ascii=False, indent=4)

    print(f"Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}")

    return json_output, extracted_definition_json_path

# Testing the function
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H://management_fee_extraction//extracts_output//output7//"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)







import os
import json
import re

def load_json(file_path):
    """Utility function to load JSON data from a file."""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        indexing_para = merged_para
        if indexing_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    This is a placeholder for the actual implementation of extracting definitions.
    It would process the filtered paragraphs and indexed keyterms to produce output.
    """
    # Assuming this function returns a JSON-serializable object as output.
    return {"definitions": [{"keyterm": keyterm, "paragraph": para} for keyterm, para in zip(list_indexed_keyterms, quotation_marks_paragraphs)]}

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    extracted_definition_json_file_name = os.path.basename(pdf_path)
    extracted_definition_json_file_name = extracted_definition_json_file_name.replace(".pdf", "_generated_definitions.json").replace(".PDF", "_generated_definitions.json")
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms from the JSON file
    data = load_json(extracted_keyterms_json_path)
    search_strings = data["keyTerms"]

    # Index key terms
    list_indexed_keyterms = [str(i) + " " + search_string for i, search_string in enumerate(search_strings)]

    # Load logical chunks
    file_data = load_json(logical_chunks_file)
    
    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            if len(items) >= 2:
                merged_para = ""
                for item in items:
                    merged_para += item["content"] + " "
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count)
            elif len(items) == 1:
                for item in items:
                    content = item["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(output, ff, ensure_ascii=False, indent=4)

    return output, extracted_definition_json_path

# Testing
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H:\management_fee_extraction\extracts_output\output7"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)
import re

def get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param count: The current count of filtered paragraphs.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :return: Updated list of filtered paragraphs and the count.
    """
    # Correct the pattern for regex search
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'

    # Check if the pattern is found in the merged paragraph
    if re.search(pattern, merged_para):
        indexing_para = merged_para

        # Ensure the paragraph is not already in the list
        if indexing_para not in quotation_marks_paragraphs:
            # Correct string concatenation and append the paragraph
            filtered_para = str(count) + " " + merged_para
            print(f"filtered_para: {filtered_para}")

            quotation_marks_paragraphs.append(filtered_para)
            count += 1

    return quotation_marks_paragraphs, count

# Example usage:
search_string = "Management Fee"
merged_para = "This paragraph talks about Management Fee and other details."
count = 1
quotation_marks_paragraphs = []

result, updated_count = get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs)

print("Filtered Paragraphs:", result)
print("Updated Count:", updated_count)





import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of search terms
        :return: List of compiled regex patterns
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content to search within
        :param patterns: List of compiled regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :param patterns: List of compiled regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        for item in data['logicalchunks']:
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge content from the provided JSON data
        :param json_data: Input JSON data as a dictionary
        :return: Dictionary with merged content
        """
        data = json.loads(json.dumps(json_data))
        combined_strings = []

        for item in data['extractsUsingPrescribedLanguage']:
            combined_string = ' '.join(item['content'] for item in item)
            combined_strings.append(combined_string)

        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data to a file
    :param json_file_path: Path to the JSON file
    :param json_data: Data to write to the file
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file)
    except Exception as e:
        print(f"Error dumping JSON file: {e}")

# File paths
data_file_path = "H:/management_fee_extraction/extracts_output/output5/LEGAL-#148874-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
output_dest_dir = "H:/management_fee_extraction/extracts_output/output5"

with open(data_file_path, "r") as json_file:
    data = json.load(json_file)

obj = KeywordFilterMethod()
paragraphs = obj.extract(data)

print(paragraphs)

paragraphs = {"extractsUsingPrescribedLanguage": paragraphs}

file_name = os.path.splitext(os.path.basename(data_file_path))[0]

filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
dump_json_file(filtered_images_json_file_path, paragraphs)

merged_content = obj.merge_content(paragraphs)
merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

print(merged_content_json_file_path)
print(merged_content)

dump_json_file(merged_content_json_file_path, merged_content)






import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        return self.extract_data_with_terms(data, patterns)

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of keywords
        :return: List of compiled regex patterns
        """
        return [re.compile(re.escape(term)) for term in terms]

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content of the node
        :param patterns: List of regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :param patterns: List of regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        logical_chunks = data.get('logicalChunks', [])  # Ensure it's a list
        for item in logical_chunks:
            if isinstance(item, list):  # Check if item is a list
                for node in item:
                    if isinstance(node, dict):  # Check if node is a dictionary
                        content = node.get('content', '')  # Safely get content
                        if self.search_terms_in_content(content, patterns):
                            extracted_data.append(item)
                            break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data: JSON data containing extractsUsingPrescribedLanguage
        :return: Merged content dictionary
        """
        extracts = json_data.get('extractsUsingPrescribedLanguage', [])  # Ensure it's a list
        combined_strings = []
        for item in extracts:
            if isinstance(item, list):  # Check if item is a list
                combined_string = ' '.join(node.get('content', '') for node in item if isinstance(node, dict))
                combined_strings.append(combined_string)
        return {'extractsUsingPrescribedLanguage': combined_strings}

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path: Path to save the JSON file
    :param json_data: Data to save
    :return: None
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))

