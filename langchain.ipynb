{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1069fce9e934db9a89bc14a3471837c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6067b8adfb4f4d9c875acec37cd64849",
              "IPY_MODEL_88b29eb9d502499aa70da8fdb26d8436",
              "IPY_MODEL_23f523f2397a4e819ba906210d45a691"
            ],
            "layout": "IPY_MODEL_64f412fe8190482288122de8f57fb9d2"
          }
        },
        "6067b8adfb4f4d9c875acec37cd64849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efb2d3c5f8f748b181d0536f160695d3",
            "placeholder": "​",
            "style": "IPY_MODEL_09d3c4a0dca2404bafc2552bd21120a7",
            "value": "Fetching 5 files: 100%"
          }
        },
        "88b29eb9d502499aa70da8fdb26d8436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be25cf42cbbc4f4898367fd1cc25c03e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73dbe7750def4092a338184f95c82da3",
            "value": 5
          }
        },
        "23f523f2397a4e819ba906210d45a691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f800ff3840a54a3cbf2f701d62491c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_cc1c941f66814cbb9839d32225bbca7f",
            "value": " 5/5 [00:02&lt;00:00,  1.44s/it]"
          }
        },
        "64f412fe8190482288122de8f57fb9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb2d3c5f8f748b181d0536f160695d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d3c4a0dca2404bafc2552bd21120a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be25cf42cbbc4f4898367fd1cc25c03e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73dbe7750def4092a338184f95c82da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f800ff3840a54a3cbf2f701d62491c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1c941f66814cbb9839d32225bbca7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e516bd5b1e504e34a3ce87b92a092114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2222b07218da4006babfb7171da5b4ca",
              "IPY_MODEL_140e1f35c3e64c26971b78c4e75aff0a",
              "IPY_MODEL_e90d1b1eda2346d7a7f77c258ce19870"
            ],
            "layout": "IPY_MODEL_b0daf348bd51405ba302e6e4b8c4323a"
          }
        },
        "2222b07218da4006babfb7171da5b4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96a3ca38383743919d364746544f9cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_67ae62987f504a72aae5d00e09d9bf3e",
            "value": "tokenizer.json: 100%"
          }
        },
        "140e1f35c3e64c26971b78c4e75aff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_354f4a4c71b34f7cb014c01b5c512b12",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_377a2228557440ff96c816f76628f8ce",
            "value": 711396
          }
        },
        "e90d1b1eda2346d7a7f77c258ce19870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c07c17f5788d43cb9c6c782dd2341ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_8d6f86ce1956474ea318b7f4a5e05440",
            "value": " 711k/711k [00:00&lt;00:00, 3.32MB/s]"
          }
        },
        "b0daf348bd51405ba302e6e4b8c4323a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a3ca38383743919d364746544f9cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ae62987f504a72aae5d00e09d9bf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "354f4a4c71b34f7cb014c01b5c512b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377a2228557440ff96c816f76628f8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c07c17f5788d43cb9c6c782dd2341ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6f86ce1956474ea318b7f4a5e05440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9773f36d82674c79890acc7d6c4af03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c97ea3c194445cc80d5e4e506754685",
              "IPY_MODEL_50bcc0a50d824b0e872ba9550b56f0e3",
              "IPY_MODEL_9e67e4146fa94458b085da12fabab9c1"
            ],
            "layout": "IPY_MODEL_cfd106cf14b24812a29fd4e271c5fa60"
          }
        },
        "5c97ea3c194445cc80d5e4e506754685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec1f0005ecf49d5a4f0ba084664274e",
            "placeholder": "​",
            "style": "IPY_MODEL_73da1153194248f8834c5758d6ab2554",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "50bcc0a50d824b0e872ba9550b56f0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159d24e8884a4d8cbba7e2e72c204f74",
            "max": 1242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f3c2853d1fe4894a0b3f0068018c066",
            "value": 1242
          }
        },
        "9e67e4146fa94458b085da12fabab9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_044aea42e0864684ad64eecbc8c55f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_78f873aff64f4201813bb688557b998f",
            "value": " 1.24k/1.24k [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "cfd106cf14b24812a29fd4e271c5fa60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec1f0005ecf49d5a4f0ba084664274e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73da1153194248f8834c5758d6ab2554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159d24e8884a4d8cbba7e2e72c204f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3c2853d1fe4894a0b3f0068018c066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "044aea42e0864684ad64eecbc8c55f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f873aff64f4201813bb688557b998f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c845b76c77584130ae4e6e3ed075e738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b4f9d7c77de42a88497939ed3a5a919",
              "IPY_MODEL_888dcb3602c44f7da3388584ecc1404c",
              "IPY_MODEL_4c4c8e3fc828429ead99de422046724c"
            ],
            "layout": "IPY_MODEL_586a7e0198c9432b96387a8ab7d92ff4"
          }
        },
        "8b4f9d7c77de42a88497939ed3a5a919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aea687509f64dadaf7e2c820d45e50e",
            "placeholder": "​",
            "style": "IPY_MODEL_64d0813edd924a54829adc8d86f4f3f0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "888dcb3602c44f7da3388584ecc1404c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c28e26ee6204af6b7a7868530b7946a",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3680627758b407996558e6d2f40ad41",
            "value": 695
          }
        },
        "4c4c8e3fc828429ead99de422046724c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a0ecae417d45a1aeab645f9e14207e",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3f0b1d06e644fb86a5e19ebb6175b0",
            "value": " 695/695 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "586a7e0198c9432b96387a8ab7d92ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aea687509f64dadaf7e2c820d45e50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d0813edd924a54829adc8d86f4f3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c28e26ee6204af6b7a7868530b7946a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3680627758b407996558e6d2f40ad41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a0ecae417d45a1aeab645f9e14207e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3f0b1d06e644fb86a5e19ebb6175b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8887b5d2aacf444abbe150814735327c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9a57d5b66ea44fea458adf4229a00b5",
              "IPY_MODEL_d1a90ff9aa08444188a56142a309f990",
              "IPY_MODEL_cf663225320341278bdd355d51bee786"
            ],
            "layout": "IPY_MODEL_510a4287650c421fbc086a64b144222a"
          }
        },
        "d9a57d5b66ea44fea458adf4229a00b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c0733864454c8996c763b086254fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_ad90060e8e894a769916f0b30fda7ee7",
            "value": "config.json: 100%"
          }
        },
        "d1a90ff9aa08444188a56142a309f990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d9f05f08874ae6a48d68f429b298ee",
            "max": 740,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cc2fa800d824d4e80737c936ff03f48",
            "value": 740
          }
        },
        "cf663225320341278bdd355d51bee786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89267f7e68aa460e91a7d48b5ba9093a",
            "placeholder": "​",
            "style": "IPY_MODEL_9b5d552704244faba3142df8cd4d1bf0",
            "value": " 740/740 [00:00&lt;00:00, 8.00kB/s]"
          }
        },
        "510a4287650c421fbc086a64b144222a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c0733864454c8996c763b086254fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad90060e8e894a769916f0b30fda7ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1d9f05f08874ae6a48d68f429b298ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc2fa800d824d4e80737c936ff03f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89267f7e68aa460e91a7d48b5ba9093a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5d552704244faba3142df8cd4d1bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30627dc2ed8e479ba79498228145d529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93220d7a5bcf46aab6921d56e38a76c0",
              "IPY_MODEL_a1a6b23aaea743f79aa784e7e936fc9f",
              "IPY_MODEL_fdcffc7a65b84c14a3f9631f348647a8"
            ],
            "layout": "IPY_MODEL_53d8d621b5414394a5117c7cfdee51f7"
          }
        },
        "93220d7a5bcf46aab6921d56e38a76c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_879ef5190bf84644971245b1a43c8d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_4deed6c6930b4fe3a6f59f20704b074c",
            "value": "model_optimized.onnx: 100%"
          }
        },
        "a1a6b23aaea743f79aa784e7e936fc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8910a6d6f5874cd99adc7c6b5ea4ad36",
            "max": 217824172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b76d93732fd14c0895a49e1785880783",
            "value": 217824172
          }
        },
        "fdcffc7a65b84c14a3f9631f348647a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4402883ff24c4d918b198ec4b5b37d54",
            "placeholder": "​",
            "style": "IPY_MODEL_4a4a22ec38b94f94bed3f1d685b54ab2",
            "value": " 218M/218M [00:01&lt;00:00, 122MB/s]"
          }
        },
        "53d8d621b5414394a5117c7cfdee51f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879ef5190bf84644971245b1a43c8d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4deed6c6930b4fe3a6f59f20704b074c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8910a6d6f5874cd99adc7c6b5ea4ad36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76d93732fd14c0895a49e1785880783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4402883ff24c4d918b198ec4b5b37d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4a22ec38b94f94bed3f1d685b54ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-pibYO7zN6v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas chromadb langchain-groq fastembed pypdf openai"
      ],
      "metadata": {
        "id": "dOi-pEn8zR_p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import torch\n",
        "\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "wM-2284fzyP_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TextChunker:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_text_from_pdf(self, path = \"/\"):\n",
        "        text = \"\"\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents = loader.load()\n",
        "        for doc in documents:\n",
        "            text = text + \"\\n\" + str(doc.page_content)\n",
        "        return text,documents\n",
        "\n",
        "    def char_count_chunking_with_overlap(self, text, chunk_size=200, chunk_overlap=50, Recursive = False):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        if Recursive:# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "    def char_count_chunking_with_nonoveralp(self, text, chunk_size=200, Recursive = False):\n",
        "       # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "        if Recursive:# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size)\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    def char_count_chunking_with_custom_delimiter(self, text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",Recursive = False):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        if Recursive:# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "    def semantic_section_chunking(self, text , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\"):\n",
        "        embed_model = FastEmbedEmbeddings(model_name = text_embedding_model_name)\n",
        "        semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=breakpoint_threshold_type)\n",
        "        semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
        "        return semantic_chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "xNCgyIqwzrvA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path =r\"/content/drive/MyDrive/chunking/ds (1).pdf\"\n",
        "\n",
        "text_chunker = TextChunker()\n",
        "text, documents  = text_chunker.extract_text_from_pdf(path = pdf_path )"
      ],
      "metadata": {
        "id": "GBO7EgWi0Xp8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_overlap(text, chunk_size=500, chunk_overlap=40)\n",
        "print(\"Char count chunking _with_overlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10PLJKJR1cQa",
        "outputId": "7b50f45b-080f-4986-e18f-6c7254c79b36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_overlap:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_nonoveralp(text, chunk_size=500)\n",
        "print(\"Char count chunking _with_nonoverlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpHaI24h1sI0",
        "outputId": "271df507-3999-496d-b17c-f7307d769fcc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_nonoverlap:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPuKFgO0AS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",Recursive = False)\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37xIl_nATRi",
        "outputId": "3db157e0-4d10-476f-d2be-c412af545388"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYfQlZCjArl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "a1069fce9e934db9a89bc14a3471837c",
            "6067b8adfb4f4d9c875acec37cd64849",
            "88b29eb9d502499aa70da8fdb26d8436",
            "23f523f2397a4e819ba906210d45a691",
            "64f412fe8190482288122de8f57fb9d2",
            "efb2d3c5f8f748b181d0536f160695d3",
            "09d3c4a0dca2404bafc2552bd21120a7",
            "be25cf42cbbc4f4898367fd1cc25c03e",
            "73dbe7750def4092a338184f95c82da3",
            "f800ff3840a54a3cbf2f701d62491c5f",
            "cc1c941f66814cbb9839d32225bbca7f",
            "e516bd5b1e504e34a3ce87b92a092114",
            "2222b07218da4006babfb7171da5b4ca",
            "140e1f35c3e64c26971b78c4e75aff0a",
            "e90d1b1eda2346d7a7f77c258ce19870",
            "b0daf348bd51405ba302e6e4b8c4323a",
            "96a3ca38383743919d364746544f9cb3",
            "67ae62987f504a72aae5d00e09d9bf3e",
            "354f4a4c71b34f7cb014c01b5c512b12",
            "377a2228557440ff96c816f76628f8ce",
            "c07c17f5788d43cb9c6c782dd2341ba4",
            "8d6f86ce1956474ea318b7f4a5e05440",
            "9773f36d82674c79890acc7d6c4af03b",
            "5c97ea3c194445cc80d5e4e506754685",
            "50bcc0a50d824b0e872ba9550b56f0e3",
            "9e67e4146fa94458b085da12fabab9c1",
            "cfd106cf14b24812a29fd4e271c5fa60",
            "fec1f0005ecf49d5a4f0ba084664274e",
            "73da1153194248f8834c5758d6ab2554",
            "159d24e8884a4d8cbba7e2e72c204f74",
            "6f3c2853d1fe4894a0b3f0068018c066",
            "044aea42e0864684ad64eecbc8c55f0d",
            "78f873aff64f4201813bb688557b998f",
            "c845b76c77584130ae4e6e3ed075e738",
            "8b4f9d7c77de42a88497939ed3a5a919",
            "888dcb3602c44f7da3388584ecc1404c",
            "4c4c8e3fc828429ead99de422046724c",
            "586a7e0198c9432b96387a8ab7d92ff4",
            "5aea687509f64dadaf7e2c820d45e50e",
            "64d0813edd924a54829adc8d86f4f3f0",
            "8c28e26ee6204af6b7a7868530b7946a",
            "a3680627758b407996558e6d2f40ad41",
            "e0a0ecae417d45a1aeab645f9e14207e",
            "6f3f0b1d06e644fb86a5e19ebb6175b0",
            "8887b5d2aacf444abbe150814735327c",
            "d9a57d5b66ea44fea458adf4229a00b5",
            "d1a90ff9aa08444188a56142a309f990",
            "cf663225320341278bdd355d51bee786",
            "510a4287650c421fbc086a64b144222a",
            "b8c0733864454c8996c763b086254fb8",
            "ad90060e8e894a769916f0b30fda7ee7",
            "b1d9f05f08874ae6a48d68f429b298ee",
            "1cc2fa800d824d4e80737c936ff03f48",
            "89267f7e68aa460e91a7d48b5ba9093a",
            "9b5d552704244faba3142df8cd4d1bf0",
            "30627dc2ed8e479ba79498228145d529",
            "93220d7a5bcf46aab6921d56e38a76c0",
            "a1a6b23aaea743f79aa784e7e936fc9f",
            "fdcffc7a65b84c14a3f9631f348647a8",
            "53d8d621b5414394a5117c7cfdee51f7",
            "879ef5190bf84644971245b1a43c8d2c",
            "4deed6c6930b4fe3a6f59f20704b074c",
            "8910a6d6f5874cd99adc7c6b5ea4ad36",
            "b76d93732fd14c0895a49e1785880783",
            "4402883ff24c4d918b198ec4b5b37d54",
            "4a4a22ec38b94f94bed3f1d685b54ab2"
          ]
        },
        "id": "7oiY0B-aAro-",
        "outputId": "20228bee-c4eb-412d-bd80-e434e2799f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1069fce9e934db9a89bc14a3471837c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e516bd5b1e504e34a3ce87b92a092114"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9773f36d82674c79890acc7d6c4af03b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c845b76c77584130ae4e6e3ed075e738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8887b5d2aacf444abbe150814735327c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_optimized.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30627dc2ed8e479ba79498228145d529"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "SldDgWylAsFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}