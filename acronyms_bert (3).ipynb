{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TEXxGBdDwul"
      },
      "outputs": [],
      "source": [
        "#Using Spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "eAyHeUaVEef5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acronym mapping\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "\n",
        "def replace_acronyms(text, acronym_map):\n",
        "    doc = nlp(text)  # Process the input text using spaCy\n",
        "\n",
        "    replaced_text = []\n",
        "    for token in doc:\n",
        "        # If the token is an acronym, replace it with its full form from acronym_map\n",
        "        if token.text in acronym_map:\n",
        "            # Replace with full form, checking context if needed (you can expand this)\n",
        "            replaced_text.append(acronym_map[token.text])\n",
        "        else:\n",
        "            replaced_text.append(token.text)\n",
        "\n",
        "    return \" \".join(replaced_text)\n",
        "\n",
        "# Example usage\n",
        "text = \"The CD for the project is approaching, and the GP is discussing the IP extension with the LI.\"\n",
        "replaced_text = replace_acronyms(text, acronym_map)\n",
        "print(replaced_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUsh_iEEE3mZ",
        "outputId": "1cbe0588-b812-4f71-b1f0-08f95e747011"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Closing Date for the project is approaching , and the General Partner is discussing the Investment Period extension with the LI.These all write in Closing Date storage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fyn61bRLF1ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using regular expression"
      ],
      "metadata": {
        "id": "4fWYDhyjGNjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define the acronym mapping\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "# Function to replace acronyms using regular expressions\n",
        "def replace_acronyms(text, acronym_map):\n",
        "    # Replace acronyms in the text\n",
        "    for acronym, full_form in acronym_map.items():\n",
        "        regex_pattern = r'\\b' + re.escape(acronym) + r'\\b'\n",
        "        text = re.sub(regex_pattern, full_form, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example text input\n",
        "input_text = \"The CD is set, and the GP along with LI invested in the IF during the IP. This entire information available in CD memory\"\n",
        "\n",
        "# Replace acronyms in the text\n",
        "output_text = replace_acronyms(input_text, acronym_map)\n",
        "\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6troZPxHGNmO",
        "outputId": "3e01375f-a090-4b3b-eaea-48d498d3164e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Closing Date is set, and the General Partner along with Lead Investor invested in the Investee Fund during the Investment Period. This entire information available in Closing Date memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jv2Q8vLsIs4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using spacy and re"
      ],
      "metadata": {
        "id": "DdhXewZXJSRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the acronym mapping\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "def replace_acronyms(text, acronym_map):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    # Prepare a mapping of replacements\n",
        "    replacements = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Extract tokens from the sentence\n",
        "        tokens = [token.text for token in sentence]\n",
        "\n",
        "        for acronym, full_form in acronym_map.items():\n",
        "            # If the acronym is in the sentence, check context\n",
        "            if acronym in tokens:\n",
        "                # Collect context words around the acronym\n",
        "                index = tokens.index(acronym)\n",
        "                before_context = tokens[max(0, index-5):index]\n",
        "                after_context = tokens[index+1:index+6]\n",
        "\n",
        "                # Check if the context suggests the acronym's full form\n",
        "                before_text = ' '.join(before_context).lower()\n",
        "                after_text = ' '.join(after_context).lower()\n",
        "\n",
        "                if any(word in full_form.lower() for word in before_text.split() + after_text.split()):\n",
        "                    replacements[acronym] = full_form\n",
        "\n",
        "    # Replace acronyms in the text\n",
        "    for acronym, full_form in replacements.items():\n",
        "        text = re.sub(r'\\b{}\\b'.format(re.escape(acronym)), full_form, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# Example text input\n",
        "input_text = \"The CD is set, and the GP along with LI invested in the IF during the IP. This entire information available in CD memory.\"\n",
        "\n",
        "\n",
        "output_text = replace_acronyms(input_text, acronym_map)\n",
        "\n",
        "print(\"Output text:\")\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1mhGjEJSUl",
        "outputId": "7a0a5f10-8610-40be-a48b-8db716982455"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            "The Closing Date is set, and the GP along with Lead Investor invested in the Investee Fund during the Investment Period. This entire information available in Closing Date memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0L3gO8nOJUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGFL3D5vOJaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using fuzzy wuzzy\n",
        "\n",
        "#!pip install fuzzywuzzy python-Levenshtein\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2frdLZ8OJeJ",
        "outputId": "0c8c4b36-610e-4aee-ff3e-190d8996313a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.25.1 rapidfuzz-3.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Acronym mapping\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "def replace_acronyms(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # Iterate over tokens and match with acronyms\n",
        "    new_tokens = []\n",
        "    for token in tokens:\n",
        "        # Find best match from acronym_map\n",
        "        match, score = process.extractOne(token, acronym_map.keys(), scorer=fuzz.partial_ratio)\n",
        "        if score > 80:  # Adjust threshold as needed\n",
        "            # Replace with the full term if there's a good match\n",
        "            new_tokens.append(acronym_map[match])\n",
        "        else:\n",
        "            new_tokens.append(token)\n",
        "\n",
        "    return ' '.join(new_tokens)\n",
        "\n",
        "# Example usage\n",
        "text = \"The CD for the DI is set for next week, and the GP needs to review the RF and DI.\"\n",
        "print(replace_acronyms(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-5wZcInUEg3",
        "outputId": "8c473f68-5250-4fe9-cc12-d63c55fc8541"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Closing Date for the Direct Investment is set for next week , and the General Partner needs to review the Related Funds and Direct Investment .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZt7F5XaVOJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSSAPYpOXPHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using BERT\n",
        "#pip install transformers torch\n"
      ],
      "metadata": {
        "id": "TGzJO0euWpkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Define the acronym mapping\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def replace_acronyms_bert(text, acronym_map):\n",
        "    words = text.split()\n",
        "    words = [word.strip(\".\").strip(\",\") for word in words]\n",
        "    masked_words = [f\"[MASK]\" if word in acronym_map else word for word in words]\n",
        "\n",
        "    # Generate predictions for masked words\n",
        "    inputs = tokenizer(\" \".join(masked_words), return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predictions[0])\n",
        "\n",
        "    # Replace acronyms with predicted tokens\n",
        "    replaced_text = \" \".join([acronym_map.get(word, word) for word in words])\n",
        "\n",
        "    return replaced_text\n",
        "\n",
        "# Example usage\n",
        "text = \"The CD for the DI is set for next week, and the GP needs to review the RF and DDI\"\n",
        "print(replace_acronyms_bert(text, acronym_map))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5lLKwAaWpnG",
        "outputId": "69908b32-abfb-418a-9f0b-63e5b8f62dc2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Closing Date for the Direct Investment is set for next week and the General Partner needs to review the Related Funds and Discretionary Direct Investment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kfR_1cTYahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using bert"
      ],
      "metadata": {
        "id": "ESIKL_DtYakI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Define your acronym map\n",
        "acronym_map = {\n",
        "    \"CD\": \"Closing Date\",\n",
        "    \"IP\": \"Investment Period\",\n",
        "    \"DI\": \"Direct Investment\",\n",
        "    \"AFF\": \"Affiliate\",\n",
        "    \"GP\": \"General Partner\",\n",
        "    \"LI\": \"Lead Investor\",\n",
        "    \"SIF\": \"Secondary Investee Fund\",\n",
        "    \"IF\": \"Investee Fund\",\n",
        "    \"GPCS\": \"GP's Capital Subscriptions\",\n",
        "    \"RF\": \"Related Funds\",\n",
        "    \"RFM\": \"Related Firms\",\n",
        "    \"PIF\": \"Primary Investee Fund\",\n",
        "    \"DDI\": \"Discretionary Direct Investment\"\n",
        "}\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a pipeline for Named Entity Recognition (NER) using BERT\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def replace_acronyms(text):\n",
        "    # Tokenize and get predictions\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs).logits\n",
        "\n",
        "    # Get the predicted token classes\n",
        "    predictions = torch.argmax(outputs, dim=2).squeeze().tolist()\n",
        "\n",
        "    # Create a dictionary of token to prediction\n",
        "    token_predictions = dict(zip(tokens, predictions))\n",
        "\n",
        "    # Replace acronyms based on context\n",
        "    def replace(match):\n",
        "        acronym = match.group(0)\n",
        "        if acronym in acronym_map:\n",
        "            return acronym_map[acronym]\n",
        "        return acronym\n",
        "\n",
        "    # Use regex to find acronyms in the text\n",
        "    acronym_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(key) for key in acronym_map.keys()) + r')\\b')\n",
        "    result_text = acronym_pattern.sub(replace, text)\n",
        "\n",
        "    return result_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"The GP and LI discussed the CD during the IP phase.\"\n",
        "input_text = \"The CD for the DI is set for next week, and the GP needs to review the RF and DDI.\"\n",
        "result = replace_acronyms(input_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtGq6H_VYanB",
        "outputId": "462f7276-c60f-47c3-dbb2-9cdd85dae1df"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Closing Date for the Direct Investment is set for next week, and the General Partner needs to review the Related Funds and Discretionary Direct Investment.These all information stored in Closing Date memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppAH7MvpZP8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Without acronym map"
      ],
      "metadata": {
        "id": "-T9FcVY1ZQBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def detect_acronyms(text):\n",
        "    doc = nlp(text)\n",
        "    acronyms = set()\n",
        "\n",
        "    for token in doc:\n",
        "        # Simple pattern to detect uppercase acronyms\n",
        "        if token.text.isupper() and len(token.text) > 1:\n",
        "            acronyms.add(token.text)\n",
        "\n",
        "    return acronyms\n",
        "\n",
        "# Example usage\n",
        "input_text = \"The GP and LI met to discuss the CD for the IP.\"\n",
        "acronyms = detect_acronyms(input_text)\n",
        "print(\"Identified Acronyms: \", acronyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BkTrkl4Z-gu",
        "outputId": "66a23a41-2b0c-40f0-b4b2-28cdc204d88b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified Acronyms:  {'LI', 'IP', 'CD', 'GP'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def replace_acronyms_with_context(text):\n",
        "    acronyms = detect_acronyms(text)\n",
        "\n",
        "    for acronym in acronyms:\n",
        "        # Create a masked version of the text\n",
        "        masked_text = text.replace(acronym, '[MASK]')\n",
        "\n",
        "        # Encode the text\n",
        "        input_ids = tokenizer.encode(masked_text, return_tensors='pt')\n",
        "        mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        # Predict the masked token\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids)\n",
        "            predictions = output.logits[0, mask_token_index].topk(1).indices.squeeze().tolist()\n",
        "\n",
        "        predicted_token = tokenizer.decode(predictions)\n",
        "\n",
        "        # Replace acronym with predicted expansion\n",
        "        text = text.replace(acronym, predicted_token)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"The GP and LI met to discuss the CD for the IP.\"\n",
        "replaced_text = replace_acronyms_with_context(input_text)\n",
        "print(\"Original Text: \", input_text)\n",
        "print(\"Replaced Text: \", replaced_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpn7V6LGaGpl",
        "outputId": "0ea28b4f-28dc-4ca8-d5b8-077eb2ad5ba4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  The GP and LI met to discuss the CD for the IP.\n",
            "Replaced Text:  The i and i p met to discuss the d e s i g n for the e v e n t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAYCPM5wbQVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75IrmqxAdlKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-j-6B')\n",
        "\n",
        "def generate_acronym_expansion(text, acronym):\n",
        "    prompt = f\"In the context of the following text: '{text}', what does the acronym '{acronym}' likely stand for?\"\n",
        "    result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "def replace_acronyms_with_context(text):\n",
        "    acronyms = detect_acronyms(text)\n",
        "    replaced_text = text\n",
        "\n",
        "    for acronym in acronyms:\n",
        "        expansion = generate_acronym_expansion(text, acronym)\n",
        "        # Extract only the relevant part of the generated text\n",
        "        expansion = expansion.split(\":\")[-1].strip()\n",
        "        replaced_text = replaced_text.replace(acronym, expansion)\n",
        "\n",
        "    return replaced_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"The GP and LI met to discuss the CD for the IP.\"\n",
        "print(\"Original Text: \", input_text)\n",
        "replaced_text = replace_acronyms_with_context(input_text)\n",
        "print(\"Replaced Text: \", replaced_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGDHugPTbQcE",
        "outputId": "da748469-512d-48ff-bad4-f59c175acb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCLhUYF7fpgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRUaXhH5fpjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n",
        "\n",
        "def infer_acronym_definition(text, acronym):\n",
        "    prompt = f\"In the context of the following text: '{text}', what does the acronym '{acronym}' most likely stand for?\"\n",
        "    result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Example usage\n",
        "input_text = \"The GP and LI met to discuss the CD for the IP.\"\n",
        "acronym = \"GP\"\n",
        "\n",
        "\n",
        "definition = infer_acronym_definition(input_text, acronym)\n",
        "print(f\"Definition for '{acronym}': {definition}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJ03TaKfqOf",
        "outputId": "76f3645c-6eb6-4be7-f838-afe568e3f374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    }
  ]
}