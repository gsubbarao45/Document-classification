{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "predicted =str([\"management fee\", \"Fund\", \"Primary Investment\", \"Revenue\"])\n",
    "\n",
    "predicted = ast.literal_eval(predicted)\n",
    "\n",
    "actual = ['management fee', 'Fund', 'Primary Investment', 'Revenue', \"Banking\",\"default\"]\n",
    "\n",
    "#You need to pass input file object here\n",
    "input_excel_file_name = \"sample.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def export_evalution_metrics_as_csv(input_excel_file_name, output_location, actual, predicted):\n",
    "    \n",
    "    input_excel_file_name = input_excel_file_name.split(\"/\")[-1]\n",
    "    input_excel_file_name = input_excel_file_name.split(\".\")[0]\n",
    "    # Binarize the predicted and actual key terms\n",
    "    all_terms = list(set(predicted + actual))\n",
    "    y_true = [1 if term in actual else 0 for term in all_terms]\n",
    "    y_pred = [1 if term in predicted else 0 for term in all_terms]\n",
    "\n",
    "    # Calculate precision, recall, f1 score, and accuracy\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Print the scores\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    output_location += f\"/{input_excel_file_name}_keytrems_evalution_metrics.csv\"\n",
    "\n",
    "    # Write the scores to a CSV file\n",
    "    with open(output_location, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Extracted file name\", input_excel_file_name])\n",
    "        writer.writerow([\"Actual keyterms :\"] + actual)\n",
    "        writer.writerow([\"Predicted keyterms :\"] + predicted)\n",
    "        writer.writerow([\"\", \"\"])\n",
    "        writer.writerow([\"Metric\", \"Value\"])\n",
    "        writer.writerow([\"Precision\", precision])\n",
    "        writer.writerow([\"Recall\", recall])\n",
    "        writer.writerow([\"F1 Score\", f1])\n",
    "        writer.writerow([\"Accuracy\", accuracy])\n",
    "\n",
    "    print(\"Metrics written to metrics.csv\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00\n",
      "Recall: 0.67\n",
      "F1 Score: 0.80\n",
      "Accuracy: 0.67\n",
      "Metrics written to metrics.csv\n"
     ]
    }
   ],
   "source": [
    "output_location = r\"C:\\Users\\venum\\Downloads\"\n",
    "\n",
    "export_evalution_metrics_as_csv(input_excel_file_name, output_location, actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00\n",
      "Recall: 0.67\n",
      "F1 Score: 0.80\n",
      "Accuracy: 0.67\n",
      "Metrics written to C:\\Users\\venum\\Downloads/sample_keyterms_evaluation_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def export_evalution_metrics_as_excel(input_excel_file_name, output_location, actual, predicted):\n",
    "    temp = input_excel_file_name\n",
    "    input_excel_file_name = input_excel_file_name.split(\"/\")[-1]\n",
    "    input_excel_file_name = input_excel_file_name.split(\".\")[0]\n",
    "    # Binarize the predicted and actual key terms\n",
    "    all_terms = list(set(predicted + actual))\n",
    "    y_true = [1 if term in actual else 0 for term in all_terms]\n",
    "    y_pred = [1 if term in predicted else 0 for term in all_terms]\n",
    "\n",
    "    # Calculate precision, recall, f1 score, and accuracy\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Print the scores\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    output_file_path = f\"{output_location}/{input_excel_file_name}_keyterms_evaluation_metrics.xlsx\"\n",
    "\n",
    "    # Create a workbook and add a worksheet\n",
    "    workbook = xlsxwriter.Workbook(output_file_path)\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    # Add a bold format to use for the headers\n",
    "    bold = workbook.add_format({'bold': True})\n",
    "\n",
    "    # Write the extracted file name\n",
    "    worksheet.write('A1', 'Extracted file name', bold)\n",
    "    worksheet.write('B1', temp)\n",
    "\n",
    "    # Write the actual key terms\n",
    "    worksheet.write('A2', 'Actual keyterms :', bold)\n",
    "    worksheet.write_row('B2', actual)\n",
    "\n",
    "    # Write the predicted key terms\n",
    "    worksheet.write('A3', 'Predicted keyterms :', bold)\n",
    "    worksheet.write_row('B3', predicted)\n",
    "\n",
    "    # Leave a blank row\n",
    "    worksheet.write('A4', '', bold)\n",
    "    worksheet.write('B4', '')\n",
    "\n",
    "    # Write the metrics headers\n",
    "    worksheet.write('A5', 'Metric', bold)\n",
    "    worksheet.write('B5', 'Value', bold)\n",
    "\n",
    "    # Write the metrics\n",
    "    metrics = [('Precision', precision), ('Recall', recall), ('F1 Score', f1), ('Accuracy', accuracy)]\n",
    "    row = 5\n",
    "    for metric, value in metrics:\n",
    "        worksheet.write(row, 0, metric)\n",
    "        worksheet.write(row, 1, value)\n",
    "        row += 1\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "    print(f\"Metrics written to {output_file_path}\")\n",
    "\n",
    "\n",
    "output_location = r\"C:\\Users\\venum\\Downloads\"\n",
    "\n",
    "export_evalution_metrics_as_excel(input_excel_file_name, output_location, actual, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
