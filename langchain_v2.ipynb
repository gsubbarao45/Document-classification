{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1069fce9e934db9a89bc14a3471837c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6067b8adfb4f4d9c875acec37cd64849",
              "IPY_MODEL_88b29eb9d502499aa70da8fdb26d8436",
              "IPY_MODEL_23f523f2397a4e819ba906210d45a691"
            ],
            "layout": "IPY_MODEL_64f412fe8190482288122de8f57fb9d2"
          }
        },
        "6067b8adfb4f4d9c875acec37cd64849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efb2d3c5f8f748b181d0536f160695d3",
            "placeholder": "​",
            "style": "IPY_MODEL_09d3c4a0dca2404bafc2552bd21120a7",
            "value": "Fetching 5 files: 100%"
          }
        },
        "88b29eb9d502499aa70da8fdb26d8436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be25cf42cbbc4f4898367fd1cc25c03e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73dbe7750def4092a338184f95c82da3",
            "value": 5
          }
        },
        "23f523f2397a4e819ba906210d45a691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f800ff3840a54a3cbf2f701d62491c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_cc1c941f66814cbb9839d32225bbca7f",
            "value": " 5/5 [00:02&lt;00:00,  1.44s/it]"
          }
        },
        "64f412fe8190482288122de8f57fb9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb2d3c5f8f748b181d0536f160695d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d3c4a0dca2404bafc2552bd21120a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be25cf42cbbc4f4898367fd1cc25c03e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73dbe7750def4092a338184f95c82da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f800ff3840a54a3cbf2f701d62491c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1c941f66814cbb9839d32225bbca7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e516bd5b1e504e34a3ce87b92a092114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2222b07218da4006babfb7171da5b4ca",
              "IPY_MODEL_140e1f35c3e64c26971b78c4e75aff0a",
              "IPY_MODEL_e90d1b1eda2346d7a7f77c258ce19870"
            ],
            "layout": "IPY_MODEL_b0daf348bd51405ba302e6e4b8c4323a"
          }
        },
        "2222b07218da4006babfb7171da5b4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96a3ca38383743919d364746544f9cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_67ae62987f504a72aae5d00e09d9bf3e",
            "value": "tokenizer.json: 100%"
          }
        },
        "140e1f35c3e64c26971b78c4e75aff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_354f4a4c71b34f7cb014c01b5c512b12",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_377a2228557440ff96c816f76628f8ce",
            "value": 711396
          }
        },
        "e90d1b1eda2346d7a7f77c258ce19870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c07c17f5788d43cb9c6c782dd2341ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_8d6f86ce1956474ea318b7f4a5e05440",
            "value": " 711k/711k [00:00&lt;00:00, 3.32MB/s]"
          }
        },
        "b0daf348bd51405ba302e6e4b8c4323a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a3ca38383743919d364746544f9cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ae62987f504a72aae5d00e09d9bf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "354f4a4c71b34f7cb014c01b5c512b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377a2228557440ff96c816f76628f8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c07c17f5788d43cb9c6c782dd2341ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6f86ce1956474ea318b7f4a5e05440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9773f36d82674c79890acc7d6c4af03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c97ea3c194445cc80d5e4e506754685",
              "IPY_MODEL_50bcc0a50d824b0e872ba9550b56f0e3",
              "IPY_MODEL_9e67e4146fa94458b085da12fabab9c1"
            ],
            "layout": "IPY_MODEL_cfd106cf14b24812a29fd4e271c5fa60"
          }
        },
        "5c97ea3c194445cc80d5e4e506754685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec1f0005ecf49d5a4f0ba084664274e",
            "placeholder": "​",
            "style": "IPY_MODEL_73da1153194248f8834c5758d6ab2554",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "50bcc0a50d824b0e872ba9550b56f0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159d24e8884a4d8cbba7e2e72c204f74",
            "max": 1242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f3c2853d1fe4894a0b3f0068018c066",
            "value": 1242
          }
        },
        "9e67e4146fa94458b085da12fabab9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_044aea42e0864684ad64eecbc8c55f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_78f873aff64f4201813bb688557b998f",
            "value": " 1.24k/1.24k [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "cfd106cf14b24812a29fd4e271c5fa60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec1f0005ecf49d5a4f0ba084664274e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73da1153194248f8834c5758d6ab2554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159d24e8884a4d8cbba7e2e72c204f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3c2853d1fe4894a0b3f0068018c066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "044aea42e0864684ad64eecbc8c55f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f873aff64f4201813bb688557b998f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c845b76c77584130ae4e6e3ed075e738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b4f9d7c77de42a88497939ed3a5a919",
              "IPY_MODEL_888dcb3602c44f7da3388584ecc1404c",
              "IPY_MODEL_4c4c8e3fc828429ead99de422046724c"
            ],
            "layout": "IPY_MODEL_586a7e0198c9432b96387a8ab7d92ff4"
          }
        },
        "8b4f9d7c77de42a88497939ed3a5a919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aea687509f64dadaf7e2c820d45e50e",
            "placeholder": "​",
            "style": "IPY_MODEL_64d0813edd924a54829adc8d86f4f3f0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "888dcb3602c44f7da3388584ecc1404c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c28e26ee6204af6b7a7868530b7946a",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3680627758b407996558e6d2f40ad41",
            "value": 695
          }
        },
        "4c4c8e3fc828429ead99de422046724c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a0ecae417d45a1aeab645f9e14207e",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3f0b1d06e644fb86a5e19ebb6175b0",
            "value": " 695/695 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "586a7e0198c9432b96387a8ab7d92ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aea687509f64dadaf7e2c820d45e50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d0813edd924a54829adc8d86f4f3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c28e26ee6204af6b7a7868530b7946a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3680627758b407996558e6d2f40ad41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a0ecae417d45a1aeab645f9e14207e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3f0b1d06e644fb86a5e19ebb6175b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8887b5d2aacf444abbe150814735327c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9a57d5b66ea44fea458adf4229a00b5",
              "IPY_MODEL_d1a90ff9aa08444188a56142a309f990",
              "IPY_MODEL_cf663225320341278bdd355d51bee786"
            ],
            "layout": "IPY_MODEL_510a4287650c421fbc086a64b144222a"
          }
        },
        "d9a57d5b66ea44fea458adf4229a00b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c0733864454c8996c763b086254fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_ad90060e8e894a769916f0b30fda7ee7",
            "value": "config.json: 100%"
          }
        },
        "d1a90ff9aa08444188a56142a309f990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d9f05f08874ae6a48d68f429b298ee",
            "max": 740,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cc2fa800d824d4e80737c936ff03f48",
            "value": 740
          }
        },
        "cf663225320341278bdd355d51bee786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89267f7e68aa460e91a7d48b5ba9093a",
            "placeholder": "​",
            "style": "IPY_MODEL_9b5d552704244faba3142df8cd4d1bf0",
            "value": " 740/740 [00:00&lt;00:00, 8.00kB/s]"
          }
        },
        "510a4287650c421fbc086a64b144222a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c0733864454c8996c763b086254fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad90060e8e894a769916f0b30fda7ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1d9f05f08874ae6a48d68f429b298ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc2fa800d824d4e80737c936ff03f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89267f7e68aa460e91a7d48b5ba9093a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5d552704244faba3142df8cd4d1bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30627dc2ed8e479ba79498228145d529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93220d7a5bcf46aab6921d56e38a76c0",
              "IPY_MODEL_a1a6b23aaea743f79aa784e7e936fc9f",
              "IPY_MODEL_fdcffc7a65b84c14a3f9631f348647a8"
            ],
            "layout": "IPY_MODEL_53d8d621b5414394a5117c7cfdee51f7"
          }
        },
        "93220d7a5bcf46aab6921d56e38a76c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_879ef5190bf84644971245b1a43c8d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_4deed6c6930b4fe3a6f59f20704b074c",
            "value": "model_optimized.onnx: 100%"
          }
        },
        "a1a6b23aaea743f79aa784e7e936fc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8910a6d6f5874cd99adc7c6b5ea4ad36",
            "max": 217824172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b76d93732fd14c0895a49e1785880783",
            "value": 217824172
          }
        },
        "fdcffc7a65b84c14a3f9631f348647a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4402883ff24c4d918b198ec4b5b37d54",
            "placeholder": "​",
            "style": "IPY_MODEL_4a4a22ec38b94f94bed3f1d685b54ab2",
            "value": " 218M/218M [00:01&lt;00:00, 122MB/s]"
          }
        },
        "53d8d621b5414394a5117c7cfdee51f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879ef5190bf84644971245b1a43c8d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4deed6c6930b4fe3a6f59f20704b074c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8910a6d6f5874cd99adc7c6b5ea4ad36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76d93732fd14c0895a49e1785880783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4402883ff24c4d918b198ec4b5b37d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4a22ec38b94f94bed3f1d685b54ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2001590e83e441338fa6ab39900910a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b56465047f324a41b672c2cbc3fc9abf",
              "IPY_MODEL_180f77d723864fecaae9f44038beda59",
              "IPY_MODEL_2c6b2c125c584d868130e675847fcf7c"
            ],
            "layout": "IPY_MODEL_dcc4f5a5a2cd44fb822a2203f1ac4bd4"
          }
        },
        "b56465047f324a41b672c2cbc3fc9abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_143f09b783ba4af7b87a87688814ffc7",
            "placeholder": "​",
            "style": "IPY_MODEL_d5d654fefae64341a0902cb823578b0e",
            "value": "Fetching 5 files: 100%"
          }
        },
        "180f77d723864fecaae9f44038beda59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2636cd6f393e41eea314cc6c5569eed1",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ab170d8365f4f818d15125a81499ab3",
            "value": 5
          }
        },
        "2c6b2c125c584d868130e675847fcf7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7e0e0e7c2340f2810e4d052514b55e",
            "placeholder": "​",
            "style": "IPY_MODEL_35fd7789f1f34e20b5166976f9fbf752",
            "value": " 5/5 [00:03&lt;00:00,  1.99s/it]"
          }
        },
        "dcc4f5a5a2cd44fb822a2203f1ac4bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "143f09b783ba4af7b87a87688814ffc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d654fefae64341a0902cb823578b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2636cd6f393e41eea314cc6c5569eed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab170d8365f4f818d15125a81499ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd7e0e0e7c2340f2810e4d052514b55e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fd7789f1f34e20b5166976f9fbf752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9e39c64bf80418c96cad5a01b513863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42225bed3352494db2d1eefc418e7218",
              "IPY_MODEL_27288aa967f74a96813061deb0f05e0a",
              "IPY_MODEL_35761e232dd44dd78e1784d8d3ac789c"
            ],
            "layout": "IPY_MODEL_da2c95a227154ac0819e0a1a55072eeb"
          }
        },
        "42225bed3352494db2d1eefc418e7218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f1e5dff20a4f819de25effaa7ba643",
            "placeholder": "​",
            "style": "IPY_MODEL_1a04eaa4ddea4f3b9491e64c0497caa3",
            "value": "config.json: 100%"
          }
        },
        "27288aa967f74a96813061deb0f05e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cf024361c414fd0959d0bc277efe70f",
            "max": 650,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f5a93f308f94952b868d3d663c7608d",
            "value": 650
          }
        },
        "35761e232dd44dd78e1784d8d3ac789c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d41bf9989ecb4d5390b7c8689125d559",
            "placeholder": "​",
            "style": "IPY_MODEL_7d08618d1b3144f8a6466e197ebd13a0",
            "value": " 650/650 [00:00&lt;00:00, 8.18kB/s]"
          }
        },
        "da2c95a227154ac0819e0a1a55072eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f1e5dff20a4f819de25effaa7ba643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a04eaa4ddea4f3b9491e64c0497caa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cf024361c414fd0959d0bc277efe70f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5a93f308f94952b868d3d663c7608d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d41bf9989ecb4d5390b7c8689125d559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d08618d1b3144f8a6466e197ebd13a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78aa8bad210a487c95be0322bbde14ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc12fba260c140f49996117198010b0a",
              "IPY_MODEL_6584287005d04856b1f6da458b1bad3c",
              "IPY_MODEL_888da9be3d8048c2ada5a181ffc733ed"
            ],
            "layout": "IPY_MODEL_eeb5144a5e174eaca2448637e7e60a76"
          }
        },
        "bc12fba260c140f49996117198010b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6877040fc9c41998c70a35e549c1ce3",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6bbac3688d4cd9b25c90c809883afc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6584287005d04856b1f6da458b1bad3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c277723a8b4c63bfa552c94d17d41d",
            "max": 1433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d0e16e49d0b401780266e42db655ef8",
            "value": 1433
          }
        },
        "888da9be3d8048c2ada5a181ffc733ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6fdd0c1342e4a67b90f41394e6ba39e",
            "placeholder": "​",
            "style": "IPY_MODEL_ec18bd7d5d0e4b6b92551f96c800c96c",
            "value": " 1.43k/1.43k [00:00&lt;00:00, 12.9kB/s]"
          }
        },
        "eeb5144a5e174eaca2448637e7e60a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6877040fc9c41998c70a35e549c1ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6bbac3688d4cd9b25c90c809883afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29c277723a8b4c63bfa552c94d17d41d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d0e16e49d0b401780266e42db655ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6fdd0c1342e4a67b90f41394e6ba39e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec18bd7d5d0e4b6b92551f96c800c96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cfa564fee914a90a9741097f1dfa372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ca25e29e7a945e2a29ffbac11acaa71",
              "IPY_MODEL_4b4b4a707c0c4711bacfbd517ca79d35",
              "IPY_MODEL_d705ca96460243acbcd036ae67061db4"
            ],
            "layout": "IPY_MODEL_06018d2b98414b92b8e0a16557afe36b"
          }
        },
        "2ca25e29e7a945e2a29ffbac11acaa71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309a9d196bb4432697f5901f2d4fb85c",
            "placeholder": "​",
            "style": "IPY_MODEL_ae20dd13b94b4c248f97d4f6309114f4",
            "value": "tokenizer.json: 100%"
          }
        },
        "4b4b4a707c0c4711bacfbd517ca79d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d2c556a89542beb5d74a43c529fa6b",
            "max": 711661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58678a2dc8f1403b8fc7a6016fc07d33",
            "value": 711661
          }
        },
        "d705ca96460243acbcd036ae67061db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e87963281747ecae932e21aa29484f",
            "placeholder": "​",
            "style": "IPY_MODEL_5677087e133741269ff98459ca6e2693",
            "value": " 712k/712k [00:01&lt;00:00, 470kB/s]"
          }
        },
        "06018d2b98414b92b8e0a16557afe36b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309a9d196bb4432697f5901f2d4fb85c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae20dd13b94b4c248f97d4f6309114f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89d2c556a89542beb5d74a43c529fa6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58678a2dc8f1403b8fc7a6016fc07d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4e87963281747ecae932e21aa29484f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5677087e133741269ff98459ca6e2693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d3b926ffe43428ab02cf485b8f47911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5dbc75e76f3429889569131ee8a2a84",
              "IPY_MODEL_3bc4a79362724414b60b81f779715ed2",
              "IPY_MODEL_d78137fac74b47eb92ff1880bf08c8e1"
            ],
            "layout": "IPY_MODEL_ca97c78b2db14024abb0a35b55c9817f"
          }
        },
        "e5dbc75e76f3429889569131ee8a2a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cec1ef8545144dfa917836d79c5acb08",
            "placeholder": "​",
            "style": "IPY_MODEL_acb2e84dbc404e9cbad68cddec375b42",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "3bc4a79362724414b60b81f779715ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa19fdf106a4e2ea53c18b5ba385c16",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7be4571d6584befa0cc284c4c3a4eb9",
            "value": 695
          }
        },
        "d78137fac74b47eb92ff1880bf08c8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9498e71ab68e408982c87a6ec2d495e9",
            "placeholder": "​",
            "style": "IPY_MODEL_6774b980d08349c4a95618dba0765825",
            "value": " 695/695 [00:00&lt;00:00, 5.01kB/s]"
          }
        },
        "ca97c78b2db14024abb0a35b55c9817f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec1ef8545144dfa917836d79c5acb08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb2e84dbc404e9cbad68cddec375b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aa19fdf106a4e2ea53c18b5ba385c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7be4571d6584befa0cc284c4c3a4eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9498e71ab68e408982c87a6ec2d495e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6774b980d08349c4a95618dba0765825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99c44087be8e444e919129a792c75208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f174128f0ea341b18fc9ccc23eb34c15",
              "IPY_MODEL_82934c9796ea41fab3a3292ad1a3e76c",
              "IPY_MODEL_a85e723b6b614c6b9b4de3e2ae6ebf6e"
            ],
            "layout": "IPY_MODEL_4d296eee6d4b40b3aeffa81d2d7f2ff3"
          }
        },
        "f174128f0ea341b18fc9ccc23eb34c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e74173f71d4dc59d850a45ca8dae98",
            "placeholder": "​",
            "style": "IPY_MODEL_465880c8b54549a3a89d79df215f6be8",
            "value": "model.onnx: 100%"
          }
        },
        "82934c9796ea41fab3a3292ad1a3e76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc855e95f784c1988ecaa74387c2a23",
            "max": 90387630,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58c2c3479a444ec8829a641ea0d0742b",
            "value": 90387630
          }
        },
        "a85e723b6b614c6b9b4de3e2ae6ebf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c7eba7890514e3b9120bcb85bc86d90",
            "placeholder": "​",
            "style": "IPY_MODEL_6557a684698844cf8d323c0fc78c5521",
            "value": " 90.4M/90.4M [00:02&lt;00:00, 67.1MB/s]"
          }
        },
        "4d296eee6d4b40b3aeffa81d2d7f2ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68e74173f71d4dc59d850a45ca8dae98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465880c8b54549a3a89d79df215f6be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cc855e95f784c1988ecaa74387c2a23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c2c3479a444ec8829a641ea0d0742b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c7eba7890514e3b9120bcb85bc86d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6557a684698844cf8d323c0fc78c5521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472b8dde15854859bbc4af591e858084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05e1b47e3482441ba3d50693e6a665dd",
              "IPY_MODEL_4c819ea9d39143ea842397ac94fc43c0",
              "IPY_MODEL_5322d2de574448798c705ae2c36d5c3d"
            ],
            "layout": "IPY_MODEL_630f701802d645d180b74cab4c086fd8"
          }
        },
        "05e1b47e3482441ba3d50693e6a665dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea258c0abed4bc3a186505044170ffa",
            "placeholder": "​",
            "style": "IPY_MODEL_e38206659c434e68b805f64a8222b8d5",
            "value": "modules.json: 100%"
          }
        },
        "4c819ea9d39143ea842397ac94fc43c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f7e570672ea4dc880b3afdbaab03873",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf933b37a3694da5b7ec907d45c26711",
            "value": 349
          }
        },
        "5322d2de574448798c705ae2c36d5c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590a9ac405c449168b9b7021582cf4f6",
            "placeholder": "​",
            "style": "IPY_MODEL_f83e6d22d7c043aa9c1efc47458b0f75",
            "value": " 349/349 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "630f701802d645d180b74cab4c086fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea258c0abed4bc3a186505044170ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38206659c434e68b805f64a8222b8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f7e570672ea4dc880b3afdbaab03873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf933b37a3694da5b7ec907d45c26711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "590a9ac405c449168b9b7021582cf4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83e6d22d7c043aa9c1efc47458b0f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e5f05dc94b46c6ac5a616f38549a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee5f29a6f7f244db984fac68c95ef7c9",
              "IPY_MODEL_12be3ecf41c344a79acbbce16acb7dd7",
              "IPY_MODEL_97070a15fd43444c9f9fcc64df40a995"
            ],
            "layout": "IPY_MODEL_7d04c62dcb7e42808c4363940806a9f8"
          }
        },
        "ee5f29a6f7f244db984fac68c95ef7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6bf1e44abd4b6396c5d15625169d59",
            "placeholder": "​",
            "style": "IPY_MODEL_26569987c1bf4dc8b33a5682cd9954ca",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "12be3ecf41c344a79acbbce16acb7dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce3a25972c904aa7b4d7ef35f29f61af",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03cfa33af4484441b5b3ea24d55659d3",
            "value": 116
          }
        },
        "97070a15fd43444c9f9fcc64df40a995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_344758b77bfd4ec7ba8b44ab59cbe51f",
            "placeholder": "​",
            "style": "IPY_MODEL_5886be9828b34a04be9cd4e92268ca45",
            "value": " 116/116 [00:00&lt;00:00, 6.69kB/s]"
          }
        },
        "7d04c62dcb7e42808c4363940806a9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6bf1e44abd4b6396c5d15625169d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26569987c1bf4dc8b33a5682cd9954ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce3a25972c904aa7b4d7ef35f29f61af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03cfa33af4484441b5b3ea24d55659d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "344758b77bfd4ec7ba8b44ab59cbe51f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5886be9828b34a04be9cd4e92268ca45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0586b78bed3471792a520fa16ee1f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a53ca2675104eab8e4e61a59d008f8a",
              "IPY_MODEL_1ce822c8dad14a7d8d51fda8a97ab755",
              "IPY_MODEL_0c3e269ccc71452ab21334c62f25a60d"
            ],
            "layout": "IPY_MODEL_704bc1afb7334972a3966d6657ce3de8"
          }
        },
        "0a53ca2675104eab8e4e61a59d008f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aea488488c94c8b86c2ed6790b85987",
            "placeholder": "​",
            "style": "IPY_MODEL_bdf682d4da9640c4a7ec542bbd2c83da",
            "value": "README.md: 100%"
          }
        },
        "1ce822c8dad14a7d8d51fda8a97ab755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd7b3c5794040b9a2cc12a2af38378c",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92c631f688d64f12a188f3a3ef625a7c",
            "value": 10621
          }
        },
        "0c3e269ccc71452ab21334c62f25a60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a0706645eb24d0681258ccd8ed641b0",
            "placeholder": "​",
            "style": "IPY_MODEL_64ae95fcc5f54b72bfdaef1847f77a54",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 658kB/s]"
          }
        },
        "704bc1afb7334972a3966d6657ce3de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aea488488c94c8b86c2ed6790b85987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf682d4da9640c4a7ec542bbd2c83da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd7b3c5794040b9a2cc12a2af38378c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c631f688d64f12a188f3a3ef625a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a0706645eb24d0681258ccd8ed641b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ae95fcc5f54b72bfdaef1847f77a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad98a20f9020476b9dbf23386be43749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28afe4da2e7c4315a7fe7f803cb360b3",
              "IPY_MODEL_058ceac08b6c4b9f9a3580a45092236e",
              "IPY_MODEL_3403815bb876441785001480b387c19c"
            ],
            "layout": "IPY_MODEL_4e703aeb1c6b41a9961c2c3e9c2d38be"
          }
        },
        "28afe4da2e7c4315a7fe7f803cb360b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efc646cec3bc44d69f87692ccb27a062",
            "placeholder": "​",
            "style": "IPY_MODEL_11692cd012a74363ac61429ed563e86b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "058ceac08b6c4b9f9a3580a45092236e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e143594d2a6b45fcb6705c3a7361a8ec",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0e0e19b729d45178d39608033e37e1d",
            "value": 53
          }
        },
        "3403815bb876441785001480b387c19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769b0421883742308b2a499d7a350b95",
            "placeholder": "​",
            "style": "IPY_MODEL_030b5491929f449a984370d26122a9fc",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.99kB/s]"
          }
        },
        "4e703aeb1c6b41a9961c2c3e9c2d38be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efc646cec3bc44d69f87692ccb27a062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11692cd012a74363ac61429ed563e86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e143594d2a6b45fcb6705c3a7361a8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e0e19b729d45178d39608033e37e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "769b0421883742308b2a499d7a350b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030b5491929f449a984370d26122a9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd537fe15bf546f1b062db96f4ae0d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3c68e6ae578406a88eb1e002c0fb76b",
              "IPY_MODEL_2a63691ede2a4132883807253c82564c",
              "IPY_MODEL_664feaee5538452c9ed0b84614271ac6"
            ],
            "layout": "IPY_MODEL_183146562aea4bb1b200ecfffd45c266"
          }
        },
        "e3c68e6ae578406a88eb1e002c0fb76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5c9176b7f2140788aa53a9700c7a73d",
            "placeholder": "​",
            "style": "IPY_MODEL_6472c94fc12b4453a5bb9bd19295e28f",
            "value": "config.json: 100%"
          }
        },
        "2a63691ede2a4132883807253c82564c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9136940003044495a4e8cb42e1b73d57",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d67abdbe00024c44a2d440e5a4eaf218",
            "value": 571
          }
        },
        "664feaee5538452c9ed0b84614271ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414518164a7d45879f53681b089a1d37",
            "placeholder": "​",
            "style": "IPY_MODEL_61927b33724245aca9d81d8d37eaf21a",
            "value": " 571/571 [00:00&lt;00:00, 25.0kB/s]"
          }
        },
        "183146562aea4bb1b200ecfffd45c266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5c9176b7f2140788aa53a9700c7a73d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6472c94fc12b4453a5bb9bd19295e28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9136940003044495a4e8cb42e1b73d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d67abdbe00024c44a2d440e5a4eaf218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "414518164a7d45879f53681b089a1d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61927b33724245aca9d81d8d37eaf21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "599873d761b841e98e87f34667b78109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70c59365738d49588aab9eb3fa99e9ce",
              "IPY_MODEL_74e6894faedd4185a72fe556d3b61e10",
              "IPY_MODEL_7e43c4893a3a403d857929c1f5c51dc7"
            ],
            "layout": "IPY_MODEL_2c8c1fb42c1148fa9ed89d734ab2315f"
          }
        },
        "70c59365738d49588aab9eb3fa99e9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4cf3cf58f3c419f8b18d5153b8abcc6",
            "placeholder": "​",
            "style": "IPY_MODEL_d4573d23b8f54d1c9f9ea9fd7822c59a",
            "value": "model.safetensors: 100%"
          }
        },
        "74e6894faedd4185a72fe556d3b61e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8a290a2e3e14a1587e904c50fcdec67",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1e65b1fc9c849078fd0510b68c53127",
            "value": 437971872
          }
        },
        "7e43c4893a3a403d857929c1f5c51dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00513f7ca6e4245879d2b59d99820f1",
            "placeholder": "​",
            "style": "IPY_MODEL_b2a027798d404867936004dc7d7696d1",
            "value": " 438M/438M [00:02&lt;00:00, 141MB/s]"
          }
        },
        "2c8c1fb42c1148fa9ed89d734ab2315f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4cf3cf58f3c419f8b18d5153b8abcc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4573d23b8f54d1c9f9ea9fd7822c59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8a290a2e3e14a1587e904c50fcdec67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e65b1fc9c849078fd0510b68c53127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b00513f7ca6e4245879d2b59d99820f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2a027798d404867936004dc7d7696d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e831c0576fa4e189f9951773cf2d3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1987c99a028d420eb132db9840881646",
              "IPY_MODEL_941fa65d1e3a43b7b868b3f6a9c4ea65",
              "IPY_MODEL_2806c86010364395b82e19ae86bab50e"
            ],
            "layout": "IPY_MODEL_c40f60633e424ebf8bbf836f3cea65c7"
          }
        },
        "1987c99a028d420eb132db9840881646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3cd4653eef7416982550345b77447aa",
            "placeholder": "​",
            "style": "IPY_MODEL_adba2fd0e9794dfc82fad143f5a9dc02",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "941fa65d1e3a43b7b868b3f6a9c4ea65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85d8e43f72844cea254e0fc52e2acd5",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b12948adf89f41448e3ee38b8386cc7a",
            "value": 363
          }
        },
        "2806c86010364395b82e19ae86bab50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92b18118fc8248908111cde8454e745e",
            "placeholder": "​",
            "style": "IPY_MODEL_0387fb9639604a8a93babef107e0e18b",
            "value": " 363/363 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "c40f60633e424ebf8bbf836f3cea65c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3cd4653eef7416982550345b77447aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adba2fd0e9794dfc82fad143f5a9dc02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85d8e43f72844cea254e0fc52e2acd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b12948adf89f41448e3ee38b8386cc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92b18118fc8248908111cde8454e745e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0387fb9639604a8a93babef107e0e18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb00ed4c2600460c99d2160f87563b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ab504b7d11b4433953a6cb58e3b307b",
              "IPY_MODEL_04c99555278a42378162badf62208a0d",
              "IPY_MODEL_318d894653154a90ae303c62efe399e5"
            ],
            "layout": "IPY_MODEL_65ae9e9e35614fbaaa814e48eb1f79af"
          }
        },
        "4ab504b7d11b4433953a6cb58e3b307b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5307120e5a4c402dbf60b9d9c8a5278b",
            "placeholder": "​",
            "style": "IPY_MODEL_e4762b6e7efc4bc3b41982356714e3af",
            "value": "vocab.txt: 100%"
          }
        },
        "04c99555278a42378162badf62208a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59345fc3a9e474ca0aec019e2f22b64",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cb07fd811764444909d53a45e11461d",
            "value": 231536
          }
        },
        "318d894653154a90ae303c62efe399e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9036e855a364f75b4b4f20e3df25349",
            "placeholder": "​",
            "style": "IPY_MODEL_14888b09eba248d39f6d3fb5e9232c4e",
            "value": " 232k/232k [00:00&lt;00:00, 1.73MB/s]"
          }
        },
        "65ae9e9e35614fbaaa814e48eb1f79af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5307120e5a4c402dbf60b9d9c8a5278b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4762b6e7efc4bc3b41982356714e3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59345fc3a9e474ca0aec019e2f22b64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb07fd811764444909d53a45e11461d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9036e855a364f75b4b4f20e3df25349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14888b09eba248d39f6d3fb5e9232c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f4957d5ebf4f4290462facae7b6786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4c717eb4b49497ab7cdbd41a20a2e68",
              "IPY_MODEL_980737f5f9cc4ca7a70cc41ed04db7db",
              "IPY_MODEL_dc1687355e9140109d14000f7d1f14a4"
            ],
            "layout": "IPY_MODEL_415f38c7eae94373b921b0fc4d13a918"
          }
        },
        "f4c717eb4b49497ab7cdbd41a20a2e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a07624270c4e03974cf60d605a298f",
            "placeholder": "​",
            "style": "IPY_MODEL_e7724ac974ba481a948aeea2369761ec",
            "value": "tokenizer.json: 100%"
          }
        },
        "980737f5f9cc4ca7a70cc41ed04db7db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_181ca6c878b74c0ab55b1f6c5437ae6f",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f47ee2e85cca4d839e76477ac6483162",
            "value": 466021
          }
        },
        "dc1687355e9140109d14000f7d1f14a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc8431ce2200434d9317e103e8d8eb02",
            "placeholder": "​",
            "style": "IPY_MODEL_905f695f77924919a11453c9c6d7d779",
            "value": " 466k/466k [00:00&lt;00:00, 2.51MB/s]"
          }
        },
        "415f38c7eae94373b921b0fc4d13a918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7a07624270c4e03974cf60d605a298f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7724ac974ba481a948aeea2369761ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "181ca6c878b74c0ab55b1f6c5437ae6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47ee2e85cca4d839e76477ac6483162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc8431ce2200434d9317e103e8d8eb02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "905f695f77924919a11453c9c6d7d779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aab2fefe8344ff6aed2d93906e19f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53b80488fe9c40aa932fde7c7a2bb03c",
              "IPY_MODEL_cd75a120dae7480faaaa37b8720b76ab",
              "IPY_MODEL_9b12d605c3ee47c6b6c7955644625e5e"
            ],
            "layout": "IPY_MODEL_f5b8537fc9554e5e91deca0aef0a8f21"
          }
        },
        "53b80488fe9c40aa932fde7c7a2bb03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7739e316e7dc439396c0361b6a6ee538",
            "placeholder": "​",
            "style": "IPY_MODEL_c4383ab8e9c347db958b21668c94cc72",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cd75a120dae7480faaaa37b8720b76ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6212407dc2ac47faaab77f3a7d6248b6",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0470176611b240f5891995eef9535c54",
            "value": 239
          }
        },
        "9b12d605c3ee47c6b6c7955644625e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d14061639e2b47bfa5ead43ba1e24b6f",
            "placeholder": "​",
            "style": "IPY_MODEL_2325175244a24fe48f5addf2712b2445",
            "value": " 239/239 [00:00&lt;00:00, 10.0kB/s]"
          }
        },
        "f5b8537fc9554e5e91deca0aef0a8f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7739e316e7dc439396c0361b6a6ee538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4383ab8e9c347db958b21668c94cc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6212407dc2ac47faaab77f3a7d6248b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0470176611b240f5891995eef9535c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d14061639e2b47bfa5ead43ba1e24b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2325175244a24fe48f5addf2712b2445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e402c02948d945a3bd5af1f5ac63cbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54e3e12146104de9a00ee1256e56c0ac",
              "IPY_MODEL_b727af3c91a342978a1d882b16eaf298",
              "IPY_MODEL_c2586d98dc3e49f6a67e15391d0f0afa"
            ],
            "layout": "IPY_MODEL_51103dd4723b49908b7968d6b9a92ee5"
          }
        },
        "54e3e12146104de9a00ee1256e56c0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_217f50140210440d927fcd2a82b26963",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c73348262946ce914a7a75cc44f7fc",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "b727af3c91a342978a1d882b16eaf298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f856bcd846e418e897691c378e0b837",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffd43af43ea64b7cac2598f628019650",
            "value": 190
          }
        },
        "c2586d98dc3e49f6a67e15391d0f0afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6813829c0ed46fba16d53ec0988d7d2",
            "placeholder": "​",
            "style": "IPY_MODEL_8c27c24f0dfd4be28da66eb4d3e25225",
            "value": " 190/190 [00:00&lt;00:00, 9.09kB/s]"
          }
        },
        "51103dd4723b49908b7968d6b9a92ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217f50140210440d927fcd2a82b26963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c73348262946ce914a7a75cc44f7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f856bcd846e418e897691c378e0b837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd43af43ea64b7cac2598f628019650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6813829c0ed46fba16d53ec0988d7d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c27c24f0dfd4be28da66eb4d3e25225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-pibYO7zN6v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas chromadb langchain-groq fastembed pypdf openai"
      ],
      "metadata": {
        "id": "dOi-pEn8zR_p"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R44fyTUXq3SM",
        "outputId": "a6b8a503-a1e1-4162-b2ea-bcce23d7626d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, SpacyTextSplitter, NLTKTextSplitter, split_text_on_tokens, SentenceTransformersTokenTextSplitter, LatexTextSplitter\n",
        "from langchain.text_splitter import PythonCodeTextSplitter, KonlpyTextSplitter, ElementType\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import torch\n",
        "\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "wM-2284fzyP_"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TextChunker:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_text_from_pdf(self, path = \"/\"):\n",
        "        text = \"\"\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents = loader.load()\n",
        "        for doc in documents:\n",
        "            text = text + \"\\n\" + str(doc.page_content)\n",
        "        return text,documents\n",
        "\n",
        "    def char_count_chunking_with_overlap(self, text, chunk_size=200, chunk_overlap=50, splitter_type = \"CharacterTextSplitter\"):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "    def char_count_chunking_with_nonoveralp(self, text, chunk_size=200, splitter_type = \"CharacterTextSplitter\"):\n",
        "       # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter clas\n",
        "           text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    def char_count_chunking_with_custom_delimiter(self, text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",splitter_type = \"CharacterTextSplitter\"):\n",
        "        # Instantiate the CharacterTextSplitter class\n",
        "        text_splitter = CharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        if splitter_type ==  \"RecursiveCharacterTextSplitter\":# Instantiate the RecursiveCharacterTextSplitter class\n",
        "           text_splitter = RecursiveCharacterTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"NLTKTextSplitter\":\n",
        "           text_splitter = NLTKTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SpacyTextSplitter\":\n",
        "           text_splitter = SpacyTextSplitter(separator=delimiter, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"SentenceTransformersTokenTextSplitter\":\n",
        "           text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"LatexTextSplitter\":\n",
        "           text_splitter = LatexTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"PythonCodeTextSplitter\":\n",
        "           text_splitter = PythonCodeTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"KonlpyTextSplitter\":\n",
        "           text_splitter = KonlpyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "        # Create documents using the text splitter\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "\n",
        "    def semantic_section_chunking(self, text , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\"):\n",
        "        embed_model = FastEmbedEmbeddings(model_name = text_embedding_model_name)\n",
        "        semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=breakpoint_threshold_type)\n",
        "        semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
        "        return semantic_chunks\n",
        "\n",
        "\n",
        "\n",
        "#Usage\n",
        "pdf_path =r\"/content/drive/MyDrive/chunking/ds (1).pdf\"\n",
        "\n",
        "text_chunker = TextChunker()\n",
        "text, documents  = text_chunker.extract_text_from_pdf(path = pdf_path )"
      ],
      "metadata": {
        "id": "xNCgyIqwzrvA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3rqsHdhsmwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"PythonCodeTextSplitter\")\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"KonlpyTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Zcna-HqsC5",
        "outputId": "b2591fec-fca0-4b00-cf6a-1b293f65a3c1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences Gabriel Peyr´ e CNRS & DMA ´Ecole Normale Sup´ erieure gabriel.peyre @ens .fr https: //mathematical-tours .github .io www.numerical-tours .com August 14, 2019 2 Chapter 1 Optimal Transport 1.1 Radon Measures Measures. We will interchangeably the term histogram or probability vector for any element a∈ Σnthat belongs to the probability simplex Σndef.={ a∈Rn + ;n∑ i=1ai= 1} . A discrete measure with weights aand locations x1,... ,xn ∈X reads α =n∑ i=1ai δxi (1.1) whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location x. Such as measure describes a probability measure if, additionally, a∈ Σn, and more generally a positive measure if each of the “weights” described in vector ais positive itself. Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous “objects” within the same framework. Such objects only need to be modelled as measures. This corresponds to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating) it against continuous functions, denoted f∈C (X). Integration of f∈C (X) against a discrete measure αcomputes a sum ∫ Xf(x )dα (x) =n∑ i=1aif (xi). More general measures, for instance on X=Rd (whered ∈N ∗is the dimension), can have a density dα (x) =ρα (x )dxw .r .t. the Lebesgue measure, often denoted ρα =dα dx, which means that ∀h ∈C (Rd),∫ Rdh(x )dα (x) =∫ Rdh(x)ρα (x )dx. An arbitrary measure α ∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by the fact that it can be integrated agains any continuous function f∈C (X) and obtain∫ Xf(x )dα (x) ∈R. IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity. Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+ (X) the set of all positive measures on X. The set of probability measures is denoted M1 + (X), which means that anyα ∈M1 + (X) is positive, and that α (X) =∫ Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent classes of measures, beyond histograms, considered in this work. 3 Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2 Figure 1.1: Schematic display of discrete distributions α= ∑n i=1ai δxi (red corresponds to empirical uniform distribution ai= 1/n, and blue to arbitrary distributions) and densities d α (x) =ρα (x )dx (in violet), in both 1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai) and in 2-D using point clouds (radius equal to ai). Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator T♯ :M (X) →M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the positions of all the points in the support of the measure T♯ αdef.=∑ iaiδT (xi). For more general measures, for instance for those with a density, the notion of push-forward plays a funda- mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow. Deﬁnition 1 (Push-forward) .ForT :X → Y , the push forward measure β =T♯α∈ M (Y )of some α ∈M (X )reads ∀h ∈C (Y),∫ Yh(y )dβ (y) =∫ Xh(T (x ))dα (x). (1.2) Equivalently, for any measurable set B⊂Y, one has β (B) =α( {x ∈X ;T (x) ∈B}). (1.3) Note thatT♯preserves positivity and total mass, so that if α ∈M1 + (X )thenT♯α ∈M1 + (Y). Intuitively, a measurable map T:X →Y , can be interpreted as a function “moving” a single point from a measurable space to another. The more general extension T♯can now “move” an entire probability measure onXtowards a new probability measure on Y. The operator T♯ “pushes forward” each elementary mass of a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a new measure onY) writtenT♯α. Note that such a push-forward T♯ :M1 + (X) →M1 + (Y) is a linear operator between measures in the sense that for two measures α1, α2onX ,T♯( α1+ α2) =T♯ α1 +T♯ α2. Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on densities linearly as a change of variables in the integration formula, indeed ρα (x) = |det (T′ (x))|ρβ (T (x)) (1.4) whereT′ (x) ∈Rd ×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate ofT). This implies, denoting y=T (x) |det (T′ (x))|=ρα (x) ρβ (y). 4 =Pi \u0000xiT ↵T] ↵def. =Pi \u0000T (xi) TT]gdef. =g \u0000TgPush-forward of measures Pull-back of functions Figure 1.2: Comparison of push-forward T♯and pull-back T♯. Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with the pull-back of function T♯ :C (Y) →C (X) which corresponds to the “warping” of functions. It is the linear map deﬁned, for g∈C (Y) byT♯g =g ◦T. Push-forward and pull-back are actually adjoint one from each others, in the sense that ∀(α ,g) ∈M (X) ×C (Y),∫ Ygd(T♯α) =∫ X(T ♯g )dα. It is important to realize that even if ( α,β) have densities ( ρα,ρβ) ,T♯ αis not equal to T♯ρβ, because of the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction between these push-forward and pull-back operators. Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri- butions of random variables. A random variable XonXis actually a map X: Ω →X from some abstract (often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1 + (X) such thatP(X ∈A) =α (A) =∫ Adα (x). Equivalently, it is the push-forward of PbyX,α =X ♯P. Applying another push-forward β =T♯ αforT :X →Y , following (1.2), is equivalent to deﬁning another random variableY=T (X) :ω∈Ω →T (X(ω)) ∈Y, so thatβis the distribution of Y. Drawing a random sample yfromYis thus simply achieved by computing y=T (x) wherexis drawn from X. Convergence of random variable. Convergence of random variable (in probability, almost sure, in law), convergence of measures (strong, weak). 1.2 Monge Problem Given a cost matrix ( Ci,j )i ∈JnK ,j ∈JmK, assuming n=m, the optimal assignment problem seeks for a bijectionσin the set Perm( n) of permutations of nelements solving min σ ∈Perm (n )1 nn∑ i=1Ci,σ (i). (1.5) One could naively evaluate the cost function above using all permutations in the set Perm( n). However, that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than 10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient algorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5 x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence, either matching σ= (1 ,2) (full line) or σ= (2 ,1) (dotted line) is optimal. (right) a Monge map can associate the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the disk marked at each location. The mapping here is such that T(x1) =T (x2) =y2 ,T (x3) =y3, whereas for 4⩽i ⩽7 we haveT(xi) =y1. Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions. Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4 corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case only two assignments exist, and they share the same cost. For discrete measures α =n∑ i=1ai δxiandβ =m∑ j=1bj δyj (1.6) the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must push the mass of αtoward the mass of β, which is to say that such a map T: {x1,... ,xn}→ {y1,... ,ym} must verify that ∀j ∈JmK ,bj=∑ i:T (xi) =yjai (1.7) which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is parameterized by a function c(x ,y) deﬁned for points ( x,y) ∈X ×Y min T{∑ ic(xi ,T (xi)) ;T♯α=β} . (1.8) Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using indicesσ :JnK →JmKso thatj=σ (i), and the mass conservation is written as ∑ i∈σ −1 (j )ai =bj. In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation constraint implies that Tis a bijection, such that T(xi) =yσ (i), and the Monge problem is equivalent to the optimal matching problem (1.5) where the cost matrix is Ci,jdef. =c (xi ,yj). Whenn̸ =m, note that, optimality aside, Monge maps may not even exist between an empirical measure to another. This happens when their weight vectors are not compatible, which is always the case when the target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows an (optimal) Monge map between αandβ, but there is no Monge map from βtoα. 6 Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces (X ,Y) as ﬁnding a map T:X →Y that minimizes min T{∫ Xc(x ,T (x ))dα (x) ;T♯α=β} (1.9) The constraint T♯α= βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward operator (1.2). 1.3 Kantorovitch Problem The assignment problem has several limitations in practical settings, also encountered when using the Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only be used to compare two points clouds of the same size. A direct generalization to discrete measures with non- uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7) (see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation. Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na- ture of transportation, namely the fact that a source point xican only be assigned to another, or transported to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially dispatched across several locations. Kantorovich moves away from the idea that mass transportation should be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded using, in place of a permutation σor a mapT, a coupling matrix P∈Rn ×m +, where Pi,jdescribes the amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj) ,xitowardsyjin the formalism of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps: U(a ,b )def.={ P∈Rn ×m + ;P1m =aand PT1n =b} , (1.10) where we used the following matrix-vector notation P1m= ∑ jPi,j  i∈Rnand PT1n=(∑ iPi,j) j∈Rm. The set of matrices U(a ,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex polytope (the convex hull of a ﬁnite set of matrices). Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in U(a ,b) if and only if PTis inU(b ,a). Kantorovich’s optimal transport problem now reads LC(a ,b )def.= min P∈U (a ,b) ⟨C ,P ⟩def.=∑ i,jCi ,jPi ,j. (1.11) This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are not necessarily unique. 7 ↵\u0000 ↵ \u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching, corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points). Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to associate two arbitrary discrete measures. Permutation Matrices as Couplings For a permutation σ ∈Perm (n), we write Pσfor the correspond- ing permutation matrix, ∀ (i ,j) ∈JnK2, (Pσ )i ,j= {1 /n ifj= σi, 0 otherwise. (1.12) One can check that in that case ⟨C ,Pσ⟩ =1 nn∑ i=1Ci, σi, which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the couplings Pare restricted to be exactly permutation matrices: min σ ∈Perm (n )1 nn∑ i=1Ci,σ (i)= min σ ∈Perm (n) ⟨C ,Pσ⟩. Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ polytope U(1n /n ,1n ,n). Indeed, for any permutation σwe have Pσ1 =1nandP σT1 =1n, whereas 1n1nT /n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that min σ ∈Perm (n) ⟨C ,Pσ⟩ ⩽LC (1n /n ,1n /n). The following proposition shows that these problems result in fact in the same optimum, namely that one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform measures a=b =1n /n, which shows that the Kantorovich relaxation is tight when considered on assignment problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special case. Proposition 1 (Kantorovich for matching) .Ifm =nanda =b =1n /n, then there exists an optimal solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈ Perm(n )for Problem (1.5) . Proof. Birkhoﬀ ’s theorem states that the set of extremal points of U(1n /n ,1n /n) is equal to the set of permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the polyhedron. 8 ⇡\u0000↵\u0000↵ ⇡\u0000↵\u0000↵ ⇡\u0000↵\u0000↵ Discrete Semi-discrete Continuous Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup. ⇡\u0000↵ ⇡\u0000↵ Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The coupling is localized along the graph of the Monge map ( x,T (x)) (displayed in black). Right: “discrete” couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare display with a black disk at position ( i,j) with radius proportional to Ti,j. Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to arbitrary measures by considering couplings π ∈M1 + (X ×Y ) which are joint distributions over the product space. The discrete case is a special situation where one imposes this product measure to be of the form π=∑ i,jPi ,jδ (xi ,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a marginal constraint on joint probability distributions U(α,β )def.={ π ∈M1 + (X ×Y ) ;PX♯π= αandPY♯π=β} . (1.13) HerePX♯andPY ♯are the push-forward (see Deﬁnition 1) by the projections PX(x ,y) =xandPY (x ,y) =y. Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π (A ×Y) = α (A) andπ (X ×B) =β (B) for setsA⊂X andB⊂Y. The Kantorovich problem (1.11) is then generalized as Lc(α,β )def.= min π ∈U(α,β)∫ X×Yc (x ,y )dπ (x ,y). (1.14) This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings, involving discrete and continuous marginals. On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x ,y) 9 \u0000↵\u0000↵⇡ \u0000↵\u0000↵⇡ \u0000↵\u0000↵⇡ ↵\u0000↵⇡ \u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps above (arrows) and couplings below. Inspired by [ ?]. is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs to impose moment condition on αandβ. Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be understood as a canonical way to lift a ground distance between points to a distance between histogram or measures. We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like to compare. The following proposition states that OT provides a meaningful distance between histograms supported on these bins. Proposition 2. We suppose n=m, and that for some p⩾1 ,C =Dp= (Dp i,j )i ,j ∈Rn ×nwhere D∈Rn ×n + is a distance on JnK,i .e. 1.D ∈Rn ×n + is symmetric; 2.Di ,j= 0if and only if i=j; 3.∀ (i ,j ,k ) ∈JnK3 ,Di ,k ⩽Di ,j +Dj ,k. Then Wp(a ,b )def.= LDp(a ,b )1 /p (1.15) (note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn ,i .e. Wpis symmetric, positive, Wp(a ,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality ∀a ,a′ ,b∈ Σn ,Wp (a ,b) ⩽Wp (a ,a′) + Wp(a′ ,b). Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal, Wp(a ,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ -diagonal elements of Dp, Wp(a ,b) >0 whenever a̸ =b (because in this case, an admissible coupling necessarily has a non-zero element outside the diagonal); by symmetry of Dp, Wp(a ,b) = 0 is itself a symmetric function. To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting, the explicit constuction of this glued coupling is simple. Let a,b ,c∈ Σn. Let PandQbe two optimal solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef. =bjifbj >0 and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne Sdef. =Pdiag (1/ ¯b )Q ∈Rn ×n +. 10 We remark that S∈U (a ,c) because S1n =Pdiag (1/ ¯b )Q1n =P (b/ ¯b) =P1Supp( b) =a where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b) =P1 =b because necessarily Pi,j= 0 forj / ∈Supp( b). Similarly one veriﬁes that S⊤1n =c. The triangle inequality follows from Wp(a ,c) =( min P∈U (a ,c) ⟨P ,Dp⟩ )1 /p ⩽ ⟨S ,Dp ⟩1 /p = ∑ ikDp ik∑ jPijQjk ¯bj 1 /p ⩽ ∑ ijk(Dij +Djk )pPijQjk ¯bj 1 /p ⩽ ∑ ijkDp ijPijQjk ¯bj 1 /p + ∑ ijkDp jkPijQjk ¯bj 1 /p = ∑ ijDp ijPij∑ kQjk ¯bj 1 /p + ∑ jkDp jkQjk∑ iPij ¯bj 1 /p = ∑ ijDp ijPij 1 /p + ∑ jkDp jkQjk 1 /p = Wp(a ,b) + Wp(b ,b). The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements inD, and the third comes from Minkowski’s inequality. Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete. Proposition 3. We assumeX=Y, and that for some p⩾1 ,c (x ,y) =d (x ,y )pwheredis a distance on X,i .e. (i )d (x ,y) =d (y ,x) ⩾0; (ii )d (x ,y) = 0 if and only if x=y; (ii)∀ (x ,y ,z ) ∈X3 ,d (x ,z) ⩽d (x ,y) +d (y ,z). Then Wp(α,β )def. =Ldp(α,β )1 /p (1.16) (note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i .e .Wpis symmetric, positive, Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality ∀(α,β,γ ) ∈M1 + (X )3 ,Wp(α,γ) ⩽Wp(α,β) +Wp(β,γ). Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ). The Wasserstein distance Wphas many important properties, the most important one being that it is a weak distance, i.e .it allows to compare singular distributions (for instance discrete ones) and to quantify spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences) are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to be ﬁxed to work). In sharp contrast, one has that for any p >0 ,Wp p( δx, δy) =d (x ,y). Indeed, it suﬃces to notice thatU( δx, δy) ={ δx ,y }and therefore the Kantorovich problem having only one feasible solution, Wp p( δx, δy) is necessarily ( d(x ,y )p )1 /p =d (x ,y). This shows that Wp( δx, δy) →0 ifx→y. This property corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne. 11 Deﬁnition 2 (Weak convergence) .( αk )kconverges weakly to αinM1 + (X) (denoted αk⇀α ) if and only if for any continuous function g∈C (X),∫ Xgdαk→∫ Xgdα. This notion of weak convergence corresponds to the convergence in law of random vectors. This convergence can be shown to be equivalent to Wp( αk,α) →0 [?, Theorem 6.8] (together with a convergence of the moments up to order pfor unbounded metric spaces). Note that there exists alternative distances which also metrize weak convergence. The simplest one are Hilbertian norms, deﬁned as ||α ||2 kdef. =Eα⊗α (k) =∫ X×Xk (x ,y )dα (x )dα (y) for a suitable choice of kernel k:X2 →R. The most famous of such kernel is the Gaussian one k(x ,y) = e− ||x −y ||2 2σ2for some choice of bandwidth σ >0. This convergence should not be confounded with the strong convergence of measures, which is metrized by the TV norm ||α ||TVdef.=|α| (X), which is the total mass of the absolute value of the measure. Algorithms Since ( ??) ˆA is a linear program, it is possible to use any classical linear program solver, such as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b =1n /n, there exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm, which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the OT problem. 1.4 Duality The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the relationship between the primal and dual problems. Proposition 4. One has LC(a ,b) = max (f ,g) ∈R (a ,b) ⟨f ,a⟩+ ⟨g ,b⟩ (1.17) where the set of admissible potentials is R(a ,b )def.={ (f ,g) ∈Rn ×Rm;∀ (i ,j) ∈JnK ×JmK ,f ⊕g ⩽C} (1.18) Proof. This result is a direct consequence of the more general result on the strong duality for linear pro- grams [ ?, p.148 ,Theo .4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17) is a lower bound on L C(a ,b) is discussed in ??. For the sake of completeness, let us derive this dual problem with the use of Lagrangian duality. The Lagangian associate to (1.11) reads min P⩾0max (f ,g) ∈Rn ×Rm ⟨C ,P⟩+ ⟨a −P1m ,f⟩+ ⟨b −P ⊤1n ,g⟩. (1.19) For linear program, one can always exchange the min and the max and get the same value of the linear program, and one thus consider max (f ,g) ∈Rn ×Rm ⟨a ,f⟩+ ⟨b ,g⟩+ min P⩾0 ⟨C −f1⊤ m−1ng⊤ ,P⟩. We conclude by remarking that min P⩾0 ⟨Q ,P⟩= {0 if Q⩾0 −∞ otherwise so that the constraint reads C−f1⊤ m−1ng⊤ =C −f ⊕g ⩾0. 12 The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal transport plan Supp( P)⊂{ (i ,j) ∈JnK ×JmK ;fi +gj =Ci ,j} . (1.20) To extend this primal-dual construction to arbitrary measures, it is important to realize that measures are naturally paired in duality with continuous functions (a measure can only be accessed through integration against continuous functions). The duality is formalized in the following proposition, which boils down to Proposition 4 when dealing with discrete measures. Proposition 5. One has Lc(α,β) = max (f ,g) ∈R (c)∫ Xf(x )dα (x) +∫ Yg(y )dβ (y), (1.21) where the set of admissible dual potentials is R(c )def.={ (f ,g) ∈C (X) ×C (Y) ;∀ (x ,y) ,f (x) +g (y) ⩽c (x ,y)}. (1.22) Here, (f ,g )is a pair of continuous functions, and are often called “Kantorovich potentials”. The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e. (fi ,gj) = (f (xi) ,g (yj)). The primal-dual optimality conditions allow to track the support of optimal plan, and (1.20) is generalized as Supp(π)⊂{ (x ,y) ∈X ×Y ;f (x) +g (y) =c (x ,y)}. (1.23) Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non- trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily Lipschitz regular, which enable to replace the constraint by a compact one. Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems are equivalent. Theorem 1 (Brenier) .In the caseX=Y =Rdandc (x ,y) = ||x −y ||2, if at least one of the two inputs measures (denoted α) has a density ρ αwith respect to the Lebesgue measure, then the optimal πin the Kantorovich formulation (1.14) is unique, and is supported on the graph (x ,T (x ))of a “Monge map” T: Rd→Rd. This means that π= (Id ,T)♯µ ,i .e. ∀h ∈C (X ×Y ),∫ X×Yh (x ,y )dπ (x ,y) =∫ Xh(x ,T (x ))dµ (x). (1.24) Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ ,T (x) =∇ϕ (x), where ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is related to the dual potential fsolving (1.21) asϕ (x) = ||x ||2 2−f (x). Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark that∫ cdπ =Cα,β −2∫ ⟨x, y⟩dπ (x ,y) where the constant is Cα,β=∫ ||x ||2dα (x) +∫ ||y ||2dβ (y). Instead of solving (1.14), one can thus consider the following problem max π ∈U(α,β)∫ X×Y ⟨x, y⟩dπ (x ,y), whose dual reads min (ϕ,ψ){∫ Xϕdα+∫ Yψdβ;∀ (x ,y), ϕ (x) +ψ (y)⩾ ⟨x, y⟩} . (1.25) 13 The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||· ||2 2−f,||· ||2 2−g). One can replace the constraint by ∀y, ψ (y)⩾ϕ∗ (y )def.= sup x⟨x, y⟩−ϕ (x). (1.26) Hereϕ ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can minimize explicitly with respect to ϕand setψ=ϕ ∗in order to consider the unconstraint problem min ϕ∫ Xϕdα+∫ Yϕ ∗dβ, (1.27) see also Section ??for a generalization of this idea to generic costs c(x ,y). By iterating this argument twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex. Condition (1.23) shows that an optimal πis supported on{ (x ,y) ;ϕ (x) +ϕ∗ (y) = ⟨x, y⟩ }which shows that such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads y∈∂ϕ (x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also diﬀerentiable α -almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α -almost everywhere as y=∇ϕ (x), and shows that necessarily π= (Id,∇ϕ)♯α. This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9) and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map). Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should be examined under the light that a convex function is the natural generalization of the notion of increasing functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?]. Note also that this theorem can be extended in many directions. The condition that αhas a density can be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller thand−1 (e .g. hypersurfaces). One can also consider costs of the form c(x ,y) =h (x −y) wherehis a strictly convex function. For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a constant) convex function which solves the following Monge-Amp ˜A ¨re-type equation det( ∂2ϕ (x))ρβ(∇ϕ (x)) =ρα (x) (1.28) where∂2ϕ (x) ∈Rd ×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ (x)) can be understood as a non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the Laplacian ∆ as a linearization since for smooth maps det( ∂2ϕ (x)) = 1 +ε∆ϕ (x) +o(ε). The convexity constraint forces det( ∂2ϕ (x)) ⩾0 and is necessary for this equation to have a solution. Special cases In general, computing OT distances is numerically involved. We review special favorable cases where the resolution of the OT problem is easy. Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on the diagonal and 1 elsewhere, namely when C=1n ×n −In, the OT distance between aandbis equal to the 1-norm of their diﬀerence, L C(a ,b) = ||a −b ||1. One can also easily check that this result extends to discrete and discrete measures in the case where c(x ,y) is 0 ifx=yand 1 when x̸ =y. The OT distance between two discrete measures αand βis equal to their total variation distance. 14 \u0000\u0000 ↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling. Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This corresponds to monotone rearrangements, if xi⩽xi ′are such that Pi,j̸= 0,Pi′ ,j′̸= 0, then necessarily yj⩽yj′. Remark 7 (1-D case – Empirical measures) .HereX =R. Assuming α =1 n∑n i=1 δxiandβ =1 n∑n j=1 δyj, and assuming (without loss of generality) that the points are ordered, i.e .x1 ⩽x2⩽... ⩽xnand y1 ⩽y2⩽... ⩽yn, then one has the simple formula Wp(α,β )p =p∑ i=1 |xi −yi |p, (1.29) i.e .locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of αandβ. That statement is only valid locally, in the sense that the order (and those vector representations) might change whenever some of the values change. That formula is a simple consequence of the more general remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case. Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function ∀x ∈R ,Cα (x )def.= ∫x − ∞dα, (1.30) which is a function Cα :R→ [0 ,1], and its pseudo-inverse C−1 α: [0 ,1] →R∪{−∞} ∀r∈ [0 ,1] ,C −1 α (r) = min x{x ∈R∪{−∞} ;Cα (x) ⩾r}. That function is also called the generalized quantile function of α. For anyp⩾1, one has Wp(α,β )p= ||C −1 α −C −1 β ||p Lp( [0 ,1])= ∫1 0|C −1 α (r) −C −1 β (r) |pdr. (1.31) This means that through the map α↦ →C −1 α, the Wasserstein distance is isometric to a linear space equipped with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally in§??. Forp= 1, one even has the simpler formula W1(α,β) = ||Cα −Cβ ||L1 (R)=∫ R|Cα (x) −Cβ (x) |dx (1.32) =∫ R⏐⏐⏐⏐ ∫x − ∞d(α−β) ⏐⏐⏐⏐dx. (1.33) 15 µ ν (tT+ (1 −t )Id)♯µ 0 0.5 10.5Cµ Cν 0 0.5 100.51 Cµ-1 Cν-1 0 0.5 100.51 T T-1 0 0.5 100.51 (Cα ,Cβ) (C −1 α ,C −1 β) ( T,T −1) (1 −t )C −1 α +tC −1 β Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant function as detailed in (1.34). which shows that W1is a norm (see§ ??for the generalization to arbitrary dimensions). An optimal Monge mapTsuch thatT♯α= βis then deﬁned by T=C −1 β ◦Cα. (1.34) Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of optimal transport in 1-D, we refer the reader to [ ?, Chapter 2]. Remark 9 (Distance between Gaussians) .Ifα =N (mα,Σα) andβ =N (mβ,Σβ) are two Gaussians in Rd, then one can show that the following map T:x↦ →mβ +A (x −mα), (1.35) where A=Σ −1 2α( Σ1 2αΣβ Σ1 2α )1 2Σ −1 2α =AT, is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed since ρβ (T (x)) = det(2πΣβ) −1 2exp(− ⟨T (x) −mβ,Σ −1 β (T (x) −mβ)⟩) = det(2πΣβ) −1 2exp(− ⟨x −mα, ATΣ −1 βA (x −mα)⟩) = det(2πΣβ) −1 2exp(− ⟨x −mα,Σ −1 α (x −mα)⟩), and sinceTis a linear map we have that |detT′ (x)|= detA= (detΣβ detΣα )1 2 and we therefore recover ρα= |detT′|ρ βmeaningT♯α=β. Notice now that Tis the gradient of the convex functionψ :x↦ →1 2⟨x −mα, A(x −mα)⟩+ ⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??) thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ?? 16 -4 -2 0 2 4 6-3-2-101234 ρβρ αFigure 1.10: Two Gaussians ρ αandρβ, represented using the contour plots of their densities, with respective mean and variance matrices mα= ( −2 ,0),Σα =1 2( 1−1 2; −1 21) andmβ= (3 ,1),Σβ=( 2,1 2;1 2,1) . The arrows originate at random points xtaken on the plane and end at the corresponding mappings of those pointsT(x) =mβ +A (x −mα). \u0000m Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ (x )def.= 1√ 2πse− (x −m )2 2s2the Gaussian density, it thus shows the interpolation G(1 −t )m0 +tm1, (1 −t) σ0 +t σ1. With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport cost of that map is W2 2(α,β) = ||mα −mβ ||2 +B(Σα,Σβ )2 (1.36) whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]), B(Σα,Σβ )2def.= tr( Σα+Σβ −2( Σ1 /2 αΣβ Σ1 /2 α )1 /2) , (1.37) where Σ1 /2is the matrix square root. One can show that Bis a distance on covariance matrices, and that B2is convex with respect to both its arguments. In the case where Σα= diag(ri )iandΣβ= diag(si )iare diagonals, the Bures metric is the Hellinger distance B(Σα,Σβ) =|| √r− √s ||2. For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√ Σ), as illustrated in Figure 1.11. For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?]. 1.5 Sinkhorn This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to the original problem. This regularization has several important advantages, but a few stand out particularly: The minimization of the regularized problen can be solved using a simple alternate minimization scheme; that scheme translates into iterations that are simple matrix products, making them particularly suited to execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights and positions of the Diracs. 17 c\"P \"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε= argminP∈ Σ3 ⟨C ,P⟩− εH (P) for a varying ε. Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as H(P )def.=−∑ i,jPi ,j (log (Pi ,j) −1), (1.38) with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis 0 or negative. The function His 1-strongly concave, because its hessian is ∂2H (P) = −diag (1 /Pi ,j) and Pi,j ⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function to obtain approximate solutions to the original transport problem (1.11): Lε C(a ,b )def.= min P∈U (a ,b) ⟨P ,C⟩− εH (P). (1.39) Since the objective is a ε -strongly convex function, problem 1.39 has a unique optimal solution. The idea to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a more “blurred” traﬃc prediction. Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized problem towards an optimal solution of the original linear program has been studied by [ ?]. Proposition 6 (Convergence with ε) .The unique solution Pεof (1.39) converges to the optimal solution with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely Pεε →0− →argmin P{ −H (P) ;P ∈U (a ,b), ⟨P ,C⟩= LC(a ,b)} (1.40) so that in particular Lε C(a ,b)ε →0− →LC (a ,b). One has Pεε→∞− →abT= (aibj )i ,j. (1.41) Proof. We consider a sequence ( εℓ) ℓsuch thatεℓ →0 andεℓ >0. We denote Pℓthe solution of (1.39) for ε=εℓ. Since U(a ,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity) such that Pℓ →P⋆. Since U(a ,b) is closed, P⋆ ∈U (a ,b). We consider any Psuch that⟨C ,P⟩= LC(a ,b). By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has 0⩽ ⟨C ,Pℓ⟩− ⟨C ,P⟩⩽εℓ (H (Pℓ) −H (P)). (1.42) 18 ⇡\"↵\u0000 \"\u0000 ↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6. Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number n=mof points (only entries of the optimal ( Pi,j )i ,jabove a small threshold are displayed as segments betweenxiandyj). Since His continuous, taking the limit ℓ→+ ∞in this expression shows that ⟨C ,P⋆⟩= ⟨C ,P ⟩so that P⋆is a feasible point of (1.40). Furthermore, dividing by ε ℓin (1.42) and taking the limit shows that H(P) ⩽H (P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆ 0to this program is unique by strict convexity of −H, one has P⋆ =P⋆ 0, and the whole sequence is converging. Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is performed in [ ?], including a ﬁrst order expansion in ε (resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13 shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to faster statistical convergence (as exposed in §??). Deﬁning the Kullback-Leibler divergence between couplings as KL(P |K )def.=∑ i,jPi ,jlog (Pi ,j Ki,j) −Pi ,j +Ki ,j, (1.43) the unique solution Pεof (1.39) is a projection onto U(a ,b) of the Gibbs kernel associated to the cost matrix Cas Ki,jdef. =e −Ci ,j ε Indeed one has that using the deﬁnition above Pε= ProjKL U(a ,b) (K )def.= argmin P∈U (a ,b )KL (P |K). (1.44) Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy by the relative entropy with respect to the product measure d α ⊗dβ (x ,y )def.= dα (x )dβ (y), and propose a regularized counterpart to (1.14) using Lε c(α,β )def.= min π ∈U(α,β)∫ X×Yc (x ,y )dπ (x ,y) + εKL(π|α⊗β) (1.45) where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43) KL(π|ξ )def.=∫ X×Ylog (dπ dξ (x ,y)) dπ (x ,y)+ ∫ X×Y (dξ (x ,y) −dπ (x ,y)), (1.46) 19 and by convention KL( π|ξ) = + ∞if πdoes not have a densitydπ dξwith respect to ξ. It is important to realize that the reference measure α⊗ βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β) plays no speciﬁc role, only its support matters. Formula (1.45) can be re-factored as a projection problem min π ∈U(α,β )KL(π |K) (1.47) whereKis the Gibbs distributions d K(x ,y )def. =e −c (x ,y) εdµ (x )dν (y). This problem is often referred to as the “static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?]. Asε →0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§?? details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting the points of two measures. Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form, which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in the sense that a coupling PinU(a ,b) hasnmvariables but n+mconstraints. Proposition 7. The solution to (1.39) is unique and has the form ∀ (i ,j) ∈JnK ×JmK ,Pi ,j =uiKi ,jvj (1.48) for two (unknown) scaling variable (u ,v) ∈Rn + ×Rm +. Proof. Introducing two dual variables f∈Rn ,g ∈Rmfor each marginal constraint, the Lagrangian of (1.39) reads E(P ,f ,g) = ⟨P ,C⟩− εH (P)− ⟨f ,P1m −a⟩− ⟨g ,PT1n −b⟩. Considering ﬁrst order conditions, we have ∂E (P ,f ,g) ∂Pi ,j =Ci ,j− εlog (Pi ,j) −fi −gj. which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j =efi/ εe −Ci ,j/ εegj/ε which can be rewritten in the form provided in the proposition using non-negative vectors uandv. The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in matrix form as P= diag( u)Kdiag (v) .u ,vmust therefore satisfy the following non-linear equations which correspond to the mass conservation constraints inherent to U(a ,b), diag(u )Kdiag (v )1m =a ,and diag( v)K ⊤diag (u )1n =b, (1.49) These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u) times Kvis u⊙ (Kv) =aand v⊙ (KTu) =b (1.50) where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm: u(ℓ +1 )def. =a Kv(ℓ )and v(ℓ +1 )def. =b KTu(ℓ +1), (1.51) initialized with an arbitrary positive vector v(0) =1m. The division operator used above between two vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent 20 `⇡(`)\" 1000 2000 3000 4000 5000-2-1 .5-1-0.50 `Figure 1.14: Left: evolution of the coupling πℓ ε= diag( U(ℓ ))Kdiag (V(ℓ)) computed at iteration ℓof Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured in term of marginal constraint violation log( ||πℓ ε1m −b ||1). solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then so doλu ,v/ λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in the same optimal coupling diag( u)Kdiag (v). Figure 1.14, top row, shows the evolution of the coupling diag(U(ℓ ))Kdiag (V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal. Remark 11 (Relation with iterative projections) .Denoting C1 adef.= {P ;P1m =a }andC2 bdef.={ P;PT1m =b} the rows and columns constraints, one has U(a ,b) =C1 a∩C2 b. One can use Bregman iterative projections [ ?] P(ℓ +1) def.= ProjKL C1a (P(ℓ)) and P(ℓ +2) def.= ProjKL C2 b(P(ℓ +1)). (1.52) Since the setsC1 aandC2 bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?]. These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning P(2ℓ )def.= diag( u(ℓ ))Kdiag (v(ℓ)), one has P(2ℓ +1) def.= diag( u(ℓ +1 ))Kdiag (v(ℓ)) and P(2ℓ +2) def.= diag( u(ℓ +1 ))Kdiag (v(ℓ +1)) In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??). Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is greatly simpliﬁed using Hilbert projective metric on Rn +,∗ (positive vectors), deﬁned as ∀ (u ,u′)∈ (Rn +,∗ )2, dH(u ,u′ )def.= log max i,i ′uiu′ i′ ui′u′ i. This can be shows to be a distance on the projective cone Rn +,∗/∼, where u∼u ′means that∃s >0 ,u =su′ (the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the triangular inequality and dH(u ,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original distance on bounded open convex sets [ ?]. The projective cone Rn +,∗/ ∼is a complete metric space for this distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the cone of positive vectors. 21 Theorem 2. Let K∈Rn ×m +,∗, then for (v ,v′)∈ (Rm +,∗ )2 dH(Kv ,Kv′)⩽λ (K )dH (v ,v′ )where  λ (K )def.=√ η (K) −1√ η (K) +1 <1 η (K )def.= max i,j ,k, ℓKi ,kKj,ℓ Kj,kKi,ℓ. Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to show the linear convergence of Sinkhorn’s iterations. Theorem 3. One has (u(ℓ) ,v(ℓ))→ (u⋆ ,v⋆ )and dH(u(ℓ) ,u⋆) =O(λ (K )2ℓ), dH(v(ℓ) ,v⋆) =O(λ (K )2ℓ). (1.53) One also has dH(u(ℓ) ,u⋆) ⩽dH (P(ℓ )1m ,a) 1−λ (K) dH(v(ℓ) ,v⋆) ⩽dH (P(ℓ), ⊤1n ,b) 1−λ (K) (1.54) where we denoted P(ℓ )def.= diag( u(ℓ ))Kdiag (v(ℓ)). Lastly, one has ∥log (P(ℓ)) −log (P⋆)∥∞ ⩽dH (u(ℓ) ,u⋆) +dH (v(ℓ) ,v⋆) (1.55) where P⋆is the unique solution of (1.39) . Proof. One notice that for any ( v,v′)∈ (Rm +,∗ )2, one has dH(v ,v′) =dH (v /v′ ,1m) =dH (1m /v ,1m /v′). This shows that dH(u(ℓ +1) ,u⋆) =dH (a Kv(ℓ) ,a Kv⋆) =dH (Kv(ℓ) ,Kv⋆)⩽λ (K )dH (v(ℓ) ,v⋆). where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality dH(u(ℓ) ,u⋆) ⩽dH (u(ℓ +1) ,u(ℓ)) +dH (u(ℓ +1) ,u⋆) ⩽dH (a Kv(ℓ) ,u(ℓ)) +λ (K )dH (u(ℓ) ,u⋆) =dH( a,u(ℓ)⊙ (Kv(ℓ))) +λ (K )dH (u(ℓ) ,u⋆), which gives the ﬁrst part of (1.54) since u(ℓ)⊙ (Kv(ℓ)) =P(ℓ )1m (the second one being similar). The proof of (1.55) follows from [ ?, Lemma 3] The bound (1.54) shows that some error measures on the marginal constraints violation, for instance ∥P(ℓ )1m −a ∥1and ∥P(ℓ )T1n −b ∥1, are useful stopping criteria to monitor the convergence. Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate degrades as ε →0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent of the scaled coupling matrix. 22 Regularized Dual and Log-domain Computations The following proposition details the dual problem associated to (1.39). Proposition 8. One has Lε C(a ,b) = max f∈Rn ,g ∈Rm ⟨f ,a⟩+ ⟨g ,b⟩−ε ⟨ef/ε ,Keg/ε⟩. (1.56) The optimal (f ,g )are linked to scalings (u ,v )appearing in (1.48) through (u ,v) = (ef/ε ,eg/ε). (1.57) Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P and dual multipliers fandgfor the marginal constraints as Pi,j =efi/ εe −Ci ,j/ εegj/ε. Substituting in the LagrangianE(P ,f ,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange dual function equals f,g↦→ ⟨ef/ε, (K ⊙C )eg/ε⟩− εH (diag (ef/ε )Kdiag (eg/ε)). (1.58) The entropy of Pscaled byε, namelyε ⟨P ,logP −1n ×m ⟩can be stated explicitly as a function of f,g ,C ⟨diag (ef/ε )Kdiag (eg/ε) ,f1mT +1ngT −C− ε1n ×m⟩ =− ⟨ef/ε, (K ⊙C )eg/ε⟩+ ⟨f ,a⟩+ ⟨g ,b⟩−ε ⟨ef/ε ,Keg/ε⟩ therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times are those displayed in (1.56). Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual problem (1.56) reads sup f,g ∈C (X) ×C (Y)∫ Xf(x )dα (x) +∫ Yg(x )dβ (x)−ε∫ X×Ye −c (x ,y) +f (x) +g (y) ε dα (x )dβ (y) This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which is retrieved in the limit ε →0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the convergence of Sinkhorn iterations, see [ ?] for more details. Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one can easily notice that, writing Q(f ,g) for the objective of (1.56) that ∇ |fQ (f ,g) =a −ef/ε⊙( Keg/ε) , (1.59) ∇ |gQ (f ,g) =b −eg/ε⊙( KTef/ε) . (1.60) Block coordinate ascent can therefore be implemented in a closed form by applying successively the following updates, starting from any arbitrary g(0), forl⩾0, f(ℓ +1)= εloga− εlog( Keg(ℓ)/ε) , (1.61) g(ℓ +1)= εlogb− εlog( KTef(ℓ +1)/ε) . (1.62) Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal- dual relations highlighted in (1.57). Indeed, we recover that at any iteration (f(ℓ) ,g(ℓ)) =ε (log (u(ℓ)) ,log (v(ℓ))). 23 Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation, using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its coordinates, namely minεz=− εlog∑ ie−zi/ε. Note that min ε (z) converges to min zfor any vector zasε →0. Indeed, min εcan be interpreted as a diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be rewritten (f(ℓ +1 ))i= minε (Cij −g(ℓ) j)j+ εlogai, (1.63) (g(ℓ +1 ))j= minε (Cij −f(ℓ) i)i+ εlogbj. (1.64) Here the term min ε (Cij −g(ℓ) j)jdenotes the soft-minimum of all values of the j-th column of matrix (C −1n (g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn ×m, we deﬁne Minrow ε (A )def.=( minε (Ai ,j )j) i∈Rn, Mincol ε (A )def.=( minε (Ai ,j )i) j∈Rm. Note that these operations are equivalent to the entropic c-transform introduced in §?? (see in particu- lar (??)). Using these notations, Sinkhorn’s iterates read f(ℓ +1)= Minrow ε (C −1ng(ℓ )T) + εloga, (1.65) g(ℓ +1)= Mincol ε (C −f(ℓ )1mT) + εlogb. (1.66) Note that as ε →0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0, because alternate minimization does not converge for constrained problems (which is the case for the un- regularized dual (1.17)). Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera- tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values ofε. Writing z = min z, that trick suggests to evaluate min εzas minεz= z− εlog∑ ie− (zi −z)/ε. (1.67) Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the previously computed scalings. This leads to the following stabilized iteration f(ℓ +1)= Minrow ε (S (f(ℓ) ,g(ℓ))) −f(ℓ)+ εlog (a) (1.68) g(ℓ +1)= Mincol ε (S (f(ℓ +1) ,g(ℓ))) −g(ℓ)+ εlog (b), (1.69) where we deﬁned S(f ,g) =( Ci,j −fi −gj) i,j. In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for arbitraryε >0, because the quantity S(f ,g) stays bounded during the iterations. The downside is that it requiresnmcomputations of exp at each step. Computing a Minrow εor Mincol εis typically substantially slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously. In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?]. 24 1.6 Extensions Wasserstein Barycenters. Given input histogram {bs }S s=1, wherebs∈ Σns, and weights λ∈ ΣS, a Wasserstein barycenter is computed by minimizing min a∈ ΣnS∑ s=1 λsLCs (a ,bs) (1.70) where the cost matrices Cs∈Rn ×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the barycenters are deﬁned on the same grid, ns=n ,Cs =C =Dpis set to be a distance matrix, so that one solves min a∈ ΣnS∑ s=1 λsWp p(a ,bs). This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved in particular uniqueness of the barycenter for c(x ,y) = ||x −y ||2overX =Rd, if one of the input measure has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the one guaranteeing the existence of a Monge map, see Remark ??). The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S couplings ( Ps)sbetween each input and the barycenter itself min a∈ Σn, (Ps ∈Rn ×ns )s {S∑ s=1 λs ⟨Ps ,Cs⟩; ∀s ,P⊤ s1ns =a ,P⊤ s1n =bs} . Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?]. Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs )sde ﬁned on some space X, the barycenter problem becomes min α ∈M1 + (X )S∑ s=1 λsLc(α, βs). (1.71) In the case where X=Rdandc (x ,y) = ||x −y ||2, [?] shows that if one of the input measures has a density, then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing barycenters of points ( xs)S s=1 ∈XSto arbitrary measures. Indeed, if βs= δxsis a single Dirac mass, then a solution to (1.71) is δx ⋆wherex ⋆is a Fr´ echet mean solving ( ??). Note that for c(x ,y) = ||x −y ||2, the mean of the barycenter α ⋆is necessarily the barycenter of the mean, i.e. ∫ Xxdα⋆ (x) =∑ sλs∫ Xxdαs (x), and the support of α ⋆is located in the convex hull of the supports of the ( αs )s. The consistency of the approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to re-cast (1.71) as a multi-marginal OT problem, see Remark ??. One can use entropic smoothing and approximate the solution of (1.70) using min a∈ ΣnS∑ s=1 λsLε Cs(a ,bs) (1.72) for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is 25 useful to integrate additional regularizations on the barycenter (e .g. to impose some smoothness). A simple but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem min (Ps )s{∑ sλsKL (Ps |Ks) ; ∀s ,PsT1m =bs ,P111=... =PS1S} (1.73) where we denoted Ksdef. =e −Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all the couplings Ps∈Rn ×nsasa =P111=... =PS1S. As detailed in [ ?], one can generalize Sinkhorn to this problem, which also corresponds to iterative projection. This can also be seen as a special case of the generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling form as Ps= diag( us)Kdiag (vs), (1.74) and the scalings are sequentially updated as ∀s ∈J1 ,SK ,v(ℓ +1) sdef. =bs KT su(ℓ) s, (1.75) ∀s ∈J1 ,SK ,u(ℓ +1) sdef. =a(ℓ +1) Ksv(ℓ +1) s, (1.76) where a(ℓ +1 )def.=∏ s(Ksv(ℓ +1) s) λs. (1.77) An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual problem, which detailed in the following proposition. Proposition 9. The optimal (us ,vs )appearing in (1.74) can be written as (us ,vs) = (efs/ε ,egs/ε )where (fs ,gs )sare the solutions of the following program (whose value matches the one of (1.72) ) max (fs ,gs )s{∑ sλs( ⟨gs ,bs⟩−ε ⟨Ksegs/ε, efs/ε⟩) ;∑ sλsfs= 0} . (1.78) Proof. Introducing Lagrange multipliers in (1.73) leads to min (Ps )s ,amax (fs ,gs )s∑ sλs( εKL (Ps |Ks) + ⟨a −Ps1m ,fs⟩ + ⟨bs −PsT1m ,gs⟩) . Strong duality holds, so that one can exchange the min and the max, and gets max (fs ,gs )s∑ sλs( ⟨gs ,bs⟩+ min PsεKL (Ps |Ks)− ⟨Ps ,fs ⊕gs⟩) + min a⟨∑ sλsfs ,a⟩. The explicit minimization on agives the constraint∑ sλsfs= 0 together with max (fs ,gs )s∑ sλs ⟨gs ,bs⟩− εKL∗ (fs ⊕gs ε |Ks) where KL∗(· |Ks) is the Legendre transform ( ??) of the function KL∗(· |Ks). This Legendre transform reads KL∗ (U |K) =∑ i,jKi ,j (eUi ,j −1), (1.79) 26 Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights ( λs )sare bilinear with respect to the four corners of the square. Shapes are represented as measures that are uniform within the boundaries of the shape and null outside. which shows the desired formula. To show (1.79), since this function is separable, one needs to compute ∀ (u ,k) ∈R2 + ,KL∗ (u |k )def.= max rur− (rlog (r /k) −r +k) whose optimality condition reads u= log(r /k), i.e .r =keu, hence the result. Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads to the expression (1.76). Figures ??and ??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure, the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer to [?] for more details and other applications to computer graphics and imaging sciences. Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈ Θ }where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ ﬁdelity” term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a suitable parameter θis obtained by minimizing directly min θ∈ ΘE(θ )def. =Lc(αθ,β). (1.80) Of course, one can consider more complicated problems: for instance, the barycenter problem described in§ ??consists in a sum of such terms. However, most of these more advanced problems can be usually solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives, or using automatic diﬀerentiation. The Wasserstein distance between two histograms or two densities is convex with respect to these inputs, as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ = Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ= ∑K i=1 θi αi is a convex combination of known atoms α1,..., αKin ΣN, Problem (1.80) remains convex (the ﬁrst case corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general not convex. 27 g✓XZ ⇣xz\u0000↵ ✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81. A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given some discrete samples ( xi)n i=1 ⊂X from some unknown distribution, the goal is to ﬁt a parametric model θ↦→αθ ∈M (X) to the observed empirical input measure β min θ∈ ΘL(αθ,β) where β =1 n∑ iδxi, (1.81) whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig- ure 1.16). In the case where α θas a densify ρ θdef.=ρα θwith respect to the Lebesgue measure (or any other ﬁxed reference measure), the maximum likelihood estimator (MLE) is obtained by solving min θLMLE(αθ,β )def.=−∑ ilog(ρθ (xi)). This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i .d. samples of some ¯β, then LMLE(α,β )n→+∞−→ KL(α|¯β) This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]). However, it fails to work when estimating singular distributions, typically when the α θdoes not has a density (so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β (so that the α θshould share the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that in several cases of practical interest, the density ρ θis inaccessible (or too hard to compute). A typical setup where both problems (singular and unknown densities) occur is for so-called generative models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ ∈M (Z) αθ =hθ,♯ ζwherehθ :Z →X where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so that the support of α θis localized along a low-dimensional “manifold” and the resulting density is highly singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density is usually intractable, while generating i.i .d. samples from α θis achieved by computing xi=hθ (zi) where (zi )iare i.i .d. samples from ζ. In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional LMLE, which needs to be written in dual form as L(α,β )def.= max (f ,g) ∈C (X )2{∫ Xf(x )dα (x) +∫ Xg(x )dβ (x) ; (f ,g) ∈R} . (1.82) Dual norms exposed in § ??correspond to imposing R={ (f, −f) ;f ∈B}, while optimal transport (1.21) setsR=R (c) as deﬁned in (1.22). 28 For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with respect toθis much more involved, and is typically highly non-convex. The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE), was initially introduced in [ ?], see also [ ?]. Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption, namely that one has at its disposal two matrices D∈Rn ×nandD′ ∈Rm ×mthat represent some relationship between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power of) distance matrices. The Gromov-Wasserstein problem reads GW(( a,D), (b ,D′ ))2def.= min P∈U (a ,b )ED ,D′ (P )def.=∑ i,j ,i′ ,j′ |Di ,i′ −D′ j,j′ |2Pi ,jPi′ ,j′. (1.83) This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?] for a particular cost. One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83)) up to isometries preserving the measures. This distance was introduced and studied in details by Memoli in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?]. Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between metric measure spaces ( X,dX, αX) and (Y ,dY, αY) where (dX ,dY) are distances and ( αX, αY) are measures on their respective spaces. One deﬁnes GW(( αX ,dX),( αY ,dY ))2def.= min π ∈U( αX, αY)∫ X2 ×Y2 |dX (x ,x′) −dY (y ,y′) |2dπ (x ,y )dπ (x′ ,y′). (1.84) GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX ,dX) and ( αY ,dY) are isometric if there exists ϕ :X →Y such thatϕ♯ αX= αYanddY(ϕ (x),ϕ (x′)) =dX (x ,x′). Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0 ,dX0, α0) and (X1 ,dX1, α1) can be chosen to be t∈ [0 ,1]↦→ (X0 ×X 1,dt,π⋆) whereπ ⋆is a solution of (1.84) and for all ((x0 ,x1), (x′ 0,x′ 1))∈ (X0 ×X 1)2, dt((x0 ,x1), (x′ 0,x′ 1))def.= (1 −t )dX0 (x0 ,x′ 0) +tdX1 (x1 ,x′ 1). This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product spaceX0 ×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85) detailed below. To approximate the computation of GW, and to help convergence of minimization schemes to better minima, one can consider the entropic regularized variant min P∈U (a ,b )ED ,D′ (P)− εH (P). (1.85) 29 Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn iterations (1.86). Extracted from [ ?]. As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations of the objective function lead to consider the succession of updates P(ℓ +1) def.= min P∈U (a ,b) ⟨P ,C(ℓ)⟩− εH (P) where (1.86) C(ℓ )def.= ∇ED ,D′ (P(ℓ)) = −D ′TP(ℓ )D, which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to compute soft maps between domains. 30 Bibliography [1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT- LAB. SIAM, 2014. [2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝ in Machine Learning , 3(1) :1 –122, 2011. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004. [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2) :219 –266, 2004. [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM Multiscale Modeling and Simulation , 5:861 –899, 2005. [6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. , 20:89 –97, 2004. [7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro- duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse recovery , 9(263-340) :227, 2010. [8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta Numerica , 25:161 –319, 2016. [9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing , 20(1) :33 –61, 1999. [10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982. [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM Multiscale Modeling and Simulation , 4(4), 2005. [12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413 –1541, 2004. [13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425 –455, Dec 1994. [14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume 375. Springer Science & Business Media, 1996. [15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans. Image Proc. , 12(8) :906 –916, 2003. [16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1. Birkh¨ auser Basel, 2013. 31 [17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008. [18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia- tional problems. Commun. on Pure and Appl. Math. , 42:577 –685, 1989. [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization , 1(3) :127 –239, 2014. [20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004. [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11) :1338 –1351, November 2003. [22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4) :259 –268, 1992. [23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich. Variational methods in imaging . Springer, 2009. [24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal , 27(3) :379 –423, 1948. [25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and related geometric multiscale analysis . Cambridge university press, 2015. 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"PythonCodeTextSplitter\")\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, splitter_type = \"PythonCodeTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "GBO7EgWi0Xp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35189d7-4124-4e27-83d7-4495c8a6ab7e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chunk 2: www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "Chunk 3: belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "Chunk 4: α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "Chunk 5: x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Chunk 6: Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "Chunk 7: “objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "Chunk 8: to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "Chunk 9: equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Chunk 10: it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "Chunk 11: ∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "Chunk 12: dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "Chunk 13: the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "Chunk 14: Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Chunk 15: Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "Chunk 16: dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "Chunk 17: +(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Chunk 18: 3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "Chunk 19: i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "Chunk 20: 1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Chunk 21: Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "Chunk 22: positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "Chunk 23: mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "Chunk 24: α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Chunk 25: β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Chunk 26: +(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "Chunk 27: measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "Chunk 28: onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "Chunk 29: new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Chunk 30: Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "Chunk 31: with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "Chunk 32: ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "Chunk 33: |det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Chunk 34: Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "Chunk 35: the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "Chunk 36: map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "Chunk 37: ∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "Chunk 38: the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "Chunk 39: registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Chunk 40: Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "Chunk 41: (often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "Chunk 42: another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "Chunk 43: yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "Chunk 44: convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "Chunk 45: bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "Chunk 46: min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "Chunk 47: that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "Chunk 48: 10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "Chunk 49: algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "Chunk 50: 5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "Chunk 51: either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "Chunk 52: the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "Chunk 53: disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Chunk 54: 4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Chunk 55: Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "Chunk 56: corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "Chunk 57: For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "Chunk 58: push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "Chunk 59: must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "Chunk 60: parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Chunk 61: min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "Chunk 62: indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "Chunk 63: constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Chunk 64: Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "Chunk 65: to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "Chunk 66: target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Chunk 67: 6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "Chunk 68: min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "Chunk 69: operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Chunk 70: Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "Chunk 71: be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "Chunk 72: uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "Chunk 73: also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "Chunk 74: (see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "Chunk 75: set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "Chunk 76: constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "Chunk 77: ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "Chunk 78: to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "Chunk 79: dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "Chunk 80: be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "Chunk 81: commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "Chunk 82: +, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "Chunk 83: of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "Chunk 84: U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "Chunk 85: P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Chunk 86: Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "Chunk 87: asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "Chunk 88: Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "Chunk 89: P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "Chunk 90: not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "Chunk 91: indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "Chunk 92: corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Chunk 93: Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Chunk 94: associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "Chunk 95: ∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "Chunk 96: ⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "Chunk 97: min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "Chunk 98: polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "Chunk 99: min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "Chunk 100: one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "Chunk 101: measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "Chunk 102: problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "Chunk 103: solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Chunk 104: Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "Chunk 105: permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "Chunk 106: minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Chunk 107: ⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "Chunk 108: scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "Chunk 109: coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "Chunk 110: couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Chunk 111: Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "Chunk 112: space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "Chunk 113: π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "Chunk 114: U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Chunk 115: Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "Chunk 116: measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "Chunk 117: α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "Chunk 118: π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "Chunk 119: and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "Chunk 120: involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "Chunk 121: weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "Chunk 122: 9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "Chunk 123: is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Chunk 124: to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "Chunk 125: and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "Chunk 126: understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "Chunk 127: measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "Chunk 128: is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "Chunk 129: to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "Chunk 130: i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "Chunk 131: Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Chunk 132: Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Chunk 133: ∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Chunk 134: Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "Chunk 135: elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "Chunk 136: a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "Chunk 137: To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "Chunk 138: gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "Chunk 139: the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "Chunk 140: and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "Chunk 141: S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "Chunk 142: because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "Chunk 143: ⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "Chunk 144: ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "Chunk 145: 1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "Chunk 146: inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Chunk 147: Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Chunk 148: (ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Chunk 149: Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Chunk 150: ∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "Chunk 151: between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "Chunk 152: weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "Chunk 153: spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "Chunk 154: are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "Chunk 155: with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "Chunk 156: p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "Chunk 157: Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Chunk 158: 11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "Chunk 159: the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "Chunk 160: convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Chunk 161: Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "Chunk 162: e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "Chunk 163: by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Chunk 164: Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "Chunk 165: as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "Chunk 166: pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "Chunk 167: exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "Chunk 168: the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "Chunk 169: which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "Chunk 170: OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "Chunk 171: naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "Chunk 172: following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "Chunk 173: Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Chunk 174: Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "Chunk 175: grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "Chunk 176: is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "Chunk 177: min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "Chunk 178: program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "Chunk 179: −∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "Chunk 180: transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "Chunk 181: are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "Chunk 182: against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "Chunk 183: Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Chunk 184: Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "Chunk 185: (fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Chunk 186: Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "Chunk 187: trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "Chunk 188: machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Chunk 189: Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Chunk 190: Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Chunk 191: are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "Chunk 192: measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Chunk 193: Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "Chunk 194: ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Chunk 195: 2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "Chunk 196: ||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            "Chunk 197: (ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "Chunk 198: 2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "Chunk 199: also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "Chunk 200: minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "Chunk 201: min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "Chunk 202: twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Chunk 203: Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "Chunk 204: such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "Chunk 205: y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "Chunk 206: diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "Chunk 207: This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "Chunk 208: and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "Chunk 209: of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "Chunk 210: problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Chunk 211: Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "Chunk 212: be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "Chunk 213: functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "Chunk 214: functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Chunk 215: Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "Chunk 216: be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "Chunk 217: thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "Chunk 218: strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "Chunk 219: constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "Chunk 220: det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "Chunk 221: non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "Chunk 222: det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Chunk 223: Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Chunk 224: Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "Chunk 225: the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "Chunk 226: discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "Chunk 227: 14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Chunk 228: Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "Chunk 229: yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "Chunk 230: y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "Chunk 231: αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "Chunk 232: might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "Chunk 233: remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "Chunk 234: with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "Chunk 235: discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "Chunk 236: circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "Chunk 237: of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "Chunk 238: ∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "Chunk 239: ∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "Chunk 240: α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "Chunk 241: with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "Chunk 242: metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "Chunk 243: geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "Chunk 244: W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "Chunk 245: T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "Chunk 246: function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Chunk 247: T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "Chunk 248: interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Chunk 249: Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "Chunk 250: where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "Chunk 251: since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "Chunk 252: α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "Chunk 253: functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "Chunk 254: thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "Chunk 255: 16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "Chunk 256: 2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Chunk 257: pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "Chunk 258: 1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "Chunk 259: With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "Chunk 260: W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "Chunk 261: Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "Chunk 262: B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "Chunk 263: B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "Chunk 264: Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "Chunk 265: 1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "Chunk 266: of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "Chunk 267: the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "Chunk 268: The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "Chunk 269: that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "Chunk 270: execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "Chunk 271: and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Chunk 272: argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "Chunk 273: H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "Chunk 274: 0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Chunk 275: Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "Chunk 276: Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "Chunk 277: to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "Chunk 278: transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "Chunk 279: solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "Chunk 280: to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "Chunk 281: that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Chunk 282: more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "Chunk 283: can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "Chunk 284: from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "Chunk 285: triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "Chunk 286: problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "Chunk 287: with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "Chunk 288: so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "Chunk 289: ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "Chunk 290: By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "Chunk 291: 0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Chunk 292: Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "Chunk 293: n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Chunk 294: betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "Chunk 295: P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "Chunk 296: 0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Chunk 297: 0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "Chunk 298: transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "Chunk 299: coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "Chunk 300: two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "Chunk 301: performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "Chunk 302: shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "Chunk 303: becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "Chunk 304: turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Chunk 305: Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "Chunk 306: i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Chunk 307: Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Chunk 308: U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "Chunk 309: by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "Chunk 310: Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "Chunk 311: KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "Chunk 312: dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Chunk 313: plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "Chunk 314: εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Chunk 315: Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "Chunk 316: details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Chunk 317: the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "Chunk 318: which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Chunk 319: Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Chunk 320: +×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Chunk 321: reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "Chunk 322: ∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "Chunk 323: which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "Chunk 324: The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "Chunk 325: matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "Chunk 326: diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "Chunk 327: times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "Chunk 328: community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "Chunk 329: these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Chunk 330: Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "Chunk 331: Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "Chunk 332: vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "Chunk 333: 20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Chunk 334: Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "Chunk 335: ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "Chunk 336: so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "Chunk 337: a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "Chunk 338: the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "Chunk 339: diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Chunk 340: Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "Chunk 341: a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "Chunk 342: C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "Chunk 343: These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "Chunk 344: and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "Chunk 345: multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Chunk 346: Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "Chunk 347: +,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "Chunk 348: +,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "Chunk 349: triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "Chunk 350: +,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "Chunk 351: theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "Chunk 352: proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "Chunk 353: +,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Chunk 354: η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "Chunk 355: show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "Chunk 356: One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "Chunk 357: ∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "Chunk 358: dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "Chunk 359: Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "Chunk 360: ⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "Chunk 361: of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "Chunk 362: ∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "Chunk 363: degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Chunk 364: Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "Chunk 365: convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Chunk 366: of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "Chunk 367: Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Chunk 368: (u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "Chunk 369: and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "Chunk 370: LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "Chunk 371: The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "Chunk 372: =−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Chunk 373: are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Chunk 374: sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "Chunk 375: is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "Chunk 376: potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "Chunk 377: usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Chunk 378: Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "Chunk 379: unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "Chunk 380: update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Chunk 381: ∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "Chunk 382: updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Chunk 383: , (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "Chunk 384: dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Chunk 385: (f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "Chunk 386: using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Chunk 387: coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "Chunk 388: diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "Chunk 389: j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "Chunk 390: (C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "Chunk 391: now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Chunk 392: i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "Chunk 393: lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Chunk 394: g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "Chunk 395: because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Chunk 396: regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "Chunk 397: tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "Chunk 398: minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "Chunk 399: previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "Chunk 400: where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "Chunk 401: arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "Chunk 402: εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "Chunk 403: therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "Chunk 404: In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "Chunk 405: 24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "Chunk 406: min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "Chunk 407: barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "Chunk 408: solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "Chunk 409: in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "Chunk 410: has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "Chunk 411: The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "Chunk 412: min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "Chunk 413: can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "Chunk 414: the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "Chunk 415: then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "Chunk 416: barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "Chunk 417: solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Chunk 418: ∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "Chunk 419: approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "Chunk 420: using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "Chunk 421: One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "Chunk 422: min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "Chunk 423: descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "Chunk 424: 25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "Chunk 425: but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "Chunk 426: sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "Chunk 427: the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "Chunk 428: this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "Chunk 429: generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "Chunk 430: and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "Chunk 431: where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "Chunk 432: problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "Chunk 433: (fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Chunk 434: ⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Chunk 435: sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "Chunk 436: ⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "Chunk 437: max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Chunk 438: KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "Chunk 439: (λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "Chunk 440: which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "Chunk 441: ∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Chunk 442: Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "Chunk 443: form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Chunk 444: to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "Chunk 445: of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "Chunk 446: the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Chunk 447: Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "Chunk 448: distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Chunk 449: Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "Chunk 450: term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Chunk 451: min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "Chunk 452: in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "Chunk 453: solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "Chunk 454: or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "Chunk 455: as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "Chunk 456: i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "Chunk 457: corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "Chunk 458: a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "Chunk 459: A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "Chunk 460: some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "Chunk 461: min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "Chunk 462: ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "Chunk 463: min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "Chunk 464: samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "Chunk 465: However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "Chunk 466: (so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "Chunk 467: the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "Chunk 468: in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "Chunk 469: models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "Chunk 470: αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "Chunk 471: that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "Chunk 472: singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "Chunk 473: is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "Chunk 474: (zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "Chunk 475: L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "Chunk 476: setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "Chunk 477: solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "Chunk 478: The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Chunk 479: was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "Chunk 480: thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "Chunk 481: these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "Chunk 482: namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "Chunk 483: between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "Chunk 484: GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "Chunk 485: full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "Chunk 486: for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "Chunk 487: metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "Chunk 488: up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "Chunk 489: in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "Chunk 490: in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Chunk 491: Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Chunk 492: Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "Chunk 493: metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "Chunk 494: GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "Chunk 495: (αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "Chunk 496: thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "Chunk 497: (X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "Chunk 498: 0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "Chunk 499: spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "Chunk 500: spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "Chunk 501: spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "Chunk 502: detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "Chunk 503: min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "Chunk 504: iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Chunk 505: Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "Chunk 506: P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "Chunk 507: iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "Chunk 508: 30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "Chunk 509: LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "Chunk 510: and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "Chunk 511: in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "Chunk 512: [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "Chunk 513: [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "Chunk 514: [6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "Chunk 515: 20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "Chunk 516: duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "Chunk 517: recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "Chunk 518: Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "Chunk 519: on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "Chunk 520: [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "Chunk 521: Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "Chunk 522: with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "Chunk 523: Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "Chunk 524: 375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "Chunk 525: Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "Chunk 526: Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "Chunk 527: [18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "Chunk 528: [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "Chunk 529: 1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "Chunk 530: [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "Chunk 531: [22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "Chunk 532: D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "Chunk 533: Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "Chunk 534: 27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "Chunk 535: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"LatexTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qzCTCiPpLKn",
        "outputId": "1f28e936-4fed-4a2b-d6e8-f03b4999237b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14,\n",
            "Chunk 2: 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability\n",
            "Chunk 3: any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at\n",
            "Chunk 4: reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if,\n",
            "Chunk 5: as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures)\n",
            "Chunk 6: ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be\n",
            "Chunk 7: the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped\n",
            "Chunk 8: deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted\n",
            "Chunk 9: against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the\n",
            "Chunk 10: measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x)\n",
            "Chunk 11: ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be\n",
            "Chunk 12: of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at\n",
            "Chunk 13: also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth\n",
            "Chunk 14: than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability\n",
            "Chunk 15: positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the\n",
            "Chunk 16: 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic\n",
            "Chunk 17: 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and\n",
            "Chunk 18: ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to\n",
            "Chunk 19: using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward\n",
            "Chunk 20: continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the\n",
            "Chunk 21: in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a\n",
            "Chunk 22: a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y ,\n",
            "Chunk 23: follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one\n",
            "Chunk 24: for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be\n",
            "Chunk 25: a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability\n",
            "Chunk 26: extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain\n",
            "Chunk 27: of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear\n",
            "Chunk 28: a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the\n",
            "Chunk 29: for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities\n",
            "Chunk 30: shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the\n",
            "Chunk 31: the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward\n",
            "Chunk 32: of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the\n",
            "Chunk 33: T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and\n",
            "Chunk 34: deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if\n",
            "Chunk 35: is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to\n",
            "Chunk 36: explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and\n",
            "Chunk 37: the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random\n",
            "Chunk 38: as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the\n",
            "Chunk 39: space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y ,\n",
            "Chunk 40: Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random\n",
            "Chunk 41: thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in\n",
            "Chunk 42: variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the\n",
            "Chunk 43: cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One\n",
            "Chunk 44: solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for\n",
            "Chunk 45: set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there\n",
            "Chunk 46: problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of\n",
            "Chunk 47: set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either\n",
            "Chunk 48: measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The\n",
            "Chunk 49: blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas\n",
            "Chunk 50: is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance\n",
            "Chunk 51: several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in\n",
            "Chunk 52: square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj\n",
            "Chunk 53: discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β,\n",
            "Chunk 54: must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This\n",
            "Chunk 55: we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ".\n",
            "Chunk 56: points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the\n",
            "Chunk 57: using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass\n",
            "Chunk 58: are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5)\n",
            "Chunk 59: equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to\n",
            "Chunk 60: not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source\n",
            "Chunk 61: measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is\n",
            "Chunk 62: no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x)\n",
            "Chunk 63: map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch\n",
            "Chunk 64: the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the\n",
            "Chunk 65: when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization\n",
            "Chunk 66: clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be\n",
            "Chunk 67: maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment\n",
            "Chunk 68: end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass\n",
            "Chunk 69: all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete\n",
            "Chunk 70: formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or\n",
            "Chunk 71: point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several\n",
            "Chunk 72: point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”)\n",
            "Chunk 73: consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in\n",
            "Chunk 74: targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or\n",
            "Chunk 75: ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge\n",
            "Chunk 76: admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand\n",
            "Chunk 77: matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a\n",
            "Chunk 78: therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric,\n",
            "Chunk 79: plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal\n",
            "Chunk 80: and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the\n",
            "Chunk 81: program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black\n",
            "Chunk 82: optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the\n",
            "Chunk 83: Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a\n",
            "Chunk 84: point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor\n",
            "Chunk 85: For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which\n",
            "Chunk 86: that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation\n",
            "Chunk 87: Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the\n",
            "Chunk 88: permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a\n",
            "Chunk 89: whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in\n",
            "Chunk 90: proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures\n",
            "Chunk 91: problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of\n",
            "Chunk 92: Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution\n",
            "Chunk 93: then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that\n",
            "Chunk 94: (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7]\n",
            "Chunk 95: theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of\n",
            "Chunk 96: if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β)\n",
            "Chunk 97: of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous”\n",
            "Chunk 98: setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black).\n",
            "Chunk 99: of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at\n",
            "Chunk 100: entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary\n",
            "Chunk 101: of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one\n",
            "Chunk 102: discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as\n",
            "Chunk 103: constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition\n",
            "Chunk 104: the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures\n",
            "Chunk 105: for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The\n",
            "Chunk 106: =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over\n",
            "Chunk 107: is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D\n",
            "Chunk 108: Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so\n",
            "Chunk 109: a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7:\n",
            "Chunk 110: c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-*\n",
            "Chunk 111: and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein\n",
            "Chunk 112: impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes\n",
            "Chunk 113: measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram\n",
            "Chunk 114: between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs\n",
            "Chunk 115: matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance\n",
            "Chunk 116: states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on\n",
            "Chunk 117: (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that\n",
            "Chunk 118: LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle\n",
            "Chunk 119: if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null\n",
            "Chunk 120: distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0\n",
            "Chunk 121: of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is\n",
            "Chunk 122: the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing\n",
            "Chunk 123: measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is\n",
            "Chunk 124: explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and\n",
            "Chunk 125: bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c)\n",
            "Chunk 126: remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0\n",
            "Chunk 127: that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c)\n",
            "Chunk 128: =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/\n",
            "Chunk 129: jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "=\n",
            "Chunk 130: Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2\n",
            "Chunk 131: comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis\n",
            "Chunk 132: and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y)\n",
            "Chunk 133: and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0\n",
            "Chunk 134: on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as\n",
            "Chunk 135: The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein\n",
            "Chunk 136: between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for\n",
            "Chunk 137: allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or\n",
            "Chunk 138: In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base\n",
            "Chunk 139: measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y).\n",
            "Chunk 140: one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily (\n",
            "Chunk 141: feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now\n",
            "Chunk 142: a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function\n",
            "Chunk 143: ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be\n",
            "Chunk 144: vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists\n",
            "Chunk 145: unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k)\n",
            "Chunk 146: norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of\n",
            "Chunk 147: one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X),\n",
            "Chunk 148: is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear\n",
            "Chunk 149: it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the\n",
            "Chunk 150: option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being\n",
            "Chunk 151: optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is\n",
            "Chunk 152: 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11)\n",
            "Chunk 153: Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization\n",
            "Chunk 154: which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal\n",
            "Chunk 155: explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials\n",
            "Chunk 156: (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for\n",
            "Chunk 157: the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L\n",
            "Chunk 158: side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to\n",
            "Chunk 159: of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the\n",
            "Chunk 160: always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking\n",
            "Chunk 161: min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian\n",
            "Chunk 162: optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to\n",
            "Chunk 163: (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be\n",
            "Chunk 164: with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4\n",
            "Chunk 165: proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual\n",
            "Chunk 166: (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich\n",
            "Chunk 167: functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The\n",
            "Chunk 168: potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}.\n",
            "Chunk 169: as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is\n",
            "Chunk 170: non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal (\n",
            "Chunk 171: Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following\n",
            "Chunk 172: Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are\n",
            "Chunk 173: density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith\n",
            "Chunk 174: inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge\n",
            "Chunk 175: and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as\n",
            "Chunk 176: this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related\n",
            "Chunk 177: that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance\n",
            "Chunk 178: the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus\n",
            "Chunk 179: Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation\n",
            "Chunk 180: ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x).\n",
            "Chunk 181: by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27)\n",
            "Chunk 182: ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint\n",
            "Chunk 183: ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice,\n",
            "Chunk 184: costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported\n",
            "Chunk 185: (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition\n",
            "Chunk 186: Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This\n",
            "Chunk 187: is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows\n",
            "Chunk 188: that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is\n",
            "Chunk 189: relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is\n",
            "Chunk 190: the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map\n",
            "Chunk 191: theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of\n",
            "Chunk 192: is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions,\n",
            "Chunk 193: deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The\n",
            "Chunk 194: theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1\n",
            "Chunk 195: sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using\n",
            "Chunk 196: function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type\n",
            "Chunk 197: which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as\n",
            "Chunk 198: ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for\n",
            "Chunk 199: the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special\n",
            "Chunk 200: for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is\n",
            "Chunk 201: where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when\n",
            "Chunk 202: zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result\n",
            "Chunk 203: One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to\n",
            "Chunk 204: two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical\n",
            "Chunk 205: Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸=\n",
            "Chunk 206: rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without\n",
            "Chunk 207: assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes\n",
            "Chunk 208: (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those\n",
            "Chunk 209: locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below.\n",
            "Chunk 210: of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone\n",
            "Chunk 211: of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?].\n",
            "Chunk 212: compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?],\n",
            "Chunk 213: transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative\n",
            "Chunk 214: .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞}\n",
            "Chunk 215: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one\n",
            "Chunk 216: quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a\n",
            "Chunk 217: the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the\n",
            "Chunk 218: real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in\n",
            "Chunk 219: which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx.\n",
            "Chunk 220: (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9:\n",
            "Chunk 221: ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm\n",
            "Chunk 222: detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates\n",
            "Chunk 223: by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??.\n",
            "Chunk 224: as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians)\n",
            "Chunk 225: Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα),\n",
            "Chunk 226: can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is\n",
            "Chunk 227: that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "=\n",
            "Chunk 228: det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover\n",
            "Chunk 229: detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [\n",
            "Chunk 230: x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4\n",
            "Chunk 231: illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance\n",
            "Chunk 232: densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the\n",
            "Chunk 233: random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting\n",
            "Chunk 234: interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving\n",
            "Chunk 235: additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?]\n",
            "Chunk 236: the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that\n",
            "Chunk 237: Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ=\n",
            "Chunk 238: arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the\n",
            "Chunk 239: W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5\n",
            "Chunk 240: of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many\n",
            "Chunk 241: formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages,\n",
            "Chunk 242: regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme\n",
            "Chunk 243: simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance\n",
            "Chunk 244: of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on\n",
            "Chunk 245: of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned\n",
            "Chunk 246: discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or\n",
            "Chunk 247: that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of\n",
            "Chunk 248: The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.=\n",
            "Chunk 249: original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to\n",
            "Chunk 250: 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns\n",
            "Chunk 251: theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to\n",
            "Chunk 252: are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation\n",
            "Chunk 253: balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the\n",
            "Chunk 254: 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution\n",
            "Chunk 255: how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the\n",
            "Chunk 256: of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been\n",
            "Chunk 257: solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the\n",
            "Chunk 258: optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in\n",
            "Chunk 259: ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution\n",
            "Chunk 260: thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is\n",
            "Chunk 261: of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one\n",
            "Chunk 262: optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating\n",
            "Chunk 263: and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal\n",
            "Chunk 264: number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that\n",
            "Chunk 265: the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is\n",
            "Chunk 266: shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is\n",
            "Chunk 267: of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp\n",
            "Chunk 268: entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b,\n",
            "Chunk 269: entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed\n",
            "Chunk 270: reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key\n",
            "Chunk 271: the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which\n",
            "Chunk 272: larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in\n",
            "Chunk 273: to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof\n",
            "Chunk 274: (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε=\n",
            "Chunk 275: one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete\n",
            "Chunk 276: arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14)\n",
            "Chunk 277: and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler\n",
            "Chunk 278: a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a\n",
            "Chunk 279: by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL(\n",
            "Chunk 280: to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the\n",
            "Chunk 281: problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was\n",
            "Chunk 282: Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to\n",
            "Chunk 283: converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two\n",
            "Chunk 284: the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using\n",
            "Chunk 285: a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but\n",
            "Chunk 286: that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable\n",
            "Chunk 287: (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g)\n",
            "Chunk 288: the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to\n",
            "Chunk 289: results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors\n",
            "Chunk 290: in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust\n",
            "Chunk 291: in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and\n",
            "Chunk 292: inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times\n",
            "Chunk 293: v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as\n",
            "Chunk 294: is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by\n",
            "Chunk 295: equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s\n",
            "Chunk 296: side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between\n",
            "Chunk 297: v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000\n",
            "Chunk 298: lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D\n",
            "Chunk 299: at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for\n",
            "Chunk 300: violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these\n",
            "Chunk 301: anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same\n",
            "Chunk 302: contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It\n",
            "Chunk 303: computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with\n",
            "Chunk 304: away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use\n",
            "Chunk 305: one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these\n",
            "Chunk 306: (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since\n",
            "Chunk 307: equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice\n",
            "Chunk 308: def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be\n",
            "Chunk 309: against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly\n",
            "Chunk 310: convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be\n",
            "Chunk 311: log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”).\n",
            "Chunk 312: up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance\n",
            "Chunk 313: projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?]\n",
            "Chunk 314: It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s\n",
            "Chunk 315: linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive\n",
            "Chunk 316: is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.=\n",
            "Chunk 317: max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3.\n",
            "Chunk 318: convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also\n",
            "Chunk 319: dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one\n",
            "Chunk 320: P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one\n",
            "Chunk 321: One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2.\n",
            "Chunk 322: we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ))\n",
            "Chunk 323: triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ))\n",
            "Chunk 324: gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal\n",
            "Chunk 325: shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row,\n",
            "Chunk 326: monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly\n",
            "Chunk 327: These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??).\n",
            "Chunk 328: the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling\n",
            "Chunk 329: increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One\n",
            "Chunk 330: problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) =\n",
            "Chunk 331: scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers\n",
            "Chunk 332: optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of\n",
            "Chunk 333: of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε,\n",
            "Chunk 334: (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of\n",
            "Chunk 335: be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in\n",
            "Chunk 336: term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input\n",
            "Chunk 337: For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a\n",
            "Chunk 338: dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of\n",
            "Chunk 339: existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and\n",
            "Chunk 340: OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark\n",
            "Chunk 341: iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an\n",
            "Chunk 342: maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can\n",
            "Chunk 343: to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block\n",
            "Chunk 344: (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0),\n",
            "Chunk 345: starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations\n",
            "Chunk 346: equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark\n",
            "Chunk 347: =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we\n",
            "Chunk 348: notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min\n",
            "Chunk 349: to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i=\n",
            "Chunk 350: (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of\n",
            "Chunk 351: the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the\n",
            "Chunk 352: as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we\n",
            "Chunk 353: or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform\n",
            "Chunk 354: are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)=\n",
            "Chunk 355: Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate\n",
            "Chunk 356: anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While\n",
            "Chunk 357: (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid\n",
            "Chunk 358: use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting\n",
            "Chunk 359: (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized\n",
            "Chunk 360: scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g)\n",
            "Chunk 361: (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity\n",
            "Chunk 362: stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis\n",
            "Chunk 363: at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no\n",
            "Chunk 364: soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is\n",
            "Chunk 365: Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6\n",
            "Chunk 366: the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by\n",
            "Chunk 367: λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all\n",
            "Chunk 368: A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter\n",
            "Chunk 369: barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input\n",
            "Chunk 370: for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a\n",
            "Chunk 371: as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween\n",
            "Chunk 372: one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids\n",
            "Chunk 373: this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark\n",
            "Chunk 374: as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem\n",
            "Chunk 375: on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this\n",
            "Chunk 376: of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary\n",
            "Chunk 377: of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the\n",
            "Chunk 378: ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of\n",
            "Chunk 379: the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input\n",
            "Chunk 380: optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a\n",
            "Chunk 381: note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs)\n",
            "Chunk 382: of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent\n",
            "Chunk 383: [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some\n",
            "Chunk 384: on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks)\n",
            "Chunk 385: KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings\n",
            "Chunk 386: encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can\n",
            "Chunk 387: corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form\n",
            "Chunk 388: ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s,\n",
            "Chunk 389: updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to\n",
            "Chunk 390: alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal\n",
            "Chunk 391: following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the\n",
            "Chunk 392: of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads\n",
            "Chunk 393: Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and\n",
            "Chunk 394: so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0\n",
            "Chunk 395: minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre\n",
            "Chunk 396: ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The\n",
            "Chunk 397: shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of\n",
            "Chunk 398: that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.=\n",
            "Chunk 399: one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while\n",
            "Chunk 400: (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77)\n",
            "Chunk 401: all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters\n",
            "Chunk 402: Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat\n",
            "Chunk 403: are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In\n",
            "Chunk 404: and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized\n",
            "Chunk 405: measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in\n",
            "Chunk 406: through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing\n",
            "Chunk 407: a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem\n",
            "Chunk 408: problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic\n",
            "Chunk 409: usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two\n",
            "Chunk 410: Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram,\n",
            "Chunk 411: when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin\n",
            "Chunk 412: a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem\n",
            "Chunk 413: one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16:\n",
            "Chunk 414: in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting.\n",
            "Chunk 415: statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical\n",
            "Chunk 416: model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see\n",
            "Chunk 417: and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum\n",
            "Chunk 418: any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a\n",
            "Chunk 419: to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal\n",
            "Chunk 420: MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not\n",
            "Chunk 421: distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be\n",
            "Chunk 422: share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to\n",
            "Chunk 423: the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is\n",
            "Chunk 424: where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually\n",
            "Chunk 425: introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not\n",
            "Chunk 426: resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from\n",
            "Chunk 427: intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics\n",
            "Chunk 428: diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ".\n",
            "Chunk 429: +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the\n",
            "Chunk 430: deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter\n",
            "Chunk 431: optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc,\n",
            "Chunk 432: class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground\n",
            "Chunk 433: Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces\n",
            "Chunk 434: space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices\n",
            "Chunk 435: that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are\n",
            "Chunk 436: A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.=\n",
            "Chunk 437: problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP)\n",
            "Chunk 438: be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular\n",
            "Chunk 439: to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability\n",
            "Chunk 440: between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in\n",
            "Chunk 441: This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for\n",
            "Chunk 442: gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have\n",
            "Chunk 443: distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric\n",
            "Chunk 444: corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One\n",
            "Chunk 445: are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up\n",
            "Chunk 446: a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark\n",
            "Chunk 447: thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows\n",
            "Chunk 448: (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for\n",
            "Chunk 449: 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze\n",
            "Chunk 450: formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over\n",
            "Chunk 451: because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the\n",
            "Chunk 452: while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization\n",
            "Chunk 453: of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy\n",
            "Chunk 454: (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later\n",
            "Chunk 455: [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85).\n",
            "Chunk 456: compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where\n",
            "Chunk 457: def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations\n",
            "Chunk 458: can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1]\n",
            "Chunk 459: soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric\n",
            "Chunk 460: SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers.\n",
            "Chunk 461: the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university\n",
            "Chunk 462: Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on\n",
            "Chunk 463: with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale\n",
            "Chunk 464: discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis.\n",
            "Chunk 465: and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image\n",
            "Chunk 466: An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An\n",
            "Chunk 467: 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic\n",
            "Chunk 468: S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´\n",
            "Chunk 469: G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale\n",
            "Chunk 470: forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a\n",
            "Chunk 471: algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet\n",
            "Chunk 472: Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems ,\n",
            "Chunk 473: Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE\n",
            "Chunk 474: for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser\n",
            "Chunk 475: to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal\n",
            "Chunk 476: press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal\n",
            "Chunk 477: and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr`\n",
            "Chunk 478: 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale\n",
            "Chunk 479: and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi.\n",
            "Chunk 480: 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer,\n",
            "Chunk 481: Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of\n",
            "Chunk 482: C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal\n",
            "Chunk 483: and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50,splitter_type = \"SentenceTransformersTokenTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "472b8dde15854859bbc4af591e858084",
            "05e1b47e3482441ba3d50693e6a665dd",
            "4c819ea9d39143ea842397ac94fc43c0",
            "5322d2de574448798c705ae2c36d5c3d",
            "630f701802d645d180b74cab4c086fd8",
            "fea258c0abed4bc3a186505044170ffa",
            "e38206659c434e68b805f64a8222b8d5",
            "4f7e570672ea4dc880b3afdbaab03873",
            "cf933b37a3694da5b7ec907d45c26711",
            "590a9ac405c449168b9b7021582cf4f6",
            "f83e6d22d7c043aa9c1efc47458b0f75",
            "f2e5f05dc94b46c6ac5a616f38549a0a",
            "ee5f29a6f7f244db984fac68c95ef7c9",
            "12be3ecf41c344a79acbbce16acb7dd7",
            "97070a15fd43444c9f9fcc64df40a995",
            "7d04c62dcb7e42808c4363940806a9f8",
            "1b6bf1e44abd4b6396c5d15625169d59",
            "26569987c1bf4dc8b33a5682cd9954ca",
            "ce3a25972c904aa7b4d7ef35f29f61af",
            "03cfa33af4484441b5b3ea24d55659d3",
            "344758b77bfd4ec7ba8b44ab59cbe51f",
            "5886be9828b34a04be9cd4e92268ca45",
            "c0586b78bed3471792a520fa16ee1f66",
            "0a53ca2675104eab8e4e61a59d008f8a",
            "1ce822c8dad14a7d8d51fda8a97ab755",
            "0c3e269ccc71452ab21334c62f25a60d",
            "704bc1afb7334972a3966d6657ce3de8",
            "4aea488488c94c8b86c2ed6790b85987",
            "bdf682d4da9640c4a7ec542bbd2c83da",
            "4bd7b3c5794040b9a2cc12a2af38378c",
            "92c631f688d64f12a188f3a3ef625a7c",
            "2a0706645eb24d0681258ccd8ed641b0",
            "64ae95fcc5f54b72bfdaef1847f77a54",
            "ad98a20f9020476b9dbf23386be43749",
            "28afe4da2e7c4315a7fe7f803cb360b3",
            "058ceac08b6c4b9f9a3580a45092236e",
            "3403815bb876441785001480b387c19c",
            "4e703aeb1c6b41a9961c2c3e9c2d38be",
            "efc646cec3bc44d69f87692ccb27a062",
            "11692cd012a74363ac61429ed563e86b",
            "e143594d2a6b45fcb6705c3a7361a8ec",
            "e0e0e19b729d45178d39608033e37e1d",
            "769b0421883742308b2a499d7a350b95",
            "030b5491929f449a984370d26122a9fc",
            "dd537fe15bf546f1b062db96f4ae0d42",
            "e3c68e6ae578406a88eb1e002c0fb76b",
            "2a63691ede2a4132883807253c82564c",
            "664feaee5538452c9ed0b84614271ac6",
            "183146562aea4bb1b200ecfffd45c266",
            "c5c9176b7f2140788aa53a9700c7a73d",
            "6472c94fc12b4453a5bb9bd19295e28f",
            "9136940003044495a4e8cb42e1b73d57",
            "d67abdbe00024c44a2d440e5a4eaf218",
            "414518164a7d45879f53681b089a1d37",
            "61927b33724245aca9d81d8d37eaf21a",
            "599873d761b841e98e87f34667b78109",
            "70c59365738d49588aab9eb3fa99e9ce",
            "74e6894faedd4185a72fe556d3b61e10",
            "7e43c4893a3a403d857929c1f5c51dc7",
            "2c8c1fb42c1148fa9ed89d734ab2315f",
            "e4cf3cf58f3c419f8b18d5153b8abcc6",
            "d4573d23b8f54d1c9f9ea9fd7822c59a",
            "a8a290a2e3e14a1587e904c50fcdec67",
            "c1e65b1fc9c849078fd0510b68c53127",
            "b00513f7ca6e4245879d2b59d99820f1",
            "b2a027798d404867936004dc7d7696d1",
            "5e831c0576fa4e189f9951773cf2d3c8",
            "1987c99a028d420eb132db9840881646",
            "941fa65d1e3a43b7b868b3f6a9c4ea65",
            "2806c86010364395b82e19ae86bab50e",
            "c40f60633e424ebf8bbf836f3cea65c7",
            "f3cd4653eef7416982550345b77447aa",
            "adba2fd0e9794dfc82fad143f5a9dc02",
            "b85d8e43f72844cea254e0fc52e2acd5",
            "b12948adf89f41448e3ee38b8386cc7a",
            "92b18118fc8248908111cde8454e745e",
            "0387fb9639604a8a93babef107e0e18b",
            "cb00ed4c2600460c99d2160f87563b2c",
            "4ab504b7d11b4433953a6cb58e3b307b",
            "04c99555278a42378162badf62208a0d",
            "318d894653154a90ae303c62efe399e5",
            "65ae9e9e35614fbaaa814e48eb1f79af",
            "5307120e5a4c402dbf60b9d9c8a5278b",
            "e4762b6e7efc4bc3b41982356714e3af",
            "b59345fc3a9e474ca0aec019e2f22b64",
            "4cb07fd811764444909d53a45e11461d",
            "e9036e855a364f75b4b4f20e3df25349",
            "14888b09eba248d39f6d3fb5e9232c4e",
            "e9f4957d5ebf4f4290462facae7b6786",
            "f4c717eb4b49497ab7cdbd41a20a2e68",
            "980737f5f9cc4ca7a70cc41ed04db7db",
            "dc1687355e9140109d14000f7d1f14a4",
            "415f38c7eae94373b921b0fc4d13a918",
            "f7a07624270c4e03974cf60d605a298f",
            "e7724ac974ba481a948aeea2369761ec",
            "181ca6c878b74c0ab55b1f6c5437ae6f",
            "f47ee2e85cca4d839e76477ac6483162",
            "dc8431ce2200434d9317e103e8d8eb02",
            "905f695f77924919a11453c9c6d7d779",
            "3aab2fefe8344ff6aed2d93906e19f0f",
            "53b80488fe9c40aa932fde7c7a2bb03c",
            "cd75a120dae7480faaaa37b8720b76ab",
            "9b12d605c3ee47c6b6c7955644625e5e",
            "f5b8537fc9554e5e91deca0aef0a8f21",
            "7739e316e7dc439396c0361b6a6ee538",
            "c4383ab8e9c347db958b21668c94cc72",
            "6212407dc2ac47faaab77f3a7d6248b6",
            "0470176611b240f5891995eef9535c54",
            "d14061639e2b47bfa5ead43ba1e24b6f",
            "2325175244a24fe48f5addf2712b2445",
            "e402c02948d945a3bd5af1f5ac63cbfb",
            "54e3e12146104de9a00ee1256e56c0ac",
            "b727af3c91a342978a1d882b16eaf298",
            "c2586d98dc3e49f6a67e15391d0f0afa",
            "51103dd4723b49908b7968d6b9a92ee5",
            "217f50140210440d927fcd2a82b26963",
            "d6c73348262946ce914a7a75cc44f7fc",
            "8f856bcd846e418e897691c378e0b837",
            "ffd43af43ea64b7cac2598f628019650",
            "f6813829c0ed46fba16d53ec0988d7d2",
            "8c27c24f0dfd4be28da66eb4d3e25225"
          ]
        },
        "id": "n203jCDsnKIi",
        "outputId": "5fd85235-e9ea-4e5c-f784-82c8ab784dfc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "472b8dde15854859bbc4af591e858084"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2e5f05dc94b46c6ac5a616f38549a0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0586b78bed3471792a520fa16ee1f66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad98a20f9020476b9dbf23386be43749"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd537fe15bf546f1b062db96f4ae0d42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "599873d761b841e98e87f34667b78109"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e831c0576fa4e189f9951773cf2d3c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb00ed4c2600460c99d2160f87563b2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f4957d5ebf4f4290462facae7b6786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aab2fefe8344ff6aed2d93906e19f0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e402c02948d945a3bd5af1f5ac63cbfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: mathematical foundations of data sciences gabriel peyr´ e cnrs & dma ´ecole normale sup´ erieure gabriel. peyre @ ens. fr https : / / mathematical - tours. github. io www. numerical - tours. com august 14, 2019 2 chapter 1 optimal transport 1. 1 radon measures measures. we will interchangeably the term histogram or probability vector for any element a∈σnthat belongs to the probability simplex σndef. = { a∈rn + ; [UNK] i = 1ai = 1 }. a discrete measure with weights aand locations x1,..., xn∈x reads α = [UNK] i = 1aiδxi ( 1. 1 ) whereδxis the dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location x. such as measure describes a probability measure if, additionally, a∈σn, and more generally a positive measure if each of the “ weights ” described in vector ais positive itself. remark 1 ( general measures ). a convenient feature of ot is that it can deal with discrete and continuous “ objects ” within the same framework. such objects only need to be modelled as measures. this corresponds to the notion of radon measures m ( x ) on the spacex. the formal deﬁnition of that set requires that xis equipped with a distance, usually denoted d, because one can only access a measure by “ testing ” ( integrating ) it against continuous functions, denoted f∈c ( x ). integration of f∈c ( x ) against a discrete measure αcomputes a sum [UNK] xf ( x ) dα ( x ) = [UNK] i = 1aif ( xi ). more general measures, for instance on x = rd ( whered∈n∗is the dimension ), can\n",
            "Chunk 2: measure αcomputes a sum [UNK] xf ( x ) dα ( x ) = [UNK] i = 1aif ( xi ). more general measures, for instance on x = rd ( whered∈n∗is the dimension ), can have a density dα ( x ) = ρα ( x ) dxw. r. t. the lebesgue measure, often denoted ρα = dα dx, which means that [UNK] ( rd ), [UNK] rdh ( x ) dα ( x ) = [UNK] rdh ( x ) ρα ( x ) dx. an arbitrary measure α∈m ( x ) ( which needs not to have a density nor be a sum of diracs ) is deﬁned by the fact that it can be integrated agains any continuous function f∈c ( x ) and [UNK] xf ( x ) dα ( x ) ∈r. ifxis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity. measure as thus in some sense “ less regular ” than functions, but more regular than distributions ( which are dual to smooth functions ). for instance, the derivative of a dirac is not a measure. we denote m + ( x ) the set of all positive measures on x. the set of probability measures is denoted m1 + ( x ), which means that anyα∈m1 + ( x ) is positive, and that α ( x ) = [UNK] xdα = 1. figure 1. 1 [UNK] a visualization of the [UNK] classes of measures, beyond histograms, considered in this work. 3 discreted = 1 discrete d = 2 density d = 1 density d = 2 figure 1. 1 : schematic display of discrete distributions α = [UNK] i = 1aiδxi ( red corresponds to empirical uniform distribution ai =\n",
            "Chunk 3: this work. 3 discreted = 1 discrete d = 2 density d = 1 density d = 2 figure 1. 1 : schematic display of discrete distributions α = [UNK] i = 1aiδxi ( red corresponds to empirical uniform distribution ai = 1 / n, and blue to arbitrary distributions ) and densities d α ( x ) = ρα ( x ) dx ( in violet ), in both 1 - d and 2 - d. discrete distributions in 1 - d are displayed using vertical segments ( with length equal to ai ) and in 2 - d using point clouds ( radius equal to ai ). operators on measures. for some continuous map t : x →y, we deﬁne the pushforward operator t♯ : m ( x ) →m ( y ). for discrete measures ( 1. 1 ), the pushforward operation consists simply in moving the positions of all the points in the support of the measure t♯αdef. = [UNK] iaiδt ( xi ). for more general measures, for instance for those with a density, the notion of push - forward plays a funda - mental to describe spatial modiﬁcations of probability measures. the formal deﬁnition reads as follow. deﬁnition 1 ( push - forward ). fort : x → y, the push forward measure β = t♯α∈ m ( y ) of some α∈m ( x ) reads [UNK] ( y ), [UNK] yh ( y ) dβ ( y ) = [UNK] xh ( t ( x ) ) dα ( x ). ( 1. 2 ) equivalently, for any measurable set b⊂y, one has β ( b ) = α ( { x∈x ; t ( x ) ∈b } ). ( 1. 3 ) note thatt♯preserves positivity and total mass,\n",
            "Chunk 4: any measurable set b⊂y, one has β ( b ) = α ( { x∈x ; t ( x ) ∈b } ). ( 1. 3 ) note thatt♯preserves positivity and total mass, so that if α∈m1 + ( x ) thent♯α∈m1 + ( y ). intuitively, a measurable map t : x→y, can be interpreted as a function “ moving ” a single point from a measurable space to another. the more general extension t♯can now “ move ” an entire probability measure onxtowards a new probability measure on y. the operator t♯ “ pushes forward ” each elementary mass of a measureαonxby applying the map tto obtain then an elementary mass in y, to build on aggregate a new measure ony ) writtent♯α. note that such a push - forward t♯ : m1 + ( x ) →m1 + ( y ) is a linear operator between measures in the sense that for two measures α1, α2onx, t♯ ( α1 + α2 ) = t♯α1 + t♯α2. remark 2 ( push - forward for densities ). explicitly doing the change of variable in formula ( 1. 2 ) for measures with densities ( ρα, ρβ ) onrd ( assumingtis smooth and a bijection ) shows that a push - forward acts on densities linearly as a change of variables in the integration formula, indeed ρα ( x ) = | det ( t ′ ( x ) ) | ρβ ( t ( x ) ) ( 1. 4 ) wheret ′ ( x ) ∈rd×dis the jacobian matrix of t ( the matrix formed by taking the gradient of each coordinate oft ). this implies, denoting y = t ( x ) |\n",
            "Chunk 5: ) ) ( 1. 4 ) wheret ′ ( x ) ∈rd×dis the jacobian matrix of t ( the matrix formed by taking the gradient of each coordinate oft ). this implies, denoting y = t ( x ) | det ( t ′ ( x ) ) | = ρα ( x ) ρβ ( y ). 4 = [UNK] ] [UNK]. = pit ( xi ) tt ] gdef. = gtgpush - forward of measures pull - back of functions figure 1. 2 : comparison of push - forward t♯and pull - back t♯. remark 3 ( push - forward vs. pull - back ). the push - forward t♯of measures should not be confounded with the pull - back of function t♯ : c ( y ) →c ( x ) which corresponds to the “ warping ” of functions. it is the linear map deﬁned, for g∈c ( y ) byt♯g = [UNK]. push - forward and pull - back are actually adjoint one from each others, in the sense that [UNK] ( α, g ) ∈m ( x ) ×c ( y ), [UNK] ygd ( t♯α ) = [UNK] x ( t♯g ) dα. it is important to realize that even if ( α, β ) have densities ( ρα, ρβ ), t♯αis not equal to t♯ρβ, because of the presence of the jacobian in ( 1. 4 ). this explains why ot should be used with caution to perform image registration, because it does not operate as an image warping method. figure 1. 2 illustrate the distinction between these push - forward and pull - back operators. remark 4 ( measures and random variables ). radon measures can also be viewed as representing the distri - butions of random variables. a random variable\n",
            "Chunk 6: figure 1. 2 illustrate the distinction between these push - forward and pull - back operators. remark 4 ( measures and random variables ). radon measures can also be viewed as representing the distri - butions of random variables. a random variable xonxis actually a map x : ω→x from some abstract ( often un - speciﬁed ) probabized space ( ω, p ), and its distribution αis the radon measure x∈m1 + ( x ) such thatp ( x∈a ) = α ( a ) = [UNK] adα ( x ). equivalently, it is the push - forward of pbyx, α = x♯p. applying another push - forward β = t♯αfort : x →y, following ( 1. 2 ), is equivalent to deﬁning another random variabley = t ( x ) : ω∈ω→t ( x ( ω ) ) ∈y, so thatβis the distribution of y. drawing a random sample yfromyis thus simply achieved by computing y = t ( x ) wherexis drawn from x. convergence of random variable. convergence of random variable ( in probability, almost sure, in law ), convergence of measures ( strong, weak ). 1. 2 monge problem given a cost matrix ( ci, j ) i∈jnk, j∈jmk, assuming n = m, the optimal assignment problem seeks for a bijectionσin the set perm ( n ) of permutations of nelements solving min σ∈perm ( n ) 1 [UNK] i = 1ci, σ ( i ). ( 1. 5 ) one could naively evaluate the cost function above using all permutations in the set perm ( n ). however, that set has size n!, which is gigantic even for small n. consider\n",
            "Chunk 7: ( i ). ( 1. 5 ) one could naively evaluate the cost function above using all permutations in the set perm ( n ). however, that set has size n!, which is gigantic even for small n. consider for instance that such a set has more than 10100elements [? ] whennis as small as 70. that problem can therefore only be solved if there exist [UNK] algorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5 x1x2y1y2x1x2y1y2x4x5x6x3y3x7figure 1. 3 : ( left ) blue dots from measure αand red dots from measure βare pairwise equidistant. hence, either matching σ = ( 1, 2 ) ( full line ) or σ = ( 2, 1 ) ( dotted line ) is optimal. ( right ) a monge map can associate the blue measure αto the red measure β. the weights αiare displayed proportionally to the area of the disk marked at each location. the mapping here is such that t ( x1 ) = t ( x2 ) = y2, t ( x3 ) = y3, whereas for [UNK] we havet ( xi ) = y1. remark 5 ( uniqueness ). note that the optimal assignment problem may have several optimal solutions. suppose for instance that n = m = 2 and that the matrix cis the pairwise distance matrix between the 4 corners of a 2 - dimensional square of side length 1, as represented in the left plot in figure 1. 3. in that case only two assignments exist, and they share the same cost. for discrete measures α = [UNK] i = 1aiδxiandβ = [UNK] j = 1bjδyj (\n",
            "Chunk 8: represented in the left plot in figure 1. 3. in that case only two assignments exist, and they share the same cost. for discrete measures α = [UNK] i = 1aiδxiandβ = [UNK] j = 1bjδyj ( 1. 6 ) the monge problem [? ] seeks for a map that associates to each point xia single point yj, and which must push the mass of αtoward the mass of β, which is to say that such a map t : { x1,..., xn } → { y1,..., ym } must verify that [UNK], bj = [UNK] i : t ( xi ) = yjai ( 1. 7 ) which we write in compact form as t♯α = β. this map should minimize some transportation cost, which is parameterized by a function c ( x, y ) deﬁned for points ( x, y ) ∈x×y min t { [UNK] ic ( xi, t ( xi ) ) ; t♯α = β }. ( 1. 8 ) such a map between discrete points can be of course encoded, assuming all x ’ s andy ’ s are distinct, using indicesσ : jnk→jmkso thatj = σ ( i ), and the mass conservation is written as [UNK] i∈σ−1 ( j ) ai = bj. in the special case when n = mand all weights are uniform, that is ai = bj = 1 / n, then the mass conservation constraint implies that tis a bijection, such that t ( xi ) = yσ ( i ), and the monge problem is equivalent to the optimal matching problem ( 1. 5 ) where the cost matrix is ci, jdef. = c ( xi, yj ). whenn = m, note that, optimality aside, mon\n",
            "Chunk 9: , and the monge problem is equivalent to the optimal matching problem ( 1. 5 ) where the cost matrix is ci, jdef. = c ( xi, yj ). whenn = m, note that, optimality aside, monge maps may not even exist between an empirical measure to another. this happens when their weight vectors are not compatible, which is always the case when the target measure has more points than the source measure. for instance, the right plot in figure 1. 3 shows an ( optimal ) monge map between αandβ, but there is no monge map from βtoα. 6 monge problem ( 1. 8 ) is extended to the setting of two arbitrary probability measures ( α, β ) on two spaces ( x, y ) as ﬁnding a map t : x→y that minimizes min t { [UNK] xc ( x, t ( x ) ) dα ( x ) ; t♯α = β } ( 1. 9 ) the constraint t♯α = βmeans that tpushes forward the mass of αtoβ, and makes use of the push - forward operator ( 1. 2 ). 1. 3 kantorovitch problem the assignment problem has several limitations in practical settings, also encountered when using the monge problem. indeed, because the assignment problem is formulated as a permutation problem, it can only be used to compare two points clouds of the same size. a direct generalization to discrete measures with non - uniform weights can be carried out using monge ’ s formalism of pushforward maps, but that formulation may also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint ( 1. 7 ) ( see the end of remark?? ). additionally, the assignment problem ( 1. 8 ) is combinatorial, whereas the feasible set for the monge problem\n",
            "Chunk 10: there does not exist feasible solutions satisfying the mass conservation constraint ( 1. 7 ) ( see the end of remark?? ). additionally, the assignment problem ( 1. 8 ) is combinatorial, whereas the feasible set for the monge problem ( 1. 9 ), consisting in all push - forward measures that satisfy the mass conservation constraint, is non - convex. both are therefore [UNK] to solve in their original formulation. kantorovitch formulation for discrete measures. the key idea of [? ] is to relax the deterministic na - ture of transportation, namely the fact that a source point xican only be assigned to another, or transported to one and one location t ( xi ) only. kantorovich proposes instead that the mass at any point xibe potentially dispatched across several locations. kantorovich moves away from the idea that mass transportation should be “ deterministic ” to consider instead a “ probabilistic ” ( or “ fuzzy ” ) transportation, which allows what is commonly known now as “ mass splitting ” from a source towards several targets. this ﬂexibility is encoded using, in place of a permutation σor a mapt, a coupling matrix p∈rn×m +, where pi, jdescribes the amount of mass ﬂowing from bin i ( or pointxi ) towards bin j ( or pointxj ), xitowardsyjin the formalism of discrete measures ( 1. 6 ). admissible couplings admit a far simpler characterization than monge maps : u ( a, b ) def. = { p∈rn×m + ; p1m = aand pt1n = b }, ( 1. 10 ) where we used the following matrix - vector notation p1m = [UNK] jpi, j i∈rnand pt1n = ( [UNK] ipi, j ) j∈rm.\n",
            "Chunk 11: ##nd pt1n = b }, ( 1. 10 ) where we used the following matrix - vector notation p1m = [UNK] jpi, j i∈rnand pt1n = ( [UNK] ipi, j ) j∈rm. the set of matrices u ( a, b ) is bounded, deﬁned by n + mequality constraints, and therefore a convex polytope ( the convex hull of a ﬁnite set of matrices ). additionally, whereas the monge formulation ( as illustrated in the right plot of figure 1. 3 ) was intrisically asymmetric, kantorovich ’ s relaxed formulation is always symmetric, in the sense that a coupling pis in u ( a, b ) if and only if ptis inu ( b, a ). kantorovich ’ s optimal transport problem now reads lc ( a, b ) def. = min p∈u ( a, b ) ⟨ c, p ⟩ def. = [UNK] i, jci, jpi, j. ( 1. 11 ) this is a linear program ( see chapter?? ), and as is usually the case with such programs, its solutions are not necessarily unique. 7 [UNK] [UNK] 1. 4 : comparison of optimal matching and generic couplings. a black segment between xiandyj indicates a non - zero element in the displayed optimal coupling pi, jsolving ( 1. 11 ). left : optimal matching, corresponding to the setting of proposition ( 1 ) ( empirical measures with the same number n = mof points ). right : these two weighted point clouds cannot be matched ; instead a kantorovich coupling can be used to associate two arbitrary discrete measures. permutation matrices as couplings for a permutation σ∈perm ( n ), we write pσfor the correspond - ing permutation matrix, [UNK] ( i, j\n",
            "Chunk 12: coupling can be used to associate two arbitrary discrete measures. permutation matrices as couplings for a permutation σ∈perm ( n ), we write pσfor the correspond - ing permutation matrix, [UNK] ( i, j ) ∈jnk2, ( pσ ) i, j = { 1 / n ifj = σi, 0 otherwise. ( 1. 12 ) one can check that in that case ⟨ c, pσ ⟩ = 1 [UNK] i = 1ci, σi, which shows that the assignment problem ( 1. 5 ) can be recast as a kantorovich problem ( 1. 11 ) where the couplings pare restricted to be exactly permutation matrices : min σ∈perm ( n ) 1 [UNK] i = 1ci, σ ( i ) = min σ∈perm ( n ) ⟨ c, pσ ⟩. next, one can easily check that the set of permutation matrices is strictly included in the so - called [UNK] polytope u ( 1n / n, 1n, n ). indeed, for any permutation σwe have pσ1 = 1nandpσt1 = 1n, whereas 1n1nt / n2is a valid coupling but not a permutation matrix. therefore, one has naturally that min σ∈perm ( n ) ⟨ c, pσ ⟩ [UNK] ( 1n / n, 1n / n ). the following proposition shows that these problems result in fact in the same optimum, namely that one can always ﬁnd a permutation matrix that minimizes kantorovich ’ s problem ( 1. 11 ) between two uniform measures a = b = 1n / n, which shows that the kantorovich relaxation is tight when considered on assignment problems. figure 1. 4 shows on the left a 2 - d example of optimal\n",
            "Chunk 13: problem ( 1. 11 ) between two uniform measures a = b = 1n / n, which shows that the kantorovich relaxation is tight when considered on assignment problems. figure 1. 4 shows on the left a 2 - d example of optimal matching corresponding to this special case. proposition 1 ( kantorovich for matching ). ifm = nanda = b = 1n / n, then there exists an optimal solution for problem ( 1. 11 ) [UNK], which is a permutation matrix associated to an optimal permutation [UNK] perm ( n ) for problem ( 1. 5 ). proof. [UNK] ’ s theorem states that the set of extremal points of u ( 1n / n, 1n / n ) is equal to the set of permutation matrices. a fundamental theorem of linear programming [?, theorem 2. 7 ] states that the minimum of a linear objective in a non - empty polyhedron, if ﬁnite, is reached at an extremal point of the polyhedron. 8 [UNK] [UNK] [UNK] discrete semi - discrete continuous figure 1. 5 : schematic viewed of input measures ( α, β ) and couplingsu ( α, β ) encountered in the three main scenario for kantorovich ot. chapter?? is dedicated to the semi - discrete setup. [UNK] [UNK] figure 1. 6 : left : “ continuous ” coupling πsolving ( 1. 13 ) between two 1 - d measure with density. the coupling is localized along the graph of the monge map ( x, t ( x ) ) ( displayed in black ). right : “ discrete ” couplingtsolving ( 1. 11 ) between two discrete measures of the form ( 1. 6 ). the non - zero entries ti, jare display with a black disk at position ( i, j ) with radius proportional to ti, j. kantorovitch\n",
            "Chunk 14: 1. 11 ) between two discrete measures of the form ( 1. 6 ). the non - zero entries ti, jare display with a black disk at position ( i, j ) with radius proportional to ti, j. kantorovitch formulation for arbitrary measures. the deﬁnition of lcin (?? ) can be extended to arbitrary measures by considering couplings π∈m1 + ( x×y ) which are joint distributions over the product space. the discrete case is a special situation where one imposes this product measure to be of the form π = [UNK] i, jpi, jδ ( xi, yj ). in the general case, the mass conservation constraint ( 1. 10 ) should be rewritten as a marginal constraint on joint probability distributions u ( α, β ) def. = { π∈m1 + ( x×y ) ; px♯π = αandpy♯π = β }. ( 1. 13 ) herepx♯andpy♯are the push - forward ( see deﬁnition 1 ) by the projections px ( x, y ) = xandpy ( x, y ) = y. figure 1. 5 shows a schematic visualization of the coupling constraints for [UNK] class of problem ( discrete measures and densities ). using ( 1. 3 ), these marginal constraints are equivalent to imposing that π ( a×y ) = α ( a ) andπ ( x×b ) = β ( b ) for setsa⊂x andb⊂y. the kantorovich problem ( 1. 11 ) is then generalized as lc ( α, β ) def. = min π∈u ( α, β ) [UNK] x×yc ( x, y ) dπ ( x, y ). ( 1. 14 ) this is an inﬁnite - dimensional linear program over a\n",
            "Chunk 15: β ) def. = min π∈u ( α, β ) [UNK] x×yc ( x, y ) dπ ( x, y ). ( 1. 14 ) this is an inﬁnite - dimensional linear program over a space of measures. figure 1. 6 shows examples of discrete and continuous optimal coupling solving ( 1. 14 ). figure 1. 7 shows other examples of optimal 1 - d couplings, involving discrete and continuous marginals. on compact domain ( x, y ), ( 1. 14 ) always has a solution, because using the weak - * topology ( so called weak topology of measures ), the set of measure is compact, and a linear function with a continuous c ( x, y ) 9 [UNK] [UNK] [UNK] [UNK] 1. 7 : four simple examples of optimal couplings between 1 - d distributions, represented as maps above ( arrows ) and couplings below. inspired by [? ]. is weak - * continuous. and the set of constraint is non empty, taking α⊗β. on non compact domain, needs to impose moment condition on αandβ. wasserstein distances. an important feature of ot is that it deﬁnes a distance between histograms and probability measures as soon as the cost matrix satisﬁes certain suitable properties. indeed, ot can be understood as a canonical way to lift a ground distance between points to a distance between histogram or measures. we ﬁrst consider the case where, using a term ﬁrst introduce by [? ], the “ ground metric ” matrix c is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like to compare. the following proposition states that ot provides a meaningful distance between histograms supported on these bins. proposition 2. we suppose n = m, and that for some [UNK], c = dp =\n",
            "Chunk 16: ##tograms we would like to compare. the following proposition states that ot provides a meaningful distance between histograms supported on these bins. proposition 2. we suppose n = m, and that for some [UNK], c = dp = ( dp i, j ) i, j∈rn×nwhere d∈rn×n + is a distance on jnk, i. e. 1. d∈rn×n + is symmetric ; 2. di, j = 0if and only if i = j ; 3. [UNK] ( i, j, k ) ∈jnk3, di, [UNK], j + dj, k. then wp ( a, b ) def. = ldp ( a, b ) 1 / p ( 1. 15 ) ( note that wpdepends on d ) deﬁnes the p - wasserstein distance on σn, i. e. wpis symmetric, positive, wp ( a, b ) = 0 if and only if a = b, and it satisﬁes the triangle inequality [UNK], a ′, b∈σn, wp ( a, b ) [UNK] ( a, a ′ ) + wp ( a ′, b ). proof. symmetry and deﬁniteness of the distance are easy to prove : since c = dphas a null diagonal, wp ( a, a ) = 0, with corresponding optimal transport matrix [UNK] = diag ( a ) ; by the positivity of all [UNK] - diagonal elements of dp, wp ( a, b ) > 0 whenever a = b ( because in this case, an admissible coupling necessarily has a non - zero element outside the diagonal ) ; by symmetry of dp, wp ( a, b ) = 0 is itself a symmetric function. to prove the triangle inequality of was\n",
            "Chunk 17: ( because in this case, an admissible coupling necessarily has a non - zero element outside the diagonal ) ; by symmetry of dp, wp ( a, b ) = 0 is itself a symmetric function. to prove the triangle inequality of wasserstein distances for arbitrary measures, [?, theorem 7. 3 ] uses the gluing lemma, which stresses the existence of couplings with a prescribed structure. in the discrete setting, the explicit constuction of this glued coupling is simple. let a, b, c∈σn. let pandqbe two optimal solutions of the transport problems between aandb, and bandcrespectively. we deﬁne [UNK]. = bjifbj > 0 and set otherwise [UNK] = 1 ( or actually any other value ). we then deﬁne sdef. = pdiag ( 1 / [UNK] ) q∈rn×n +. 10 we remark that s∈u ( a, c ) because s1n = pdiag ( 1 / [UNK] ) q1n = p ( b / [UNK] ) = p1supp ( b ) = a where we denoted 1supp ( b ) the indicator of the support of b, and we use the fact that p1supp ( b ) = p1 = b because necessarily pi, j = 0 forj / ∈supp ( b ). similarly one veriﬁes that [UNK] = c. the triangle inequality follows from wp ( a, c ) = ( min p∈u ( a, c ) ⟨ p, dp ⟩ ) 1 / p [UNK] ⟨ s, dp ⟩ 1 / p = [UNK] ikdp [UNK] jpijqjk [UNK] 1 / p [UNK] [UNK] ijk ( dij + djk ) ppijqjk [UNK] 1 / p [UNK] [UNK] ijkdp ijpijqjk [UNK] 1\n",
            "Chunk 18: ##p ⟩ 1 / p = [UNK] ikdp [UNK] jpijqjk [UNK] 1 / p [UNK] [UNK] ijk ( dij + djk ) ppijqjk [UNK] 1 / p [UNK] [UNK] ijkdp ijpijqjk [UNK] 1 / p + [UNK] ijkdp jkpijqjk [UNK] 1 / p = [UNK] ijdp [UNK] kqjk [UNK] 1 / p + [UNK] jkdp [UNK] ipij [UNK] 1 / p = [UNK] ijdp ijpij 1 / p + [UNK] jkdp jkqjk 1 / p = wp ( a, b ) + wp ( b, b ). the ﬁrst inequality is due to the suboptimality of s, the second is the usual triangle inequality for elements ind, and the third comes from minkowski ’ s inequality. proposition 2 generalizes from histogram to arbitrary measures that need not be discrete. proposition 3. we assumex = y, and that for some [UNK], c ( x, y ) = d ( x, y ) pwheredis a distance on x, i. e. ( i ) d ( x, y ) = d ( y, x ) [UNK] ; ( ii ) d ( x, y ) = 0 if and only if x = y ; ( ii ) [UNK] ( x, y, z ) ∈x3, d ( x, z ) [UNK] ( x, y ) + d ( y, z ). then wp ( α, β ) def. = ldp ( α, β ) 1 / p ( 1. 16 ) ( note thatwpdepends on d ) deﬁnes the p - wasserstein distance on x, i. e. wpis symmetric, positive, wp ( α, β ) = 0 if and only if α = β, and it satisﬁes\n",
            "Chunk 19: ##s on d ) deﬁnes the p - wasserstein distance on x, i. e. wpis symmetric, positive, wp ( α, β ) = 0 if and only if α = β, and it satisﬁes the triangle inequality [UNK] ( α, β, γ ) ∈m1 + ( x ) 3, wp ( α, γ ) [UNK] ( α, β ) + wp ( β, γ ). proof. the proof follows the same approach as that for proposition 2 and relies on the existence of a coupling between ( α, γ ) obtained by “ guying ” optimal couplings between ( α, β ) and ( β, γ ). the wasserstein distance wphas many important properties, the most important one being that it is a weak distance, i. e. it allows to compare singular distributions ( for instance discrete ones ) and to quantify spatial shift between the supports of the distributions. in particular, “ classical ” distances ( or divergences ) are not even deﬁned between discrete distributions ( the l2norm can only be applied to continuous measures with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi, yj ) to be ﬁxed to work ). in sharp contrast, one has that for any p > 0, wp p ( δx, δy ) = d ( x, y ). indeed, it [UNK] to notice thatu ( δx, δy ) = { δx, y } and therefore the kantorovich problem having only one feasible solution, wp p ( δx, δy ) is necessarily ( d ( x, y ) p ) 1 / p = d ( x, y ). this shows that wp ( δx, δy ) →0 ifx→y. this property corresponds to\n",
            "Chunk 20: ##x, δy ) is necessarily ( d ( x, y ) p ) 1 / p = d ( x, y ). this shows that wp ( δx, δy ) →0 ifx→y. this property corresponds to the fact that wpis a way to quantify the weak convergence as we now deﬁne. 11 deﬁnition 2 ( weak convergence ). ( αk ) kconverges weakly to αinm1 + ( x ) ( [UNK] ) if and only if for any continuous function g∈c ( x ), [UNK] [UNK] xgdα. this notion of weak convergence corresponds to the convergence in law of random vectors. this convergence can be shown to be equivalent to wp ( αk, α ) →0 [?, theorem 6. 8 ] ( together with a convergence of the moments up to order pfor unbounded metric spaces ). note that there exists alternative distances which also metrize weak convergence. the simplest one are hilbertian norms, deﬁned as | | α | | 2 kdef. = eα⊗α ( k ) = [UNK] x×xk ( x, y ) dα ( x ) dα ( y ) for a suitable choice of kernel k : x2→r. the most famous of such kernel is the gaussian one k ( x, y ) = e− | | x−y | | 2 2σ2for some choice of bandwidth σ > 0. this convergence should not be confounded with the strong convergence of measures, which is metrized by the tv norm | | α | | tvdef. = | α | ( x ), which is the total mass of the absolute value of the measure. algorithms since (?? ) [UNK] is a linear program, it is possible to use any classical linear program solver, such as interior point\n",
            "Chunk 21: . = | α | ( x ), which is the total mass of the absolute value of the measure. algorithms since (?? ) [UNK] is a linear program, it is possible to use any classical linear program solver, such as interior point methods or simplex. in practice, the network simplex is an [UNK] option, and it used pivoting rule adapted to the ot constraint set. in the case of the assignment problem, a = b = 1n / n, there exists faster combinatorial optimization scheme, the most famous ones being the hungarian algorithm and the auction algorithm, which have roughly o ( n3 ) complexity. section 1. 5 details an approximate algorithm, which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the ot problem. 1. 4 duality the kantorovich problem ( 1. 11 ) is a constrained convex minimization problem, and as such, it can be naturally paired with a so - called dual problem, which is a constrained concave maximization problem. the following fundamental proposition, which is a special case of fenchel - rockafellar duality theory, explains the relationship between the primal and dual problems. proposition 4. one has lc ( a, b ) = max ( f, g ) ∈r ( a, b ) ⟨ f, a ⟩ + ⟨ g, b ⟩ ( 1. 17 ) where the set of admissible potentials is r ( a, b ) def. = { ( f, g ) ∈rn×rm ; [UNK] ( i, j ) ∈jnk×jmk, [UNK] } ( 1. 18 ) proof. this result is a direct consequence of the more general result on the strong duality for linear pro - grams [?, p. 148, theo. 4. 4 ]. the easier part of that result, namely that the\n",
            "Chunk 22: . 18 ) proof. this result is a direct consequence of the more general result on the strong duality for linear pro - grams [?, p. 148, theo. 4. 4 ]. the easier part of that result, namely that the right - hand side of equation ( 1. 17 ) is a lower bound on l c ( a, b ) is discussed in??. for the sake of completeness, let us derive this dual problem with the use of lagrangian duality. the lagangian associate to ( 1. 11 ) reads min [UNK] ( f, g ) ∈rn×rm ⟨ c, p ⟩ + ⟨ a−p1m, f ⟩ + ⟨ [UNK], g ⟩. ( 1. 19 ) for linear program, one can always exchange the min and the max and get the same value of the linear program, and one thus consider max ( f, g ) ∈rn×rm ⟨ a, f ⟩ + ⟨ b, g ⟩ + min [UNK] ⟨ [UNK] [UNK], p ⟩. we conclude by remarking that min [UNK] ⟨ q, p ⟩ = { 0 if [UNK] −∞ otherwise so that the constraint reads [UNK] [UNK] = [UNK]. 12 the primal - dual optimality relation for the lagrangian ( 1. 19 ) allows to locate the support of the optimal transport plan supp ( p ) ⊂ { ( i, j ) ∈jnk×jmk ; fi + gj = ci, j }. ( 1. 20 ) to extend this primal - dual construction to arbitrary measures, it is important to realize that measures are naturally paired in duality with continuous functions ( a measure can only be accessed through integration against continuous functions ). the duality is formalized in the following proposition, which boils down to proposition 4 when dealing with discrete measures. proposition 5. one has lc ( α, β ) = max (\n",
            "Chunk 23: measure can only be accessed through integration against continuous functions ). the duality is formalized in the following proposition, which boils down to proposition 4 when dealing with discrete measures. proposition 5. one has lc ( α, β ) = max ( f, g ) ∈r ( c ) [UNK] xf ( x ) dα ( x ) + [UNK] yg ( y ) dβ ( y ), ( 1. 21 ) where the set of admissible dual potentials is r ( c ) def. = { ( f, g ) ∈c ( x ) ×c ( y ) ; [UNK] ( x, y ), f ( x ) + g ( y ) [UNK] ( x, y ) }. ( 1. 22 ) here, ( f, g ) is a pair of continuous functions, and are often called “ kantorovich potentials ”. the discrete case ( 1. 17 ) corresponds to the dual vectors being samples of the continuous potentials, i. e. ( fi, gj ) = ( f ( xi ), g ( yj ) ). the primal - dual optimality conditions allow to track the support of optimal plan, and ( 1. 20 ) is generalized as supp ( π ) ⊂ { ( x, y ) ∈x×y ; f ( x ) + g ( y ) = c ( x, y ) }. ( 1. 23 ) note that in contrast to the primal problem ( 1. 14 ), showing the existence of solutions to ( 1. 21 ) is non - trivial, because the constraint set r ( c ) is not compact and the function to minimize non - coercive. using the machinery of c - transform detailed in section??, one can however show that optimal ( f, g ) are necessarily lipschitz regular, which enable to replace the constraint by a compact one. benier ’\n",
            "Chunk 24: - coercive. using the machinery of c - transform detailed in section??, one can however show that optimal ( f, g ) are necessarily lipschitz regular, which enable to replace the constraint by a compact one. benier ’ s theorem and monge - amp ` ere pde the following celebrated theorem of [? ] ensures that in rdforp = 2, if at least one of the two inputs measures has a density, then kantorovitch and monge problems are equivalent. theorem 1 ( brenier ). in the casex = y = rdandc ( x, y ) = | | x−y | | 2, if at least one of the two inputs measures ( denoted α ) has a density ραwith respect to the lebesgue measure, then the optimal πin the kantorovich formulation ( 1. 14 ) is unique, and is supported on the graph ( x, t ( x ) ) of a “ monge map ” t : rd→rd. this means that π = ( id, t ) ♯µ, i. e. [UNK] ( x×y ), [UNK] x×yh ( x, y ) dπ ( x, y ) = [UNK] xh ( x, t ( x ) ) dµ ( x ). ( 1. 24 ) furthermore, this map tis uniquely deﬁned as the gradient of a convex function [UNK], t ( x ) = [UNK] ( x ), where [UNK] the unique ( up to an additive constant ) convex function such that ( [UNK] ) ♯µ = ν. this convex function is related to the dual potential fsolving ( 1. 21 ) [UNK] ( x ) = | | x | | 2 2−f ( x ). proof. we sketch the main ingredients of the proof, more details can be found for instance in [? ]. we remark\n",
            "Chunk 25: ##solving ( 1. 21 ) [UNK] ( x ) = | | x | | 2 2−f ( x ). proof. we sketch the main ingredients of the proof, more details can be found for instance in [? ]. we remark [UNK] cdπ = cα, [UNK] ⟨ x, y ⟩ dπ ( x, y ) where the constant is cα, β = [UNK] | | x | | 2dα ( x ) + [UNK] | | y | | 2dβ ( y ). instead of solving ( 1. 14 ), one can thus consider the following problem max π∈u ( α, β ) [UNK] x×y ⟨ x, y ⟩ dπ ( x, y ), whose dual reads min ( [UNK], ψ ) { [UNK] [UNK] + [UNK] yψdβ ; [UNK] ( x, y ), [UNK] ( x ) + ψ ( y ) [UNK] ⟨ x, y ⟩ }. ( 1. 25 ) 13 the relation between these variables and those of ( 1. 22 ) is ( [UNK], ψ ) = ( | | · | | 2 2−f, | | · | | 2 2−g ). one can replace the constraint by [UNK], ψ ( y ) [UNK] ( y ) def. = sup x ⟨ x, y ⟩ [UNK] ( x ). ( 1. 26 ) [UNK] the legendre transform of [UNK] is a convex function as a supremum of linear forms ( see also (?? ) ). since the objective appearing in ( 1. 27 ) is linear and the integrating measures positive, one can minimize explicitly with respect to [UNK] setψ = [UNK] order to consider the unconstraint problem min [UNK] [UNK] + [UNK] [UNK], ( 1. 27 ) see also section?? for a generalization of this idea to generic costs c ( x, y ). by iterating this argument twice, one can\n",
            "Chunk 26: the unconstraint problem min [UNK] [UNK] + [UNK] [UNK], ( 1. 27 ) see also section?? for a generalization of this idea to generic costs c ( x, y ). by iterating this argument twice, one can replace [UNK], which is a convex function, and thus impose in ( 1. 27 ) that [UNK] convex. condition ( 1. 23 ) shows that an optimal πis supported on { ( x, y ) ; [UNK] ( x ) + [UNK] ( y ) = ⟨ x, y ⟩ } which shows that such anyis optimal for the minimization ( 1. 26 ) of the legendre transform, whose optimality condition reads [UNK] ( x ). [UNK] convex, it is [UNK] almost everywhere, and since αhas a density, it is also [UNK] α - almost everywhere. this shows that for each x, the associated yis uniquely deﬁned α - almost everywhere as y = [UNK] ( x ), and shows that necessarily π = ( id, [UNK] ) ♯α. this results shows that in the setting of w2with non - singular densities, the monge problem ( 1. 9 ) and its kantorovich relaxation ( 1. 14 ) are equal ( the relaxation is tight ). this is the continuous analog of proposition 1 for the assignment case ( 1 ), which states that the minimum of the optimal transport problem is achieved, when the marginals are equal and uniform, at a permutation matrix ( a discrete map ). brenier ’ s theorem, stating that an optimal transport map must be the gradient of a convex function, should be examined under the light that a convex function is the natural generalization of the notion of increasing functions in dimension more than one. optimal transport can thus plays an important role to deﬁne quantile functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems\n",
            "Chunk 27: convex function is the natural generalization of the notion of increasing functions in dimension more than one. optimal transport can thus plays an important role to deﬁne quantile functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [? ]. note also that this theorem can be extended in many directions. the condition that αhas a density can be weakened to the condition that it does not give mass to “ small sets ” having [UNK] dimension smaller thand−1 ( e. g. hypersurfaces ). one can also consider costs of the form c ( x, y ) = h ( x−y ) wherehis a strictly convex function. for measures with densities, using ( 1. 4 ), one obtains that [UNK] the unique ( up to the addition of a constant ) convex function which solves the following monge - amp [UNK] - type equation det ( [UNK] ( x ) ) ρβ ( [UNK] ( x ) ) = ρα ( x ) ( 1. 28 ) [UNK] ( x ) ∈rd×dis the hessian of [UNK]. the monge - amp ` ere operator det ( [UNK] ( x ) ) can be understood as a non - linear degenerate laplacian. in the limit of small displacements, [UNK] = id + [UNK], one indeed recovers the laplacian ∆ as a linearization since for smooth maps det ( [UNK] ( x ) ) = 1 + [UNK] ( x ) + o ( ε ). the convexity constraint forces det ( [UNK] ( x ) ) [UNK] and is necessary for this equation to have a solution. special cases in general, computing ot distances is numerically involved. we review special favorable cases where the resolution of the ot problem is easy. remark 6 ( binary cost matrix and 1 - norm ). one can easily check that when the cost matrix cis zero on the diagonal and 1 elsewhere,\n",
            "Chunk 28: is numerically involved. we review special favorable cases where the resolution of the ot problem is easy. remark 6 ( binary cost matrix and 1 - norm ). one can easily check that when the cost matrix cis zero on the diagonal and 1 elsewhere, namely when c = 1n×n−in, the ot distance between aandbis equal to the 1 - norm of their [UNK], l c ( a, b ) = | | a−b | | 1. one can also easily check that this result extends to discrete and discrete measures in the case where c ( x, y ) is 0 ifx = yand 1 when x = y. the ot distance between two discrete measures αandβis equal to their total variation distance. 14 [UNK] 1. 8 : 1 - d optimal couplings : each arrow xi→yjindicate a non - zero pi, jin the optimal coupling. top : empirical measures with same number of points ( optimal matching ). bottom : generic case. this corresponds to monotone rearrangements, if [UNK] ′ are such that pi, j = 0, pi ′, j ′ = 0, then necessarily [UNK] ′. remark 7 ( 1 - d case – empirical measures ). herex = r. assuming α = 1 [UNK] i = 1δxiandβ = 1 [UNK] j = 1δyj, and assuming ( without loss of generality ) that the points are ordered, i. e. [UNK]... [UNK] [UNK]... [UNK], then one has the simple formula wp ( α, β ) p = [UNK] i = 1 | xi−yi | p, ( 1. 29 ) i. e. locally ( if one assumes distinct points ), wp ( α, β ) is theℓpnorm between two vectors of ordered values of αandβ. that statement is only valid locally, in the sense that the\n",
            "Chunk 29: ) i. e. locally ( if one assumes distinct points ), wp ( α, β ) is theℓpnorm between two vectors of ordered values of αandβ. that statement is only valid locally, in the sense that the order ( and those vector representations ) might change whenever some of the values change. that formula is a simple consequence of the more general remark given below. figure 1. 8, top row, illustrates the 1 - d transportation map between empirical measures with the same number of points. the bottom row shows how this monotone map generalizes to arbitrary discrete measures. it is possible to leverage this 1 - d computation to also compute [UNK] ot on the circle, see [? ]. note that in the case of concave cost of the distance, for instance when p < 1, the behaviour of the optimal transport plan is very [UNK], see [? ], which describes an [UNK] solver in this case. remark 8 ( 1 - d case – generic case ). for a measure αonr, we introduce the cumulative function [UNK], cα ( x ) def. = [UNK] −∞dα, ( 1. 30 ) which is a function cα : r→ [ 0, 1 ], and its pseudo - inverse c−1 α : [ 0, 1 ] →r∪ { −∞ } [UNK] [ 0, 1 ], c−1 α ( r ) = min x { x∈r∪ { −∞ } ; cα ( x ) [UNK] }. that function is also called the generalized quantile function of α. for [UNK], one has wp ( α, β ) p = | | c−1 α−c−1 β | | p lp ( [ 0, 1 ] ) = [UNK] 0 | c−1 α ( r ) −c−1 β ( r ) | pdr. ( 1. 31 ) this means that through the\n",
            "Chunk 30: | c−1 α−c−1 β | | p lp ( [ 0, 1 ] ) = [UNK] 0 | c−1 α ( r ) −c−1 β ( r ) | pdr. ( 1. 31 ) this means that through the map α↦→c−1 α, the wasserstein distance is isometric to a linear space equipped with thelpnorm, or, equivalently, that the wasserstein distance for measures on the real line is a hilbertian metric. this makes the geometry of 1 - d optimal transport very simple, but also very [UNK] from its geometry in higher dimensions, which is not hilbertian as discussed in proposition?? and more generally in §??. forp = 1, one even has the simpler formula w1 ( α, β ) = | | cα−cβ | | l1 ( r ) = [UNK] r | cα ( x ) −cβ ( x ) | dx ( 1. 32 ) = [UNK] [UNK] −∞d ( α−β ) [UNK]. ( 1. 33 ) 15 µ ν ( tt + ( 1−t ) id ) ♯µ 0 0. 5 10. 5cµ cν 0 0. 5 100. 51 cµ - 1 cν - 1 0 0. 5 100. 51 t t - 1 0 0. 5 100. 51 ( cα, cβ ) ( c−1 α, c−1 β ) ( t, t−1 ) ( 1−t ) c−1 α + tc−1 β figure 1. 9 : computation of ot and displacement interpolation between two 1 - d measures, using cumulant function as detailed in ( 1. 34 ). which shows that w1is a norm ( see §?? for the generalization to arbitrary dimensions ). an optimal monge maptsuch thatt♯α = βis then deﬁned\n",
            "Chunk 31: function as detailed in ( 1. 34 ). which shows that w1is a norm ( see §?? for the generalization to arbitrary dimensions ). an optimal monge maptsuch thatt♯α = βis then deﬁned by t = c−1 [UNK]. ( 1. 34 ) figure 1. 9 illustrates the computation of 1 - d ot through cumulative functions. it also displays displacement interpolations, computed as detailed in (?? ), see also remark??. for a detailed survey of the properties of optimal transport in 1 - d, we refer the reader to [?, chapter 2 ]. remark 9 ( distance between gaussians ). ifα = n ( mα, σα ) andβ = n ( mβ, σβ ) are two gaussians in rd, then one can show that the following map t : x↦→mβ + a ( x−mα ), ( 1. 35 ) where a = σ−1 2α ( σ1 2ασβσ1 2α ) 1 2σ−1 2α = at, is such that t♯ρα = ρβ. indeed, one simply has to notice that the change of variables formula ( 1. 4 ) is satisﬁed since ρβ ( t ( x ) ) = det ( 2πσβ ) −1 2exp ( − ⟨ t ( x ) −mβ, σ−1 β ( t ( x ) −mβ ) ⟩ ) = det ( 2πσβ ) −1 2exp ( − ⟨ x−mα, atσ−1 βa ( x−mα ) ⟩ ) = det ( 2πσβ ) −1 2exp ( − ⟨ x−mα, σ−1 α ( x−mα ) ⟩ ), and sincetis a linear map we have that | det\n",
            "Chunk 32: ##−mα ) ⟩ ) = det ( 2πσβ ) −1 2exp ( − ⟨ x−mα, σ−1 α ( x−mα ) ⟩ ), and sincetis a linear map we have that | dett ′ ( x ) | = deta = ( detσβ detσα ) 1 2 and we therefore recover ρα = | dett ′ | ρβmeaningt♯α = β. notice now that tis the gradient of the convex functionψ : x↦→1 2 ⟨ x−mα, a ( x−mα ) ⟩ + ⟨ mβ, x ⟩ to conclude, using brenier ’ s theorem [? ] ( see remark?? ) thattis optimal. both that map tand the corresponding potential ψare illustrated in figures 1. 10 and?? 16 - 4 - 2 0 2 4 6 - 3 - 2 - 101234 ρβραfigure 1. 10 : two gaussians ραandρβ, represented using the contour plots of their densities, with respective mean and variance matrices mα = ( −2, 0 ), σα = 1 2 ( 1−1 2 ; −1 21 ) andmβ = ( 3, 1 ), σβ = ( 2, 1 2 ; 1 2, 1 ). the arrows originate at random points xtaken on the plane and end at the corresponding mappings of those pointst ( x ) = mβ + a ( x−mα ). m figure 1. 11 : computation of displacement interpolation between two 1 - d gaussians. denoting gm, σ ( x ) def. = 1√ 2πse− ( x−m ) 2 2s2the gaussian density, it thus shows the interpolation g ( 1−t ) m0 + t\n",
            "Chunk 33: . denoting gm, σ ( x ) def. = 1√ 2πse− ( x−m ) 2 2s2the gaussian density, it thus shows the interpolation g ( 1−t ) m0 + tm1, ( 1−t ) σ0 + tσ1. with additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport cost of that map is w2 2 ( α, β ) = | | mα−mβ | | 2 + b ( σα, σβ ) 2 ( 1. 36 ) wherebis the so - called bures ’ metric [? ] between positive deﬁnite matrices ( see also [?,? ] ), b ( σα, σβ ) 2def. = tr ( σα + σβ−2 ( σ1 / 2 ασβσ1 / 2 α ) 1 / 2 ), ( 1. 37 ) where σ1 / 2is the matrix square root. one can show that bis a distance on covariance matrices, and that b2is convex with respect to both its arguments. in the case where σα = diag ( ri ) iandσβ = diag ( si ) iare diagonals, the bures metric is the hellinger distance b ( σα, σβ ) = | | √r−√s | | 2. for 1 - d gaussians, w2is thus the euclidean distance on the 2 - d plane ( m, √ σ ), as illustrated in figure 1. 11. for a detailed treatment of the wasserstein geometry of gaussian distributions, we refer to [? ]. 1. 5 sinkhorn this section introduces a family of numerical scheme to approximate solutions to kantorovich formulation of optimal transport and its many generalizations. it operates by adding\n",
            "Chunk 34: ##serstein geometry of gaussian distributions, we refer to [? ]. 1. 5 sinkhorn this section introduces a family of numerical scheme to approximate solutions to kantorovich formulation of optimal transport and its many generalizations. it operates by adding an entropic regularization penalty to the original problem. this regularization has several important advantages, but a few stand out particularly : the minimization of the regularized problen can be solved using a simple alternate minimization scheme ; that scheme translates into iterations that are simple matrix products, making them particularly suited to execution of gpu ; the resulting approximate distance is smooth with respect to input histogram weights and positions of the diracs. 17 c \" p \" figure 1. 12 : impact of εon the optimization of a linear function on the simplex, solving pε = argminp∈σ3 ⟨ c, p ⟩ −εh ( p ) for a varying ε. entropic regularization. the discrete entropy of a coupling matrix is deﬁned as h ( p ) def. = [UNK] i, jpi, j ( log ( pi, j ) −1 ), ( 1. 38 ) with an analogous deﬁnition for vectors, with the convention that h ( a ) = −∞ if one of the entries ajis 0 or negative. the function his 1 - strongly concave, because its hessian is ∂2h ( p ) = −diag ( 1 / pi, j ) and pi, [UNK]. the idea of the entropic regularization of optimal transport is to use −has a regularizing function to obtain approximate solutions to the original transport problem ( 1. 11 ) : lε c ( a, b ) def. = min p∈u ( a, b ) ⟨ p, c ⟩ −εh ( p ). ( 1. 39\n",
            "Chunk 35: obtain approximate solutions to the original transport problem ( 1. 11 ) : lε c ( a, b ) def. = min p∈u ( a, b ) ⟨ p, c ⟩ −εh ( p ). ( 1. 39 ) since the objective is a ε - strongly convex function, problem 1. 39 has a unique optimal solution. the idea to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in transportation theory [? ] : actual [UNK] patterns in a network do not agree with those predicted by the solution of the optimal transport problem. indeed, the former are more [UNK] than the latter, which tend to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1. 11. to balance for that, researchers in transportation proposed a model, called the “ gravity ” model [? ], that is able to form a more “ blurred ” [UNK] prediction. figure 1. 12 illustrates the [UNK] of the entropy to regularize a linear program over the simples σ 3 ( which can thus be visualized as a triangle in 2 - d ). note how the entropy pushes the original lp solution away from the boundary of the triangle. the optimal pεprogressively moves toward an “ entropic center ” of the triangle. this is further detailed in the proposition below. the convergence of the solution of that regularized problem towards an optimal solution of the original linear program has been studied by [? ]. proposition 6 ( convergence with ε ). the unique solution pεof ( 1. 39 ) converges to the optimal solution with maximal entropy within the set of all optimal solutions of the kantorovich problem, namely pεε→0−→argmin p { −h ( p ) ; p∈u ( a, b ), ⟨ p, c ⟩\n",
            "Chunk 36: with maximal entropy within the set of all optimal solutions of the kantorovich problem, namely pεε→0−→argmin p { −h ( p ) ; p∈u ( a, b ), ⟨ p, c ⟩ = lc ( a, b ) } ( 1. 40 ) so that in particular lε c ( a, b ) ε→0−→lc ( a, b ). one has pεε→∞−→abt = ( aibj ) i, j. ( 1. 41 ) proof. we consider a sequence ( εℓ ) ℓsuch thatεℓ→0 andεℓ > 0. we denote pℓthe solution of ( 1. 39 ) for ε = εℓ. since u ( a, b ) is bounded, we can extract a sequence ( that we do not relabel for sake of simplicity ) such that [UNK]. since u ( a, b ) is closed, [UNK] ( a, b ). we consider any psuch that ⟨ c, p ⟩ = lc ( a, b ). by optimality of pandpℓfor their respective optimization problems ( for ε = 0 andε = εℓ ), one has [UNK] ⟨ c, pℓ ⟩ − ⟨ c, p ⟩ [UNK] ( h ( pℓ ) −h ( p ) ). ( 1. 42 ) 18 [UNK] \" [UNK] \" [UNK] 1. 13 : impact of εon coupling between densities and discrete distributions, illustrating proposition 6. left : between two 1 - d densities. right : between two 2 - d discrete empirical densities with same number n = mof points ( only entries of the optimal ( pi, j ) i, jabove a small threshold are displayed as segments betweenxiandyj ). since his continuous, taking the limit ℓ→ + ∞in this expression shows that ⟨ c\n",
            "Chunk 37: ##f points ( only entries of the optimal ( pi, j ) i, jabove a small threshold are displayed as segments betweenxiandyj ). since his continuous, taking the limit ℓ→ + ∞in this expression shows that ⟨ c, [UNK] ⟩ = ⟨ c, p ⟩ so that [UNK] a feasible point of ( 1. 40 ). furthermore, dividing by εℓin ( 1. 42 ) and taking the limit shows that h ( p ) [UNK] ( [UNK] ), which shows that [UNK] a solution of ( 1. 40 ). since the solution [UNK] 0to this program is unique by strict convexity of −h, one has [UNK] = [UNK] 0, and the whole sequence is converging. formula ( 1. 40 ) states that for low regularization, the solution converges to the maximum entropy optimal transport coupling. in sharp contrast, ( 1. 41 ) shows that for large regularization, the solution converges to the coupling with maximal entropy between two prescribed marginals a, b, namely the joint probability between two independent random variables with prescribed distributions. a reﬁned analysis of this convergence is performed in [? ], including a ﬁrst order expansion in ε ( resp. 1 / ε ) nearε = 0 ( respε = + ∞ ). figure 1. 13 shows visually the [UNK] of these two convergence. a key insight is that, as εincreases, the optimal coupling becomes less and less sparse ( in the sense of having entries larger than a prescribed thresholds ), which in turn as the [UNK] of both accelerating computational algorithms ( as we study in § 1. 5 ) but also leading to faster statistical convergence ( as exposed in §?? ). deﬁning the kullback - leibler divergence between couplings as kl ( p | k ) def. = [UNK] i, jpi, jlog (\n",
            "Chunk 38: also leading to faster statistical convergence ( as exposed in §?? ). deﬁning the kullback - leibler divergence between couplings as kl ( p | k ) def. = [UNK] i, jpi, jlog ( pi, j ki, j ) −pi, j + ki, j, ( 1. 43 ) the unique solution pεof ( 1. 39 ) is a projection onto u ( a, b ) of the gibbs kernel associated to the cost matrix cas ki, jdef. = e−ci, j ε indeed one has that using the deﬁnition above pε = projkl u ( a, b ) ( k ) def. = argmin p∈u ( a, b ) kl ( p | k ). ( 1. 44 ) remark 10 ( general formulation ). one can consider arbitrary measures by replacing the discrete entropy by the relative entropy with respect to the product measure d α⊗dβ ( x, y ) def. = dα ( x ) dβ ( y ), and propose a regularized counterpart to ( 1. 14 ) using lε c ( α, β ) def. = min π∈u ( α, β ) [UNK] x×yc ( x, y ) dπ ( x, y ) + εkl ( π | α⊗β ) ( 1. 45 ) where the relative entropy is a generalization of the discrete kullback - leibler divergence ( 1. 43 ) kl ( π | ξ ) def. = [UNK] x×ylog ( dπ dξ ( x, y ) ) dπ ( x, y ) + [UNK] x×y ( dξ ( x, y ) −dπ ( x, y ) ), ( 1. 46 ) 19 and by convention kl ( π | ξ ) = + ∞if\n",
            "Chunk 39: dπ ( x, y ) + [UNK] x×y ( dξ ( x, y ) −dπ ( x, y ) ), ( 1. 46 ) 19 and by convention kl ( π | ξ ) = + ∞ifπdoes not have a densitydπ dξwith respect to ξ. it is important to realize that the reference measure α⊗βchosen in ( 1. 45 ) to deﬁne the entropic regularizing term kl ( · | α⊗β ) plays no speciﬁc role, only its support matters. formula ( 1. 45 ) can be re - factored as a projection problem min π∈u ( α, β ) kl ( π | k ) ( 1. 47 ) wherekis the gibbs distributions d k ( x, y ) def. = e−c ( x, y ) εdµ ( x ) dν ( y ). this problem is often referred to as the “ static schr¨ odinger problem ” [?,? ], since it was initially considered by schr¨ odinger in statistical physics [? ]. asε→0, the unique solution to ( 1. 47 ) converges to the maximum entropy solution to ( 1. 14 ), see [?,? ]. §?? details an alternate “ dynamic ” formulation of the schr¨ odinger problem over the space of paths connecting the points of two measures. sinkhorn ’ s algorithm the following proposition shows that the solution of ( 1. 39 ) has a speciﬁc form, which can be parameterized using n + mvariables. that parameterization is therefore essentially dual, in the sense that a coupling pinu ( a, b ) hasnmvariables but n + mconstraints. proposition 7. the solution to ( 1. 39 ) is unique and has\n",
            "Chunk 40: . that parameterization is therefore essentially dual, in the sense that a coupling pinu ( a, b ) hasnmvariables but n + mconstraints. proposition 7. the solution to ( 1. 39 ) is unique and has the form [UNK] ( i, j ) ∈jnk×jmk, pi, j = uiki, jvj ( 1. 48 ) for two ( unknown ) scaling variable ( u, v ) ∈rn + ×rm +. proof. introducing two dual variables f∈rn, g∈rmfor each marginal constraint, the lagrangian of ( 1. 39 ) reads e ( p, f, g ) = ⟨ p, c ⟩ −εh ( p ) − ⟨ f, p1m−a ⟩ − ⟨ g, pt1n−b ⟩. considering ﬁrst order conditions, we have ∂e ( p, f, g ) ∂pi, j = ci, j−εlog ( pi, j ) −fi−gj. which results, for an optimal pcoupling to the regularized problem, in the expression pi, j = efi / εe−ci, j / εegj / ε which can be rewritten in the form provided in the proposition using non - negative vectors uandv. the factorization of the optimal solution exhibited in equation ( 1. 48 ) can be conveniently rewritten in matrix form as p = diag ( u ) kdiag ( v ). u, vmust therefore satisfy the following non - linear equations which correspond to the mass conservation constraints inherent to u ( a, b ), diag ( u ) kdiag ( v ) 1m = a, and diag ( v ) [UNK] ( u ) 1n = b, ( 1. 49 ) these two equations can be further simpliﬁed, since\n",
            "Chunk 41: , diag ( u ) kdiag ( v ) 1m = a, and diag ( v ) [UNK] ( u ) 1n = b, ( 1. 49 ) these two equations can be further simpliﬁed, since diag ( v ) 1mis simply v, and the multiplication of diag ( u ) times kvis [UNK] ( kv ) = aand [UNK] ( ktu ) = b ( 1. 50 ) [UNK] to entry - wise multiplication of vectors. that problem is known in the numerical analysis community as the matrix scaling problem ( see [? ] and references therein ). an intuitive way to try to solve these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left - hand side of equation ( 1. 50 ) and then vto satisfy its right - hand side. these two updates deﬁne sinkhorn ’ s algorithm : u ( ℓ + 1 ) def. = a kv ( ℓ ) and v ( ℓ + 1 ) def. = b ktu ( ℓ + 1 ), ( 1. 51 ) initialized with an arbitrary positive vector v ( 0 ) = 1m. the division operator used above between two vectors is to be understood entry - wise. note that a [UNK] initialization will likely lead to a [UNK] 20 ` [UNK] ( ` ) \" 1000 2000 3000 4000 5000 - 2 - 1. 5 - 1 - 0. 50 ` figure 1. 14 : left : evolution of the coupling πℓ ε = diag ( u ( ℓ ) ) kdiag ( v ( ℓ ) ) computed at iteration ℓof sinkhorn ’ s iterations, for 1 - d densities. right : impact of εthe convergence rate of sinkhorn, as measured in term of marginal constraint violation log ( | | πℓ ε1m−b | | 1 ). solution for\n",
            "Chunk 42: ’ s iterations, for 1 - d densities. right : impact of εthe convergence rate of sinkhorn, as measured in term of marginal constraint violation log ( | | πℓ ε1m−b | | 1 ). solution for u, v, since u, vare only deﬁned up to a multiplicative constant ( if u, vsatisfy ( 1. 49 ) then so doλu, v / λfor anyλ > 0 ). it turns out however that these iterations converge ( see remark 11 for a justiﬁcation using iterative projections, and remark 13 for a strict contraction result ) and all result in the same optimal coupling diag ( u ) kdiag ( v ). figure 1. 14, top row, shows the evolution of the coupling diag ( u ( ℓ ) ) kdiag ( v ( ℓ ) ) computed by sinkhorn iterations. it evolves from the gibbs kernel ktowards the optimal coupling solving ( 1. 39 ) by progressively shifting the mass away from the diagonal. remark 11 ( relation with iterative projections ). denoting c1 adef. = { p ; p1m = a } andc2 bdef. = { p ; pt1m = b } the rows and columns constraints, one has u ( a, b ) = c1 a∩c2 b. one can use bregman iterative projections [? ] p ( ℓ + 1 ) def. = projkl c1a ( p ( ℓ ) ) and p ( ℓ + 2 ) def. = projkl c2 b ( p ( ℓ + 1 ) ). ( 1. 52 ) since the setsc1 aandc2 bare [UNK], these iterations are known to converge to the solution of ( 1. 44 ), see [? ]. these iterate are equivalent\n",
            "Chunk 43: ℓ + 1 ) ). ( 1. 52 ) since the setsc1 aandc2 bare [UNK], these iterations are known to converge to the solution of ( 1. 44 ), see [? ]. these iterate are equivalent to sinkhorn iterations ( 1. 51 ) since deﬁning p ( 2ℓ ) def. = diag ( u ( ℓ ) ) kdiag ( v ( ℓ ) ), one has p ( 2ℓ + 1 ) def. = diag ( u ( ℓ + 1 ) ) kdiag ( v ( ℓ ) ) and p ( 2ℓ + 2 ) def. = diag ( u ( ℓ + 1 ) ) kdiag ( v ( ℓ + 1 ) ) in practice however one should prefer using ( 1. 51 ) which only requires manipulating scaling vectors and multiplication against a gibbs kernel, which can often be accelerated ( see below remarks?? and?? ). remark 12 ( hilbert metric ). as initially explained by [? ], the global convergence analysis of sinkhorn is greatly simpliﬁed using hilbert projective metric on rn +, ∗ ( positive vectors ), deﬁned as [UNK] ( u, u ′ ) ∈ ( rn +, ∗ ) 2, dh ( u, u ′ ) def. = log max i, i ′ uiu ′ i ′ ui ′ u ′ i. this can be shows to be a distance on the projective cone rn +, ∗ / [UNK], where [UNK] ′ means [UNK] > 0, u = su ′ ( the vector are equal up to rescaling, hence the naming “ projective ” ). this means that dhsatisﬁes the triangular inequality and dh ( u, u ′ ) = 0 if and only if [UNK] ′. this is a projective version of hilbert ’ s original distance on bounded open convex sets [? ]. the projective\n",
            "Chunk 44: this means that dhsatisﬁes the triangular inequality and dh ( u, u ′ ) = 0 if and only if [UNK] ′. this is a projective version of hilbert ’ s original distance on bounded open convex sets [? ]. the projective cone rn +, ∗ / [UNK] a complete metric space for this distance. it was introduced independently by [? ] and [? ] to provide a quantitative proof of perron - frobenius theorem, which, as explained in remark?? is linked to a local linearization of sinkhorn ’ s iterates. they proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the cone of positive vectors. 21 theorem 2. let k∈rn×m +, ∗, then for ( v, v ′ ) ∈ ( rm +, ∗ ) 2 dh ( kv, kv ′ ) [UNK] ( k ) dh ( v, v ′ ) where λ ( k ) def. = √ η ( k ) −1√ η ( k ) + 1 < 1 η ( k ) def. = max i, j, k, ℓki, kkj, ℓ kj, kki, ℓ. remark 13 ( global convergence ). the following theorem, proved by [? ], makes use of this theorem 2 to show the linear convergence of sinkhorn ’ s iterations. theorem 3. one has ( u ( ℓ ), v ( ℓ ) ) → ( [UNK], [UNK] ) and dh ( u ( ℓ ), [UNK] ) = o ( λ ( k ) 2ℓ ), dh ( v ( ℓ ), [UNK] ) = o ( λ ( k ) 2ℓ ). ( 1. 53 ) one also has dh ( u ( ℓ ), [UNK] ) [UNK] ( p ( ℓ ) 1m, a ) 1−λ ( k ) dh ( v ( ℓ ), [UNK] ) [UNK]\n",
            "Chunk 45: k ) 2ℓ ). ( 1. 53 ) one also has dh ( u ( ℓ ), [UNK] ) [UNK] ( p ( ℓ ) 1m, a ) 1−λ ( k ) dh ( v ( ℓ ), [UNK] ) [UNK] ( p ( ℓ ), [UNK], b ) 1−λ ( k ) ( 1. 54 ) where we denoted p ( ℓ ) def. = diag ( u ( ℓ ) ) kdiag ( v ( ℓ ) ). lastly, one has [UNK] ( p ( ℓ ) ) −log ( [UNK] ) [UNK] ( u ( ℓ ), [UNK] ) + dh ( v ( ℓ ), [UNK] ) ( 1. 55 ) where [UNK] the unique solution of ( 1. 39 ). proof. one notice that for any ( v, v ′ ) ∈ ( rm +, ∗ ) 2, one has dh ( v, v ′ ) = dh ( v / v ′, 1m ) = dh ( 1m / v, 1m / v ′ ). this shows that dh ( u ( ℓ + 1 ), [UNK] ) = dh ( a kv ( ℓ ), a [UNK] ) = dh ( kv ( ℓ ), [UNK] ) [UNK] ( k ) dh ( v ( ℓ ), [UNK] ). where we used theorem 2. this shows ( 1. 53 ). one also has, using the triangular inequality dh ( u ( ℓ ), [UNK] ) [UNK] ( u ( ℓ + 1 ), u ( ℓ ) ) + dh ( u ( ℓ + 1 ), [UNK] ) [UNK] ( a kv ( ℓ ), u ( ℓ ) ) + λ ( k ) dh ( u ( ℓ ), [UNK] ) = dh ( a, u ( ℓ ) [UNK] ( kv ( ℓ ) ) ) + λ ( k ) dh ( u ( ℓ ), [UNK] ), which gives the ﬁrst part of ( 1.\n",
            "Chunk 46: ( u ( ℓ ), [UNK] ) = dh ( a, u ( ℓ ) [UNK] ( kv ( ℓ ) ) ) + λ ( k ) dh ( u ( ℓ ), [UNK] ), which gives the ﬁrst part of ( 1. 54 ) since u ( ℓ ) [UNK] ( kv ( ℓ ) ) = p ( ℓ ) 1m ( the second one being similar ). the proof of ( 1. 55 ) follows from [?, lemma 3 ] the bound ( 1. 54 ) shows that some error measures on the marginal constraints violation, for instance [UNK] ( ℓ ) [UNK] ( ℓ ) [UNK], are useful stopping criteria to monitor the convergence. figure 1. 14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate degrades as ε→0. these results are proved in [? ] and are tightly connected to nonlinear perron - frobenius theory [? ]. perron - frobenius theory corresponds to the linearization of the iterations, see (?? ). this convergence analysis is extended in [? ], who shows that each iteration of sinkhorn increases the permanent of the scaled coupling matrix. 22 regularized dual and log - domain computations the following proposition details the dual problem associated to ( 1. 39 ). proposition 8. one has lε c ( a, b ) = max f∈rn, g∈rm ⟨ f, a ⟩ + ⟨ g, b ⟩ −ε ⟨ ef / ε, keg / ε ⟩. ( 1. 56 ) the optimal ( f, g ) are linked to scalings ( u, v ) appearing in ( 1. 48 ) through ( u, v ) = ( ef / ε, eg / ε ). ( 1. 57 ) proof. we start from the end of the proof of proposition 7, which links the optimal primal\n",
            "Chunk 47: appearing in ( 1. 48 ) through ( u, v ) = ( ef / ε, eg / ε ). ( 1. 57 ) proof. we start from the end of the proof of proposition 7, which links the optimal primal solution p and dual multipliers fandgfor the marginal constraints as pi, j = efi / εe−ci, j / εegj / ε. substituting in the lagrangiane ( p, f, g ) of equation ( 1. 5 ) the optimal pas a function of fandg, we obtain that the lagrange dual function equals f, g↦→ ⟨ ef / ε, ( [UNK] ) eg / ε ⟩ −εh ( diag ( ef / ε ) kdiag ( eg / ε ) ). ( 1. 58 ) the entropy of pscaled byε, namelyε ⟨ p, logp−1n×m ⟩ can be stated explicitly as a function of f, g, c ⟨ diag ( ef / ε ) kdiag ( eg / ε ), f1mt + 1ngt−c−ε1n×m ⟩ = − ⟨ ef / ε, ( [UNK] ) eg / ε ⟩ + ⟨ f, a ⟩ + ⟨ g, b ⟩ −ε ⟨ ef / ε, keg / ε ⟩ therefore, the ﬁrst term in ( 1. 58 ) cancels out with the ﬁrst term in the entropy above. the remaining times are those displayed in ( 1. 56 ). remark 14. dual for generic measures for generic ( non - necessarily discrete ) input measures ( α, β ), the dual problem ( 1. 56 ) reads sup f, g∈c ( x ) ×c ( y ) [UNK] xf ( x ) dα ( x ) + [UNK] yg\n",
            "Chunk 48: necessarily discrete ) input measures ( α, β ), the dual problem ( 1. 56 ) reads sup f, g∈c ( x ) ×c ( y ) [UNK] xf ( x ) dα ( x ) + [UNK] yg ( x ) dβ ( x ) [UNK] x×ye−c ( x, y ) + f ( x ) + g ( y ) ε dα ( x ) dβ ( y ) this corresponds to a smoothing of the constraint r ( c ) appearing in the original problem ( 1. 21 ), which is retrieved in the limit ε→0. proving existence ( i. e. the sup is actually a max ) of these kantorovich potentials ( f, g ) in the case of entropic transport is less easy than for classical ot ( because one cannot usec - transform and potentials are not automatically lipschitz ). proof of existence can be done using the convergence of sinkhorn iterations, see [? ] for more details. remark 15 ( sinkhorn as a block coordinate ascent on the dual problem ). a simple approach to solve the unconstrained maximization problem ( 1. 56 ) is to use an exact block coordinate ascent strategy, namely to update alternatively fandgto cancel their gradients with respect to the objective of ( 1. 56 ). indeed, one can easily notice that, writing q ( f, g ) for the objective of ( 1. 56 ) that ∇ | fq ( f, g ) = a−ef / [UNK] ( keg / ε ), ( 1. 59 ) ∇ | gq ( f, g ) = b−eg / [UNK] ( ktef / ε ). ( 1. 60 ) block coordinate ascent can therefore be implemented in a closed form by applying successively the following updates, starting from any arbitrary g ( 0 ), [UNK],\n",
            "Chunk 49: g ) = b−eg / [UNK] ( ktef / ε ). ( 1. 60 ) block coordinate ascent can therefore be implemented in a closed form by applying successively the following updates, starting from any arbitrary g ( 0 ), [UNK], f ( ℓ + 1 ) = εloga−εlog ( keg ( ℓ ) / ε ), ( 1. 61 ) g ( ℓ + 1 ) = εlogb−εlog ( ktef ( ℓ + 1 ) / ε ). ( 1. 62 ) such iterations are mathematically equivalent to the sinkhorn iterations ( 1. 51 ) when considering the primal - dual relations highlighted in ( 1. 57 ). indeed, we recover that at any iteration ( f ( ℓ ), g ( ℓ ) ) = ε ( log ( u ( ℓ ) ), log ( v ( ℓ ) ) ). 23 remark 16 ( soft - min rewriting ). iterations ( 1. 61 ) and ( 1. 62 ) can be given an alternative interpretation, using the following notation. given a vector zof real numbers we write min εzfor the soft - minimum of its coordinates, namely minεz = [UNK] ie−zi / ε. note that min ε ( z ) converges to min zfor any vector zasε→0. indeed, min εcan be interpreted as a [UNK] approximation of the min function. using these notations, equations ( 1. 61 ) and ( 1. 62 ) can be rewritten ( f ( ℓ + 1 ) ) i = minε ( cij−g ( ℓ ) j ) j + εlogai, ( 1. 63 ) ( g ( ℓ + 1 ) ) j = minε ( cij−f ( ℓ ) i ) i + εlogbj. ( 1. 64 ) here the term min ε ( cij−\n",
            "Chunk 50: ##ai, ( 1. 63 ) ( g ( ℓ + 1 ) ) j = minε ( cij−f ( ℓ ) i ) i + εlogbj. ( 1. 64 ) here the term min ε ( cij−g ( ℓ ) j ) jdenotes the soft - minimum of all values of the j - th column of matrix ( c−1n ( g ( ℓ ) ) [UNK] ). to simplify notations, we introduce an operator that takes a matrix as input and outputs now a column vector of the soft - minimum values of its columns or rows. namely, for any matrix a∈rn×m, we deﬁne minrow ε ( a ) def. = ( minε ( ai, j ) j ) i∈rn, mincol ε ( a ) def. = ( minε ( ai, j ) i ) j∈rm. note that these operations are equivalent to the entropic c - transform introduced in §?? ( see in particu - lar (?? ) ). using these notations, sinkhorn ’ s iterates read f ( ℓ + 1 ) = minrow ε ( c−1ng ( ℓ ) t ) + εloga, ( 1. 65 ) g ( ℓ + 1 ) = mincol ε ( c−f ( ℓ ) 1mt ) + εlogb. ( 1. 66 ) note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε = 0, because alternate minimization does not converge for constrained problems ( which is the case for the un - regularized dual ( 1. 17 ) ). remark 17 ( log - domain sinkhorn ). while mathematically equivalent to the sinkhorn updates ( 1. 51 ), itera - tions ( 1. 63 ) and\n",
            "Chunk 51: for the un - regularized dual ( 1. 17 ) ). remark 17 ( log - domain sinkhorn ). while mathematically equivalent to the sinkhorn updates ( 1. 51 ), itera - tions ( 1. 63 ) and ( 1. 64 ) suggest to use the log - sum - exp stabilization trick to avoid underﬂow for small values ofε. writing z = min z, that trick suggests to evaluate min εzas minεz = [UNK] ie− ( zi−z ) / ε. ( 1. 67 ) instead of substracting z to stabilize the log domain iterations as in ( 1. 67 ), one can actually substract the previously computed scalings. this leads to the following stabilized iteration f ( ℓ + 1 ) = minrow ε ( s ( f ( ℓ ), g ( ℓ ) ) ) −f ( ℓ ) + εlog ( a ) ( 1. 68 ) g ( ℓ + 1 ) = mincol ε ( s ( f ( ℓ + 1 ), g ( ℓ ) ) ) −g ( ℓ ) + εlog ( b ), ( 1. 69 ) where we deﬁned s ( f, g ) = ( ci, j−fi−gj ) i, j. in contrast to the original iterations ( 1. 51 ), these log - domain iterations ( 1. 68 ) and ( 1. 69 ) are stable for arbitraryε > 0, because the quantity s ( f, g ) stays bounded during the iterations. the downside is that it requiresnmcomputations of exp at each step. computing a minrow εor mincol εis typically substantially slower than matrix multiplications, and requires computing line by line soft - minima of matrices s. there is therefore no [UNK] way to parallelize the application of sinkhorn maps for\n",
            "Chunk 52: step. computing a minrow εor mincol εis typically substantially slower than matrix multiplications, and requires computing line by line soft - minima of matrices s. there is therefore no [UNK] way to parallelize the application of sinkhorn maps for several marginals simultaneously. in euclidean domain of small dimension, it is possible to develop [UNK] multiscale solvers with a decaying εstrategy to signiﬁcantly speed up the computation using sparse grids [? ]. 24 1. 6 extensions wasserstein barycenters. given input histogram { bs } s s = 1, wherebs∈σns, and weights λ∈σs, a wasserstein barycenter is computed by minimizing min [UNK] s = 1λslcs ( a, bs ) ( 1. 70 ) where the cost matrices cs∈rn×nsneed to be speciﬁed. a typical setup is “ eulerian ”, so that all the barycenters are deﬁned on the same grid, ns = n, cs = c = dpis set to be a distance matrix, so that one solves min [UNK] s = 1λswp p ( a, bs ). this barycenter problem ( 1. 70 ) was originally introduced by [? ] following earlier ideas of [? ]. they proved in particular uniqueness of the barycenter for c ( x, y ) = | | x−y | | 2overx = rd, if one of the input measure has a density with respect to the lebesgue measure ( and more generally under the same hypothesis as the one guaranteeing the existence of a monge map, see remark?? ). the barycenter problem for histograms ( 1. 70 ) is in fact a linear program, since one can look for the\n",
            "Chunk 53: same hypothesis as the one guaranteeing the existence of a monge map, see remark?? ). the barycenter problem for histograms ( 1. 70 ) is in fact a linear program, since one can look for the s couplings ( ps ) sbetween each input and the barycenter itself min a∈σn, ( ps∈rn×ns ) s { [UNK] s = 1λs ⟨ ps, cs ⟩ ; [UNK], [UNK] s1ns = a, [UNK] s1n = bs }. although this problem is an lp, its scale forbids the use generic solvers for medium scale problems. one can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [? ]. remark 18. barycenter of arbitrary measures given a set of input measure ( βs ) sdeﬁned on some space x, the barycenter problem becomes min α∈m1 + ( x ) [UNK] s = 1λslc ( α, βs ). ( 1. 71 ) in the case where x = rdandc ( x, y ) = | | x−y | | 2, [? ] shows that if one of the input measures has a density, then this barycenter is unique. problem ( 1. 71 ) can be viewed as a generalization of the problem of computing barycenters of points ( xs ) s s = 1∈xsto arbitrary measures. indeed, if βs = δxsis a single dirac mass, then a solution to ( 1. 71 ) is [UNK] a fr´ echet mean solving (?? ). note that for c ( x, y ) = | | x−y | | 2, the mean of the barycenter [UNK] necessarily the barycenter of the mean, i\n",
            "Chunk 54: fr´ echet mean solving (?? ). note that for c ( x, y ) = | | x−y | | 2, the mean of the barycenter [UNK] necessarily the barycenter of the mean, i. e. [UNK] [UNK] ( x ) = [UNK] [UNK] xxdαs ( x ), and the support of [UNK] located in the convex hull of the supports of the ( αs ) s. the consistency of the approximation of the inﬁnite dimensional optimization ( 1. 71 ) when approximating the input distribution using discrete ones ( and thus solving ( 1. 70 ) in place ) is studied in [? ]. let us also note that it is possible to re - cast ( 1. 71 ) as a multi - marginal ot problem, see remark??. one can use entropic smoothing and approximate the solution of ( 1. 70 ) using min [UNK] s = 1λslε cs ( a, bs ) ( 1. 72 ) for someε > 0. this is a smooth convex minimization problem, which can be tackled using gradient descent [? ]. an alternative is to use descent method ( typically quasi - newton ) on the semi - dual [? ], which is 25 useful to integrate additional regularizations on the barycenter ( e. g. to impose some smoothness ). a simple but [UNK] approach, as remarked in [? ] is to rewrite ( 1. 72 ) as a ( weighted ) kl projection problem min ( ps ) s { [UNK] sλskl ( ps | ks ) ; [UNK], pst1m = bs, p111 =... = ps1s } ( 1. 73 ) where we denoted ksdef. = e−cs / ε. here, the barycenter ais implicitly encoded in the row marginals of\n",
            "Chunk 55: , p111 =... = ps1s } ( 1. 73 ) where we denoted ksdef. = e−cs / ε. here, the barycenter ais implicitly encoded in the row marginals of all the couplings ps∈rn×nsasa = p111 =... = ps1s. as detailed in [? ], one can generalize sinkhorn to this problem, which also corresponds to iterative projection. this can also be seen as a special case of the generalized sinkhorn detailed in §??. the optimal couplings ( ps ) ssolving ( 1. 73 ) are computed in scaling form as ps = diag ( us ) kdiag ( vs ), ( 1. 74 ) and the scalings are sequentially updated as [UNK], sk, v ( ℓ + 1 ) sdef. = bs kt su ( ℓ ) s, ( 1. 75 ) [UNK], sk, u ( ℓ + 1 ) sdef. = a ( ℓ + 1 ) ksv ( ℓ + 1 ) s, ( 1. 76 ) where a ( ℓ + 1 ) def. = [UNK] s ( ksv ( ℓ + 1 ) s ) λs. ( 1. 77 ) an alternative way to derive these iterations is to perform alternate minimization on the variables of a dual problem, which detailed in the following proposition. proposition 9. the optimal ( us, vs ) appearing in ( 1. 74 ) can be written as ( us, vs ) = ( efs / ε, egs / ε ) where ( fs, gs ) sare the solutions of the following program ( whose value matches the one of ( 1. 72 ) ) max ( fs, gs ) s { [UNK] sλs ( ⟨ gs, bs ⟩ −ε ⟨ ksegs / ε, e\n",
            "Chunk 56: sare the solutions of the following program ( whose value matches the one of ( 1. 72 ) ) max ( fs, gs ) s { [UNK] sλs ( ⟨ gs, bs ⟩ −ε ⟨ ksegs / ε, efs / ε ⟩ ) ; [UNK] sλsfs = 0 }. ( 1. 78 ) proof. introducing lagrange multipliers in ( 1. 73 ) leads to min ( ps ) s, amax ( fs, gs ) [UNK] sλs ( εkl ( ps | ks ) + ⟨ a−ps1m, fs ⟩ + ⟨ bs−pst1m, gs ⟩ ). strong duality holds, so that one can exchange the min and the max, and gets max ( fs, gs ) [UNK] sλs ( ⟨ gs, bs ⟩ + min psεkl ( ps | ks ) − ⟨ ps, fs⊕gs ⟩ ) + min a ⟨ [UNK] sλsfs, a ⟩. the explicit minimization on agives the [UNK] sλsfs = 0 together with max ( fs, gs ) [UNK] sλs ⟨ gs, bs ⟩ −εkl∗ ( fs⊕gs ε | ks ) where kl∗ ( · | ks ) is the legendre transform (?? ) of the function kl∗ ( · | ks ). this legendre transform reads kl∗ ( u | k ) = [UNK] i, jki, j ( eui, j−1 ), ( 1. 79 ) 26 figure 1. 15 : barycenters between 4 input 3 - d shapes using entropic regularization ( 1. 72 ). the weights ( λs ) sare bilinear with respect to the four corners of the square. shapes are represented as measures that are uniform within the boundaries of the shape and\n",
            "Chunk 57: d shapes using entropic regularization ( 1. 72 ). the weights ( λs ) sare bilinear with respect to the four corners of the square. shapes are represented as measures that are uniform within the boundaries of the shape and null outside. which shows the desired formula. to show ( 1. 79 ), since this function is separable, one needs to compute [UNK] ( u, k ) ∈r2 +, kl∗ ( u | k ) def. = max rur− ( rlog ( r / k ) −r + k ) whose optimality condition reads u = log ( r / k ), i. e. r = keu, hence the result. minimizing ( 1. 78 ) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed form by ( 1. 75 ). minimizing ( 1. 78 ) with respect to all the ( fs ) srequires to solve for ausing ( 1. 77 ) and leads to the expression ( 1. 76 ). figures?? and?? show applications to 2 - d and 3 - d shapes interpolation. figure?? shows a computation of barycenters on a surface, where the ground cost is the square of the geodesic distance. for this ﬁgure, the computations are performed using the geodesic in heat approximation detailed in remark??. we refer to [? ] for more details and other applications to computer graphics and imaging sciences. wasserstein loss. in statistics, text processing or imaging, one must usually compare a probability distribution βarising from measurements to a model, namely a parameterized family of distributions { αθ, θ∈ θ } where θ is a subset of an euclidean space. such a comparison is done through a “ loss ” or a “ ﬁdel\n",
            "Chunk 58: distribution βarising from measurements to a model, namely a parameterized family of distributions { αθ, θ∈ θ } where θ is a subset of an euclidean space. such a comparison is done through a “ loss ” or a “ ﬁdelity ” term, which, in this section, is the wasserstein distance. in the simplest scenario, the computation of a suitable parameter θis obtained by minimizing directly min θ∈θe ( θ ) def. = lc ( αθ, β ). ( 1. 80 ) of course, one can consider more complicated problems : for instance, the barycenter problem described in §?? consists in a sum of such terms. however, most of these more advanced problems can be usually solved by adapting tools deﬁned for basic case : either using the chain rule to compute explicitly derivatives, or using automatic [UNK]. the wasserstein distance between two histograms or two densities is convex with respect to these inputs, as shown by ( 1. 17 ) and ( 1. 21 ) respectively. therefore, when the parameter θis itself a histogram, namely θ = σnandαθ = θ, or more generally when θdescribeskweights in the simplex, θ = σ k, andαθ = [UNK] i = 1θiαi is a convex combination of known atoms α1,..., αkin σn, problem ( 1. 80 ) remains convex ( the ﬁrst case corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with a wasserstein loss [? ] ). however, for more general parameterizations θ↦→αθ, problem ( 1. 80 ) is in general not convex. 27 [UNK] 1. 16 : schematic display of the density ﬁtting problem 1. 81. a practical\n",
            "Chunk 59: ] ). however, for more general parameterizations θ↦→αθ, problem ( 1. 80 ) is in general not convex. 27 [UNK] 1. 16 : schematic display of the density ﬁtting problem 1. 81. a practical problem of paramount importance in statistic and machine learning is density ﬁtting. given some discrete samples ( xi ) n i = 1⊂x from some unknown distribution, the goal is to ﬁt a parametric model θ↦→αθ∈m ( x ) to the observed empirical input measure β min θ∈θl ( αθ, β ) where β = 1 [UNK] iδxi, ( 1. 81 ) wherelis some “ loss ” function between a discrete and a “ continuous ” ( arbitrary ) distribution ( see fig - ure 1. 16 ). in the case where αθas a densify ρθdef. = ραθwith respect to the lebesgue measure ( or any other ﬁxed reference measure ), the maximum likelihood estimator ( mle ) is obtained by solving min θlmle ( αθ, β ) def. = [UNK] ilog ( ρθ ( xi ) ). this corresponds to using an empirical counterpart of a kullback - leibler loss since, assuming the xiare i. i. d. samples of some [UNK], then lmle ( α, β ) n→ + ∞−→ kl ( α | [UNK] ) this mle approach is known to lead to optimal estimation procedures in many cases ( see for instance [? ] ). however, it fails to work when estimating singular distributions, typically when the αθdoes not has a density ( so thatlmle ( αθ, β ) = + ∞ ) or when ( xi ) iare samples from some singular [UNK] ( so that the αθshould share the same support as β\n",
            "Chunk 60: the αθdoes not has a density ( so thatlmle ( αθ, β ) = + ∞ ) or when ( xi ) iare samples from some singular [UNK] ( so that the αθshould share the same support as βfor kl ( α | [UNK] ) to be ﬁnite, but this support is usually unknown ). another issue is that in several cases of practical interest, the density ρθis inaccessible ( or too hard to compute ). a typical setup where both problems ( singular and unknown densities ) occur is for so - called generative models, where the parametric measure is written as a push - forward of a ﬁxed reference measure ζ∈m ( z ) αθ = hθ, ♯ζwherehθ : z→x where the push - forward operator is introduced in deﬁnition 1. the space zis usually low - dimensional, so that the support of αθis localized along a low - dimensional “ manifold ” and the resulting density is highly singular ( it does not have a density with respect to lebesgue measure ). furthermore, computing this density is usually intractable, while generating i. i. d. samples from αθis achieved by computing xi = hθ ( zi ) where ( zi ) iare i. i. d. samples from ζ. in order to cope with such [UNK] scenario, one has to use weak metrics in place of the mle functional lmle, which needs to be written in dual form as l ( α, β ) def. = max ( f, g ) ∈c ( x ) 2 { [UNK] xf ( x ) dα ( x ) + [UNK] xg ( x ) dβ ( x ) ; ( f, g ) ∈r }. ( 1. 82 ) dual norms exposed in §?? correspond to imposing r = {\n",
            "Chunk 61: xf ( x ) dα ( x ) + [UNK] xg ( x ) dβ ( x ) ; ( f, g ) ∈r }. ( 1. 82 ) dual norms exposed in §?? correspond to imposing r = { ( f, −f ) ; f∈b }, while optimal transport ( 1. 21 ) setsr = r ( c ) as deﬁned in ( 1. 22 ). 28 for a ﬁxed θ, evaluating the energy to be minimized in ( 1. 81 ) using such a loss function corresponds to solving a semi - discrete optimal transport, which is the focus of chapter??. minimizing the energy with respect toθis much more involved, and is typically highly non - convex. the class of estimators obtained using l = lc, often called “ minimum kantorovitch estimators ” ( mke ), was initially introduced in [? ], see also [? ]. gromov - wasserstein. optimal transport needs a ground cost cto compare histograms ( a, b ), it can thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre - register these spaces to deﬁne a ground cost. to address this issue, one can instead only assume a weaker assumption, namely that one has at its disposal two matrices d∈rn×nandd ′ ∈rm×mthat represent some relationship between the points on which the histograms are deﬁned. a typical scenario is when these matrices are ( power of ) distance matrices. the gromov - wasserstein problem reads gw ( ( a, d ), ( b, d ′ ) ) 2def. = min p∈u ( a, b ) ed, d ′ ( p ) def. = [UNK] i,\n",
            "Chunk 62: ##omov - wasserstein problem reads gw ( ( a, d ), ( b, d ′ ) ) 2def. = min p∈u ( a, b ) ed, d ′ ( p ) def. = [UNK] i, j, i ′, j ′ | di, i ′ −d ′ j, j ′ | 2pi, jpi ′, j ′. ( 1. 83 ) this is a non - convex problem, which can be recast as a quadratic assignment problem ( qap ) [? ] and is in full generality np - hard to solve for arbitrary inputs. it is in fact equivalent to a graph matching problem [? ] for a particular cost. one can show that gw satisﬁes the triangular inequality, and in fact it deﬁnes a distance between metric spaces equipped with a probability distribution ( here assumed to be discrete in deﬁnition ( 1. 83 ) ) up to isometries preserving the measures. this distance was introduced and studied in details by memoli in [? ]. an in - depth mathematical exposition ( in particular, its geodesic structure and gradient ﬂows ) is given in [? ]. see also [? ] for applications in computer vision. this distance is also tightly connected with the gromov - [UNK] distance [? ] between metric spaces, which have been used for shape matching [?,? ]. remark 19. gromov - wasserstein distance the general setting corresponds to computing couplings between metric measure spaces ( x, dx, αx ) and ( y, dy, αy ) where ( dx, dy ) are distances and ( αx, αy ) are measures on their respective spaces. one deﬁnes gw ( ( αx, dx ), ( αy, dy ) ) 2def. = min\n",
            "Chunk 63: ##x, dy ) are distances and ( αx, αy ) are measures on their respective spaces. one deﬁnes gw ( ( αx, dx ), ( αy, dy ) ) 2def. = min π∈u ( αx, αy ) [UNK] x2×y2 | dx ( x, x ′ ) −dy ( y, y ′ ) | 2dπ ( x, y ) dπ ( x ′, y ′ ). ( 1. 84 ) gw deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αx, dx ) and ( αy, dy ) are isometric if there exists [UNK] : x→y such [UNK] = αyanddy ( [UNK] ( x ), [UNK] ( x ′ ) ) = dx ( x, x ′ ). remark 20. gromov - wasserstein geodesics the space of metric spaces ( up to isometries ) endowed with thisgw distance ( 1. 84 ) has a geodesic structure. [? ] shows that the geodesic between ( x0, dx0, α0 ) and ( x1, dx1, α1 ) can be chosen to be t∈ [ 0, 1 ] ↦→ ( x0×x 1, dt, [UNK] ) [UNK] a solution of ( 1. 84 ) and for all ( ( x0, x1 ), ( x ′ 0, x ′ 1 ) ) ∈ ( x0×x 1 ) 2, dt ( ( x0, x1 ), ( x ′ 0, x ′ 1 ) ) def. = ( 1−t ) dx0 ( x0, x ′ 0 ) + tdx1 ( x1, x ′ 1 ). this formula allows one to deﬁne and\n",
            "Chunk 64: x ′ 0, x ′ 1 ) ) def. = ( 1−t ) dx0 ( x0, x ′ 0 ) + tdx1 ( x1, x ′ 1 ). this formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric spaces, see [? ]. it is however [UNK] to handle numerically, because it involves computations over the product spacex0×x 1. a heuristic approach is used in [? ] to deﬁne geodesics and barycenters of metric measure spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing ( 1. 85 ) detailed below. to approximate the computation of gw, and to help convergence of minimization schemes to better minima, one can consider the entropic regularized variant min p∈u ( a, b ) ed, d ′ ( p ) −εh ( p ). ( 1. 85 ) 29 figure 1. 17 : example of fuzzy correspondences computed by solving gw problem ( 1. 85 ) with sinkhorn iterations ( 1. 86 ). extracted from [? ]. as proposed initially in [?,? ], and later revisited in [? ] for applications in graphics, one can use iteratively sinkhorn ’ s algorithm to progressively compute a stationary point of ( 1. 85 ). indeed, successive linearizations of the objective function lead to consider the succession of updates p ( ℓ + 1 ) def. = min p∈u ( a, b ) ⟨ p, c ( ℓ ) ⟩ −εh ( p ) where ( 1. 86 ) c ( ℓ ) def. = ∇ed, d ′ ( p ( ℓ ) ) = −d ′ tp ( ℓ ) d, which can be interpreted as a mirror - descent scheme [?\n",
            "Chunk 65: ( p ) where ( 1. 86 ) c ( ℓ ) def. = ∇ed, d ′ ( p ( ℓ ) ) = −d ′ tp ( ℓ ) d, which can be interpreted as a mirror - descent scheme [? ]. each update can thus be solved using sinkhorn iterations ( 1. 51 ) with cost c ( ℓ ). figure ( 1. 17 ) illustrates the use of this entropic gromov - wasserstein to compute soft maps between domains. 30 bibliography [ 1 ] amir beck. introduction to nonlinear optimization : theory, algorithms, and applications with mat - lab. siam, 2014. [ 2 ] stephen boyd, neal parikh, eric chu, borja peleato, and jonathan eckstein. distributed optimization and statistical learning via the alternating direction method of multipliers. foundations and trends [UNK] in machine learning, 3 ( 1 ) : 1 – 122, 2011. [ 3 ] stephen boyd and lieven vandenberghe. convex optimization. cambridge university press, 2004. [ 4 ] e. cand ` es and d. donoho. new tight frames of curvelets and optimal representations of objects with piecewise c2singularities. commun. on pure and appl. math., 57 ( 2 ) : 219 – 266, 2004. [ 5 ] e. j. cand ` es, l. demanet, d. l. donoho, and l. ying. fast discrete curvelet transforms. siam multiscale modeling and simulation, 5 : 861 – 899, 2005. [ 6 ] a. chambolle. an algorithm for total variation minimization and applications. j. math. imaging vis., 20 : 89 – 97, 2004. [ 7 ] antonin chambolle, vicent caselles, daniel cremers, matteo novaga\n",
            "Chunk 66: . an algorithm for total variation minimization and applications. j. math. imaging vis., 20 : 89 – 97, 2004. [ 7 ] antonin chambolle, vicent caselles, daniel cremers, matteo novaga, and thomas pock. an intro - duction to total variation for image analysis. theoretical foundations and numerical methods for sparse recovery, 9 ( 263 - 340 ) : 227, 2010. [ 8 ] antonin chambolle and thomas pock. an introduction to continuous optimization for imaging. acta numerica, 25 : 161 – 319, 2016. [ 9 ] s. s. chen, d. l. donoho, and m. a. saunders. atomic decomposition by basis pursuit. siam journal on scientiﬁc computing, 20 ( 1 ) : 33 – 61, 1999. [ 10 ] philippe g ciarlet. introduction ` a l ’ analyse num´ erique matricielle et ` a l ’ optimisation. 1982. [ 11 ] p. l. combettes and v. r. wajs. signal recovery by proximal forward - backward splitting. siam multiscale modeling and simulation, 4 ( 4 ), 2005. [ 12 ] i. daubechies, m. defrise, and c. de mol. an iterative thresholding algorithm for linear inverse problems with a sparsity constraint. commun. on pure and appl. math., 57 : 1413 – 1541, 2004. [ 13 ] d. donoho and i. johnstone. ideal spatial adaptation via wavelet shrinkage. biometrika, 81 : 425 – 455, dec 1994. [ 14 ] heinz werner engl, martin hanke, and andreas neubauer. regularization of inverse problems, volume 375. springer science & business media, 1996\n",
            "Chunk 67: ##age. biometrika, 81 : 425 – 455, dec 1994. [ 14 ] heinz werner engl, martin hanke, and andreas neubauer. regularization of inverse problems, volume 375. springer science & business media, 1996. [ 15 ] m. figueiredo and r. nowak. an em algorithm for wavelet - based image restoration. ieee trans. image proc., 12 ( 8 ) : 906 – 916, 2003. [ 16 ] simon foucart and holger rauhut. a mathematical introduction to compressive sensing, volume 1. birkh¨ auser basel, 2013. 31 [ 17 ] stephane mallat. a wavelet tour of signal processing : the sparse way. academic press, 2008. [ 18 ] d. mumford and j. shah. optimal approximation by piecewise smooth functions and associated varia - tional problems. commun. on pure and appl. math., 42 : 577 – 685, 1989. [ 19 ] neal parikh, stephen boyd, et al. proximal algorithms. foundations and trends [UNK] optimization, 1 ( 3 ) : 127 – 239, 2014. [ 20 ] gabriel peyr´ e. l ’ alg ` ebre discr ` ete de la transform´ ee de fourier. ellipses, 2004. [ 21 ] j. portilla, v. strela, m. j. wainwright, and simoncelli e. p. image denoising using scale mixtures of gaussians in the wavelet domain. ieee trans. image proc., 12 ( 11 ) : 1338 – 1351, november 2003. [ 22 ] l. i. rudin, s. osher, and e. fatemi. nonlinear total variation based noise removal algorithms. phys. d, 60 (\n",
            "Chunk 68: 12 ( 11 ) : 1338 – 1351, november 2003. [ 22 ] l. i. rudin, s. osher, and e. fatemi. nonlinear total variation based noise removal algorithms. phys. d, 60 ( 1 - 4 ) : 259 – 268, 1992. [ 23 ] otmar scherzer, markus grasmair, harald grossauer, markus haltmeier, frank lenzen, and l sirovich. variational methods in imaging. springer, 2009. [ 24 ] c. e. shannon. a mathematical theory of communication. the bell system technical journal, 27 ( 3 ) : 379 – 423, 1948. [ 25 ] jean - luc starck, fionn murtagh, and jalal fadili. sparse image and signal processing : wavelets and related geometric multiscale analysis. cambridge university press, 2015. 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36un5gZLleJK",
        "outputId": "01e91a5d-a02c-4cb5-ae1c-5cad2fbc63f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 254, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 279, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 303, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 299, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 445, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 234, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 212, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 349, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 287, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 299, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 217, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 253, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 250, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 222, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 250, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 204, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 380, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 315, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 238, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 220, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 227, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 216, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 333, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 405, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 219, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 298, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 253, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 264, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 283, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 218, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 212, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 464, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 251, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 210, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 219, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 275, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 238, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 236, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 253, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 202, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 204, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 329, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 205, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 224, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 368, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 254, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 431, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 212, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 213, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 259, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 251, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 261, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 325, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 343, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 449, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 240, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 289, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 328, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 237, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 256, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 223, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 258, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 301, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 232, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 270, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 243, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 226, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 238, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 240, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 277, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 227, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 280, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 227, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 262, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 219, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 202, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 278, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 239, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 345, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 232, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 207, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 392, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 213, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 218, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 294, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 203, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 219, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 222, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 224, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 244, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 229, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 249, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 215, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures.\n",
            "Chunk 2: We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "Chunk 3: A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x.\n",
            "Chunk 4: Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Chunk 5: Remark 1 (General measures) .A\n",
            "\n",
            "convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework.\n",
            "\n",
            "Such objects only need to be modelled as measures.\n",
            "Chunk 6: This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Chunk 7: Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x)\n",
            "\n",
            "=n∑\n",
            "i=1aif(xi).\n",
            "\n",
            "\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x)\n",
            "Chunk 8: =ρα(x)dxw.r.t.\n",
            "\n",
            "the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "Chunk 9: An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Chunk 10: Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions).\n",
            "\n",
            "For instance, the derivative of a Dirac is not a measure.\n",
            "Chunk 11: We denote M+(X) the\n",
            "set of all positive measures on X.\n",
            "\n",
            "The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1.\n",
            "Chunk 12: Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "\n",
            "\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1:\n",
            "Chunk 13: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x)\n",
            "Chunk 14: =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Chunk 15: Operators on measures.\n",
            "\n",
            "For some continuous map T:X\n",
            "\n",
            "→Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y).\n",
            "Chunk 16: For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "Chunk 17: For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures.\n",
            "Chunk 18: The formal deﬁnition reads as follow.\n",
            "\n",
            "\n",
            "Deﬁnition 1 (Push-forward)\n",
            "\n",
            ".ForT:X\n",
            "\n",
            "→ Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x).\n",
            "Chunk 19: (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}).\n",
            "\n",
            "(1.3)\n",
            "Note thatT♯preserves positivity and total mass,\n",
            "Chunk 20: so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another.\n",
            "Chunk 21: The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α.\n",
            "Chunk 22: Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x)\n",
            "Chunk 23: =|det(T′(x))|ρβ(T(x))\n",
            "\n",
            "(1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT).\n",
            "\n",
            "This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "Chunk 24: 4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2:\n",
            "Chunk 25: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions.\n",
            "Chunk 26: It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "Chunk 27: It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4).\n",
            "Chunk 28: This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method.\n",
            "Chunk 29: Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "\n",
            "\n",
            "Remark 4 (Measures and random variables)\n",
            "Chunk 30: Remark 4 (Measures and random variables)\n",
            "\n",
            ".Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables.\n",
            "Chunk 31: A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x).\n",
            "Chunk 32: Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X\n",
            "\n",
            "→Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :\n",
            "Chunk 33: ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable.\n",
            "Chunk 34: Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "Chunk 35: 1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i).\n",
            "Chunk 36: (1.5)\n",
            "\n",
            "\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n).\n",
            "Chunk 37: However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70.\n",
            "Chunk 38: That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "Chunk 39: 5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant.\n",
            "Chunk 40: Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal.\n",
            "\n",
            "(right) a Monge map can associate\n",
            "the blue measure αto the red measure β.\n",
            "Chunk 41: The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location.\n",
            "\n",
            "The mapping here is such that T(x1) =T(x2) =y2,T(x3)\n",
            "\n",
            "=y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Chunk 42: =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "\n",
            "\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Chunk 43: Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3.\n",
            "Chunk 44: In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "Chunk 45: For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β.\n",
            "Chunk 46: This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ".\n",
            "\n",
            "(1.8)\n",
            "Chunk 47: Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "Chunk 48: In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Chunk 49: Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another.\n",
            "Chunk 50: This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure.\n",
            "Chunk 51: For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "Chunk 52: 6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "Chunk 53: The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "Chunk 54: 1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem.\n",
            "Chunk 55: Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size.\n",
            "Chunk 56: A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??).\n",
            "Chunk 57: Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex .\n",
            "Chunk 58: Both are therefore diﬃcult to solve in their original formulation.\n",
            "\n",
            "\n",
            "Kantorovitch formulation for discrete measures.\n",
            "Chunk 59: The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only.\n",
            "Chunk 60: Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations.\n",
            "Chunk 61: Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets.\n",
            "Chunk 62: This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6).\n",
            "Chunk 63: Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "Chunk 64: The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Chunk 65: Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Chunk 66: Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "Chunk 67: This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings.\n",
            "Chunk 68: A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11).\n",
            "Chunk 69: Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Chunk 70: Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Chunk 71: Permutation Matrices as Couplings For a permutation σ∈Perm(n)\n",
            "Chunk 72: , we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Chunk 73: Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n).\n",
            "Chunk 74: Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix.\n",
            "\n",
            "Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "Chunk 75: The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems.\n",
            "Chunk 76: Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "\n",
            "\n",
            "Proposition 1 (Kantorovich for matching) .Ifm\n",
            "Chunk 77: =nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "\n",
            "\n",
            "Proof.\n",
            "Chunk 78: Proof.\n",
            "\n",
            "Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices.\n",
            "Chunk 79: A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "Chunk 80: 8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT.\n",
            "\n",
            "Chapter ?\n",
            "Chunk 81: Chapter ?\n",
            "\n",
            "?is dedicated to the semi-discrete setup.\n",
            "\n",
            "\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density.\n",
            "Chunk 82: The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black).\n",
            "\n",
            "Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6).\n",
            "Chunk 83: The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures.\n",
            "Chunk 84: The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space.\n",
            "Chunk 85: The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj).\n",
            "Chunk 86: In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ".\n",
            "Chunk 87: (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities).\n",
            "Chunk 88: Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y).\n",
            "Chunk 89: (1.14)\n",
            "\n",
            "\n",
            "This is an inﬁnite-dimensional linear program over a space of measures.\n",
            "\n",
            "Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14).\n",
            "Chunk 90: Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "Chunk 91: On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "Chunk 92: ↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below.\n",
            "\n",
            "Inspired by [ ?].\n",
            "is weak-* continuous.\n",
            "Chunk 93: Inspired by [ ?].\n",
            "is weak-* continuous.\n",
            "\n",
            "And the set of constraint is non empty, taking α⊗β.\n",
            "\n",
            "On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "\n",
            "\n",
            "Wasserstein distances.\n",
            "Chunk 94: Wasserstein distances.\n",
            "\n",
            "An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties.\n",
            "Chunk 95: Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "Chunk 96: We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare.\n",
            "Chunk 97: The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "\n",
            "\n",
            "Proposition 2.\n",
            "Chunk 98: We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0\n",
            "Chunk 99: if and only if a=b,\n",
            "\n",
            "and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof.\n",
            "Chunk 100: Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "Chunk 101: To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure.\n",
            "Chunk 102: In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple.\n",
            "\n",
            "Let a,b,c∈Σn.\n",
            "\n",
            "Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively.\n",
            "Chunk 103: We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value).\n",
            "\n",
            "We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "Chunk 104: We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b).\n",
            "Chunk 105: Similarly one veriﬁes that S⊤1n=c.\n",
            "\n",
            "\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "Chunk 106: min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "Chunk 107: The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Chunk 108: Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "\n",
            "\n",
            "Proposition 3.\n",
            "Chunk 109: Proposition 3.\n",
            "\n",
            "We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y)\n",
            "\n",
            "+d(y,z).\n",
            "Chunk 110: Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof.\n",
            "Chunk 111: The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "Chunk 112: The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions.\n",
            "Chunk 113: In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work).\n",
            "Chunk 114: In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y).\n",
            "Chunk 115: Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y).\n",
            "Chunk 116: This shows that Wp(δx,δy)→0 ifx→y.\n",
            "\n",
            "This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "Chunk 117: 11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα.\n",
            "Chunk 118: This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "\n",
            "\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0\n",
            "Chunk 119: [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "\n",
            "\n",
            "Note that there exists alternative distances which also metrize weak convergence.\n",
            "Chunk 120: The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k)\n",
            "\n",
            "=∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:\n",
            "\n",
            "X2→R.\n",
            "Chunk 121: X2→R.\n",
            "\n",
            "The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "Chunk 122: This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Chunk 123: Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex.\n",
            "Chunk 124: In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set.\n",
            "Chunk 125: In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity.\n",
            "Chunk 126: Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "Chunk 127: 1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem.\n",
            "Chunk 128: The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "\n",
            "\n",
            "Proposition 4.\n",
            "Chunk 129: Proposition 4.\n",
            "\n",
            "One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof.\n",
            "Chunk 130: This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4].\n",
            "Chunk 131: The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??.\n",
            "Chunk 132: For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality.\n",
            "\n",
            "The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "Chunk 133: For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ".\n",
            "Chunk 134: (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions).\n",
            "Chunk 135: The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "\n",
            "\n",
            "Proposition 5.\n",
            "\n",
            "One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x)\n",
            "Chunk 136: One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x)\n",
            "\n",
            "+∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x)\n",
            "\n",
            "+g(y)⩽c(x,y)}.\n",
            "\n",
            "(1.22)\n",
            "Chunk 137: +g(y)⩽c(x,y)}.\n",
            "\n",
            "(1.22)\n",
            "\n",
            "\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "Chunk 138: The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)).\n",
            "Chunk 139: The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x)\n",
            "\n",
            "+g(y) =c(x,y)}.\n",
            "Chunk 140: (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive.\n",
            "Chunk 141: Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Chunk 142: Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Chunk 143: Theorem 1 (Brenier) .In\n",
            "Chunk 144: the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd.\n",
            "Chunk 145: This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x).\n",
            "\n",
            "(1.24)\n",
            "\n",
            "\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x)\n",
            "Chunk 146: =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν.\n",
            "\n",
            "This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "\n",
            "\n",
            "Proof.\n",
            "Chunk 147: Proof.\n",
            "\n",
            "We sketch the main ingredients of the proof, more details can be found for instance in [ ?].\n",
            "\n",
            "We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x)\n",
            "Chunk 148: +∫\n",
            "||y||2dβ(y).\n",
            "\n",
            "Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ".\n",
            "Chunk 149: (1.25)\n",
            "13\n",
            "\n",
            "\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g).\n",
            "\n",
            "One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x).\n",
            "Chunk 150: (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)).\n",
            "Chunk 151: Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??\n",
            "Chunk 152: for a generalization of this idea to generic costs c(x,y).\n",
            "\n",
            "By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Chunk 153: Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x)\n",
            "Chunk 154: +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x).\n",
            "Chunk 155: Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere.\n",
            "Chunk 156: This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "Chunk 157: This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight).\n",
            "Chunk 158: This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Chunk 159: Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one.\n",
            "Chunk 160: Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Chunk 161: Note also that this theorem can be extended in many directions.\n",
            "Chunk 162: The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces).\n",
            "Chunk 163: One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "Chunk 164: For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x))\n",
            "Chunk 165: =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ.\n",
            "\n",
            "The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian.\n",
            "Chunk 166: In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x))\n",
            "\n",
            "= 1 +ε∆ϕ(x)\n",
            "\n",
            "+o(ε).\n",
            "Chunk 167: = 1 +ε∆ϕ(x)\n",
            "\n",
            "+o(ε).\n",
            "\n",
            "\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "\n",
            "\n",
            "Special cases In general, computing OT distances is numerically involved.\n",
            "Chunk 168: We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "\n",
            "\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm)\n",
            "Chunk 169: .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1.\n",
            "Chunk 170: One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y.\n",
            "Chunk 171: The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "Chunk 172: 14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "\n",
            "\n",
            "Top: empirical measures with same number of points (optimal matching).\n",
            "Chunk 173: Bottom: generic case.\n",
            "\n",
            "This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX\n",
            "Chunk 174: =R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ.\n",
            "Chunk 175: That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change.\n",
            "Chunk 176: That formula is a simple consequence of the more general\n",
            "remark given below.\n",
            "\n",
            "Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points.\n",
            "Chunk 177: The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures.\n",
            "\n",
            "It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?].\n",
            "Chunk 178: Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Chunk 179: Remark 8 (1-D case – Generic case) .For\n",
            "Chunk 180: a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "Chunk 181: That function is also called the generalized quantile function of α.\n",
            "\n",
            "For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr.\n",
            "\n",
            "(1.31)\n",
            "Chunk 182: This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric.\n",
            "Chunk 183: This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??\n",
            "Chunk 184: and more generally\n",
            "in§??.\n",
            "\n",
            "Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx.\n",
            "Chunk 185: (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions).\n",
            "Chunk 186: An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions.\n",
            "Chunk 187: It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??.\n",
            "Chunk 188: For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "\n",
            "\n",
            "Remark 9 (Distance between Gaussians) .Ifα\n",
            "Chunk 189: =N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ.\n",
            "Chunk 190: Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x))\n",
            "Chunk 191: = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β.\n",
            "Chunk 192: Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "\n",
            "\n",
            "thatTis optimal.\n",
            "Chunk 193: thatTis optimal.\n",
            "\n",
            "Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "Chunk 194: 16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "Chunk 195: andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ".\n",
            "\n",
            "The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "Chunk 196: \u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians.\n",
            "Chunk 197: Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "Chunk 198: With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "Chunk 199: whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root.\n",
            "Chunk 200: One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments.\n",
            "Chunk 201: In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "Chunk 202: For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "Chunk 203: For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "Chunk 204: 1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations.\n",
            "Chunk 205: It operates by adding an entropic regularization penalty to\n",
            "the original problem.\n",
            "Chunk 206: This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "Chunk 207: 17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "\n",
            "\n",
            "Entropic Regularization.\n",
            "Chunk 208: The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative.\n",
            "Chunk 209: The function His 1-strongly concave, because its hessian is ∂2H(P)\n",
            "\n",
            "=−diag(1/Pi,j) and\n",
            "Pi,j⩽1.\n",
            "Chunk 210: The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P).\n",
            "Chunk 211: (1.39)\n",
            "\n",
            "\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution.\n",
            "Chunk 212: The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem.\n",
            "Chunk 213: Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11.\n",
            "Chunk 214: To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Chunk 215: Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D).\n",
            "Chunk 216: Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle.\n",
            "\n",
            "The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle.\n",
            "Chunk 217: This is further detailed in the proposition below.\n",
            "\n",
            "The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Chunk 218: Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "Chunk 219: so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "\n",
            "\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof.\n",
            "\n",
            "We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0.\n",
            "Chunk 220: We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b).\n",
            "Chunk 221: We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "\n",
            "\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)).\n",
            "Chunk 222: (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities.\n",
            "Chunk 223: Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Chunk 224: Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40).\n",
            "Chunk 225: Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40).\n",
            "Chunk 226: Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Chunk 227: Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling.\n",
            "Chunk 228: In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions.\n",
            "Chunk 229: A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp.\n",
            "\n",
            "1/ε)\n",
            "\n",
            "nearε= 0\n",
            "\n",
            "(respε= +∞).\n",
            "\n",
            "Figure 1.13\n",
            "shows visually the eﬀect of these two convergence.\n",
            "Chunk 230: A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Chunk 231: Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K).\n",
            "Chunk 232: (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "Chunk 233: dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ.\n",
            "Chunk 234: It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Chunk 235: Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis\n",
            "\n",
            "the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y).\n",
            "Chunk 236: This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Chunk 237: Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Chunk 238: Sinkhorn’s\n",
            "\n",
            "Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables.\n",
            "Chunk 239: That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "\n",
            "\n",
            "Proposition 7.\n",
            "Chunk 240: Proposition 7.\n",
            "\n",
            "The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof.\n",
            "Chunk 241: Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Chunk 242: Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "Chunk 243: The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "Chunk 244: These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu)\n",
            "Chunk 245: =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors.\n",
            "\n",
            "That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein).\n",
            "Chunk 246: An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side.\n",
            "Chunk 247: These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise.\n",
            "Chunk 248: Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities.\n",
            "Chunk 249: Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "Chunk 250: solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0).\n",
            "Chunk 251: It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v).\n",
            "Chunk 252: Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations.\n",
            "Chunk 253: It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "\n",
            "\n",
            "Remark 11 (Relation with iterative projections)\n",
            "Chunk 254: Remark 11 (Relation with iterative projections)\n",
            "\n",
            ".Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b.\n",
            "Chunk 255: One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)).\n",
            "Chunk 256: (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "Chunk 257: These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "Chunk 258: In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??\n",
            "Chunk 259: and??).\n",
            "Chunk 260: Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "Chunk 261: This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”).\n",
            "Chunk 262: This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?].\n",
            "Chunk 263: The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance.\n",
            "\n",
            "It was introduced independently by [ ?] and\n",
            "Chunk 264: [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates.\n",
            "Chunk 265: They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "\n",
            "\n",
            "21\n",
            "Theorem 2.\n",
            "Chunk 266: Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Chunk 267: Theorem 3.\n",
            "\n",
            "One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ).\n",
            "Chunk 268: (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)).\n",
            "\n",
            "Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆)\n",
            "Chunk 269: Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆)\n",
            "\n",
            "+dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "\n",
            "\n",
            "Proof.\n",
            "\n",
            "One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m)\n",
            "Chunk 270: =dH(1m/v,1m/v′).\n",
            "\n",
            "\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "\n",
            "\n",
            "where we used Theorem 2.\n",
            "\n",
            "This shows (1.53).\n",
            "Chunk 271: where we used Theorem 2.\n",
            "\n",
            "This shows (1.53).\n",
            "\n",
            "One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ))\n",
            "\n",
            "+dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "\n",
            "\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "Chunk 272: +λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "\n",
            "\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ))\n",
            "\n",
            "=P(ℓ)1m(the second one being similar).\n",
            "Chunk 273: The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Chunk 274: Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0.\n",
            "Chunk 275: These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?].\n",
            "\n",
            "Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??).\n",
            "Chunk 276: This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations\n",
            "Chunk 277: The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "\n",
            "\n",
            "Proposition 8.\n",
            "\n",
            "One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "Chunk 278: The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε).\n",
            "\n",
            "(1.57)\n",
            "Proof.\n",
            "Chunk 279: (1.57)\n",
            "Proof.\n",
            "\n",
            "We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε.\n",
            "Chunk 280: Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)).\n",
            "\n",
            "(1.58)\n",
            "Chunk 281: The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above.\n",
            "Chunk 282: The remaining times\n",
            "are those displayed in (1.56).\n",
            "Chunk 283: Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x)\n",
            "Chunk 284: +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "\n",
            "\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0.\n",
            "Chunk 285: Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz).\n",
            "Chunk 286: Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "\n",
            "\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A\n",
            "Chunk 287: simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56).\n",
            "Chunk 288: Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ".\n",
            "Chunk 289: (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ".\n",
            "Chunk 290: (1.62)\n",
            "\n",
            "\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57).\n",
            "Chunk 291: Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ))\n",
            "\n",
            "=ε(log(u(ℓ)),log(v(ℓ))).\n",
            "Chunk 292: =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "\n",
            "\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation.\n",
            "Chunk 293: Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "\n",
            "\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0.\n",
            "Chunk 294: Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function.\n",
            "Chunk 295: Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj.\n",
            "\n",
            "(1.64)\n",
            "Chunk 296: (1.64)\n",
            "\n",
            "\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤).\n",
            "Chunk 297: To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows.\n",
            "Chunk 298: Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "\n",
            "\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Chunk 299: i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "\n",
            "\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??\n",
            "\n",
            "(see in particu-\n",
            "lar (??)).\n",
            "Chunk 300: (see in particu-\n",
            "lar (??)).\n",
            "\n",
            "Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "\n",
            "\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT)\n",
            "\n",
            "+εlogb.\n",
            "Chunk 301: (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Chunk 302: Remark 17 (Log-domain Sinkhorn) .While\n",
            "Chunk 303: mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε.\n",
            "Chunk 304: Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε.\n",
            "Chunk 305: (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings.\n",
            "Chunk 306: This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)=\n",
            "\n",
            "Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "\n",
            "\n",
            "Ci,j−fi−gj)\n",
            "Chunk 307: Ci,j−fi−gj)\n",
            "\n",
            "\n",
            "i,j.\n",
            "Chunk 308: i,j.\n",
            "\n",
            "\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations.\n",
            "Chunk 309: The downside is that it\n",
            "requiresnmcomputations of exp at each step.\n",
            "Chunk 310: Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "Chunk 311: In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters.\n",
            "Chunk 312: Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed.\n",
            "Chunk 313: A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "Chunk 314: This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?].\n",
            "Chunk 315: They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "Chunk 316: The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Chunk 317: Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems.\n",
            "Chunk 318: One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs).\n",
            "Chunk 319: (1.71)\n",
            "\n",
            "\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique.\n",
            "Chunk 320: Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures.\n",
            "Chunk 321: Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??).\n",
            "Chunk 322: Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s.\n",
            "Chunk 323: The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?].\n",
            "Chunk 324: Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "Chunk 325: One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0.\n",
            "Chunk 326: This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?].\n",
            "Chunk 327: An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness).\n",
            "Chunk 328: A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε.\n",
            "Chunk 329: Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S.\n",
            "Chunk 330: As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection.\n",
            "\n",
            "This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??.\n",
            "Chunk 331: The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs.\n",
            "Chunk 332: (1.77)\n",
            "\n",
            "\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "\n",
            "\n",
            "Proposition 9.\n",
            "Chunk 333: The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ".\n",
            "Chunk 334: (1.78)\n",
            "Proof.\n",
            "\n",
            "Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Chunk 335: Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks).\n",
            "Chunk 336: This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72).\n",
            "Chunk 337: The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square.\n",
            "\n",
            "Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "Chunk 338: which shows the desired formula.\n",
            "Chunk 339: To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Chunk 340: Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75).\n",
            "Chunk 341: Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "\n",
            "\n",
            "Figures ??\n",
            "\n",
            "and??show applications to 2-D and 3-D shapes interpolation.\n",
            "Chunk 342: Figure ?\n",
            "\n",
            "?shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance.\n",
            "Chunk 343: For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??.\n",
            "Chunk 344: We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "\n",
            "\n",
            "Wasserstein Loss.\n",
            "Chunk 345: In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space.\n",
            "Chunk 346: Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance.\n",
            "Chunk 347: In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β).\n",
            "\n",
            "(1.80)\n",
            "Chunk 348: (1.80)\n",
            "\n",
            "\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms.\n",
            "Chunk 349: However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "Chunk 350: The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively.\n",
            "Chunk 351: Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]).\n",
            "Chunk 352: However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "\n",
            "\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "Chunk 353: A practical problem of paramount importance in statistic and machine learning is density ﬁtting.\n",
            "Chunk 354: Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis\n",
            "Chunk 355: some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "Chunk 356: In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "Chunk 357: This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "Chunk 358: This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "Chunk 359: However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown).\n",
            "Chunk 360: Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "Chunk 361: A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z\n",
            "Chunk 362: )\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1.\n",
            "Chunk 363: The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure).\n",
            "Chunk 364: Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d.\n",
            "\n",
            "samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "Chunk 365: In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x)\n",
            "Chunk 366: +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ".\n",
            "\n",
            "(1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "\n",
            "\n",
            "28\n",
            "Chunk 367: 28\n",
            "\n",
            "\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??.\n",
            "Chunk 368: Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "Chunk 369: The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "\n",
            "\n",
            "Gromov-Wasserstein.\n",
            "Chunk 370: Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost.\n",
            "Chunk 371: To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned.\n",
            "Chunk 372: A typical scenario is when these matrices are (power\n",
            "of) distance matrices.\n",
            "Chunk 373: The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "Chunk 374: This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs.\n",
            "Chunk 375: It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "Chunk 376: One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures.\n",
            "Chunk 377: This distance was introduced and studied in details by Memoli\n",
            "in [?].\n",
            "\n",
            "An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?].\n",
            "Chunk 378: See also [ ?] for applications in computer vision.\n",
            "Chunk 379: This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Chunk 380: Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces.\n",
            "Chunk 381: One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′).\n",
            "Chunk 382: (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′))\n",
            "\n",
            "=dX(x,x′).\n",
            "Chunk 383: =dX(x,x′).\n",
            "\n",
            "\n",
            "Remark 20.Gromov-Wasserstein geodesics\n",
            "\n",
            "The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure.\n",
            "Chunk 384: [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "Chunk 385: This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?].\n",
            "Chunk 386: It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1.\n",
            "Chunk 387: A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "Chunk 388: To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P).\n",
            "Chunk 389: (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86).\n",
            "\n",
            "Extracted from [ ?].\n",
            "Chunk 390: As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85).\n",
            "Chunk 391: Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ))\n",
            "Chunk 392: =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?].\n",
            "\n",
            "Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ).\n",
            "Chunk 393: Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "\n",
            "\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck.\n",
            "Chunk 394: 30\n",
            "Bibliography\n",
            "[1] Amir Beck.\n",
            "\n",
            "Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB.\n",
            "\n",
            "SIAM, 2014.\n",
            "Chunk 395: SIAM, 2014.\n",
            "\n",
            "\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.\n",
            "\n",
            "Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers.\n",
            "Chunk 396: Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "\n",
            "\n",
            "[3] Stephen Boyd and Lieven Vandenberghe.\n",
            "\n",
            "Convex optimization .\n",
            "\n",
            "Cambridge university press, 2004.\n",
            "\n",
            "\n",
            "[4] E. Cand` es and D. Donoho.\n",
            "Chunk 397: [4] E. Cand` es and D. Donoho.\n",
            "\n",
            "New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities.\n",
            "\n",
            "Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 57(2):219–266, 2004.\n",
            "Chunk 398: on Pure and Appl.\n",
            "\n",
            "Math. , 57(2):219–266, 2004.\n",
            "\n",
            "\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying.\n",
            "\n",
            "Fast discrete curvelet transforms.\n",
            "Chunk 399: Fast discrete curvelet transforms.\n",
            "\n",
            "SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "\n",
            "\n",
            "[6] A. Chambolle.\n",
            "\n",
            "An algorithm for total variation minimization and applications.\n",
            "\n",
            "J. Math.\n",
            "Chunk 400: J. Math.\n",
            "\n",
            "Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "\n",
            "\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock.\n",
            "\n",
            "An intro-\n",
            "duction to total variation for image analysis.\n",
            "Chunk 401: Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "\n",
            "\n",
            "[8] Antonin Chambolle and Thomas Pock.\n",
            "\n",
            "An introduction to continuous optimization for imaging.\n",
            "Chunk 402: Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "\n",
            "\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders.\n",
            "\n",
            "Atomic decomposition by basis pursuit.\n",
            "\n",
            "SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "Chunk 403: [10] Philippe G Ciarlet.\n",
            "\n",
            "Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation.\n",
            "\n",
            "1982.\n",
            "\n",
            "\n",
            "[11] P. L. Combettes and V. R. Wajs.\n",
            "Chunk 404: 1982.\n",
            "\n",
            "\n",
            "[11] P. L. Combettes and V. R. Wajs.\n",
            "\n",
            "Signal recovery by proximal forward-backward splitting.\n",
            "\n",
            "SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "Chunk 405: [12] I. Daubechies, M. Defrise, and C. De Mol.\n",
            "\n",
            "An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint.\n",
            "\n",
            "Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 57:1413–1541, 2004.\n",
            "Chunk 406: on Pure and Appl.\n",
            "\n",
            "Math. , 57:1413–1541, 2004.\n",
            "\n",
            "\n",
            "[13] D. Donoho and I. Johnstone.\n",
            "\n",
            "Ideal spatial adaptation via wavelet shrinkage.\n",
            "\n",
            "Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "Chunk 407: Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "\n",
            "\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer.\n",
            "\n",
            "Regularization of inverse problems , volume\n",
            "375.\n",
            "\n",
            "Springer Science & Business Media, 1996.\n",
            "Chunk 408: Springer Science & Business Media, 1996.\n",
            "\n",
            "\n",
            "[15] M. Figueiredo and R. Nowak.\n",
            "\n",
            "An EM Algorithm for Wavelet-Based Image Restoration.\n",
            "\n",
            "IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "Chunk 409: IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "\n",
            "\n",
            "[16] Simon Foucart and Holger Rauhut.\n",
            "\n",
            "A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "\n",
            "\n",
            "31\n",
            "Chunk 410: 31\n",
            "\n",
            "\n",
            "[17] Stephane Mallat.\n",
            "\n",
            "A wavelet tour of signal processing: the sparse way .\n",
            "\n",
            "Academic press, 2008.\n",
            "\n",
            "\n",
            "[18] D. Mumford and J. Shah.\n",
            "Chunk 411: [18] D. Mumford and J. Shah.\n",
            "\n",
            "Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems.\n",
            "\n",
            "Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 42:577–685, 1989.\n",
            "Chunk 412: on Pure and Appl.\n",
            "\n",
            "Math. , 42:577–685, 1989.\n",
            "\n",
            "\n",
            "[19] Neal Parikh, Stephen Boyd, et al.\n",
            "\n",
            "Proximal algorithms.\n",
            "\n",
            "Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "Chunk 413: [20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier .\n",
            "\n",
            "Ellipses, 2004.\n",
            "Chunk 414: Ellipses, 2004.\n",
            "\n",
            "\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain.\n",
            "\n",
            "IEEE Trans.\n",
            "Chunk 415: IEEE Trans.\n",
            "\n",
            "Image Proc. , 12(11):1338–1351, November 2003.\n",
            "\n",
            "\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi.\n",
            "\n",
            "Nonlinear total variation based noise removal algorithms.\n",
            "\n",
            "Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "Chunk 416: Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "\n",
            "\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "\n",
            "\n",
            "Variational methods in imaging .\n",
            "\n",
            "Springer, 2009.\n",
            "Chunk 417: Variational methods in imaging .\n",
            "\n",
            "Springer, 2009.\n",
            "\n",
            "\n",
            "[24] C. E. Shannon.\n",
            "\n",
            "A mathematical theory of communication.\n",
            "\n",
            "The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "Chunk 418: [25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili.\n",
            "\n",
            "Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis .\n",
            "\n",
            "Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23nXLdrOmfbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_overlap(text, chunk_size=500, chunk_overlap=40,splitter_type = \"SpacyTextSplitter\")\n",
        "print(\"Char count chunking _with_overlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10PLJKJR1cQa",
        "outputId": "bfa98501-e456-41e6-dc1a-e22cce907252"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_overlap:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures.\n",
            "\n",
            "We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "Chunk 2: A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x.\n",
            "\n",
            "Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "\n",
            "\n",
            "Remark 1 (General measures) .A\n",
            "Chunk 3: Remark 1 (General measures) .A\n",
            "\n",
            "convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework.\n",
            "\n",
            "Such objects only need to be modelled as measures.\n",
            "\n",
            "This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Chunk 4: Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x)\n",
            "\n",
            "=n∑\n",
            "i=1aif(xi).\n",
            "\n",
            "\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x)\n",
            "\n",
            "=ρα(x)dxw.r.t.\n",
            "\n",
            "the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "Chunk 5: An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "\n",
            "\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions).\n",
            "\n",
            "For instance, the derivative of a Dirac is not a measure.\n",
            "Chunk 6: We denote M+(X) the\n",
            "set of all positive measures on X.\n",
            "\n",
            "The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1.\n",
            "\n",
            "Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "\n",
            "\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1:\n",
            "Chunk 7: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x)\n",
            "\n",
            "=ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "\n",
            "\n",
            "Operators on measures.\n",
            "\n",
            "For some continuous map T:X\n",
            "\n",
            "→Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y).\n",
            "Chunk 8: For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "\n",
            "\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures.\n",
            "\n",
            "The formal deﬁnition reads as follow.\n",
            "\n",
            "\n",
            "Deﬁnition 1 (Push-forward)\n",
            "\n",
            ".ForT:X\n",
            "Chunk 9: Deﬁnition 1 (Push-forward)\n",
            "\n",
            ".ForT:X\n",
            "\n",
            "→ Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x).\n",
            "\n",
            "(1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}).\n",
            "\n",
            "(1.3)\n",
            "Note thatT♯preserves positivity and total mass,\n",
            "\n",
            "so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another.\n",
            "Chunk 10: The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α.\n",
            "Chunk 11: Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x)\n",
            "\n",
            "=|det(T′(x))|ρβ(T(x))\n",
            "Chunk 12: =|det(T′(x))|ρβ(T(x))\n",
            "\n",
            "(1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT).\n",
            "\n",
            "This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "\n",
            "\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2:\n",
            "Chunk 13: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions.\n",
            "\n",
            "It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "Chunk 14: It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4).\n",
            "\n",
            "This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method.\n",
            "\n",
            "Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "\n",
            "\n",
            "Remark 4 (Measures and random variables)\n",
            "Chunk 15: Remark 4 (Measures and random variables)\n",
            "\n",
            ".Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables.\n",
            "\n",
            "A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x).\n",
            "\n",
            "Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X\n",
            "Chunk 16: →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :\n",
            "\n",
            "ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable.\n",
            "\n",
            "Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "Chunk 17: 1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i).\n",
            "\n",
            "(1.5)\n",
            "\n",
            "\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n).\n",
            "\n",
            "However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70.\n",
            "Chunk 18: That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "\n",
            "\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant.\n",
            "\n",
            "Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal.\n",
            "\n",
            "(right) a Monge map can associate\n",
            "the blue measure αto the red measure β.\n",
            "Chunk 19: The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location.\n",
            "\n",
            "The mapping here is such that T(x1) =T(x2) =y2,T(x3)\n",
            "\n",
            "=y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "\n",
            "\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "\n",
            "\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3.\n",
            "Chunk 20: In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "\n",
            "\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β.\n",
            "Chunk 21: This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ".\n",
            "\n",
            "(1.8)\n",
            "\n",
            "\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "Chunk 22: In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "\n",
            "\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another.\n",
            "Chunk 23: This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure.\n",
            "\n",
            "For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "\n",
            "\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "Chunk 24: The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "\n",
            "\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem.\n",
            "\n",
            "Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size.\n",
            "Chunk 25: A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??).\n",
            "Chunk 26: Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex .\n",
            "\n",
            "Both are therefore diﬃcult to solve in their original formulation.\n",
            "\n",
            "\n",
            "Kantorovitch formulation for discrete measures.\n",
            "Chunk 27: The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only.\n",
            "\n",
            "Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations.\n",
            "Chunk 28: Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets.\n",
            "Chunk 29: This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6).\n",
            "\n",
            "Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "Chunk 30: The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "\n",
            "\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Chunk 31: Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "\n",
            "\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings.\n",
            "\n",
            "A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11).\n",
            "Chunk 32: Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "\n",
            "\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "\n",
            "\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n)\n",
            "Chunk 33: , we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Chunk 34: Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n).\n",
            "\n",
            "Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix.\n",
            "\n",
            "Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "Chunk 35: The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems.\n",
            "\n",
            "Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "\n",
            "\n",
            "Proposition 1 (Kantorovich for matching) .Ifm\n",
            "Chunk 36: =nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "\n",
            "\n",
            "Proof.\n",
            "\n",
            "Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices.\n",
            "\n",
            "A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "Chunk 37: 8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT.\n",
            "\n",
            "Chapter ?\n",
            "\n",
            "?is dedicated to the semi-discrete setup.\n",
            "\n",
            "\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density.\n",
            "\n",
            "The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black).\n",
            "Chunk 38: Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6).\n",
            "\n",
            "The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures.\n",
            "\n",
            "The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space.\n",
            "Chunk 39: The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj).\n",
            "\n",
            "In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ".\n",
            "Chunk 40: (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities).\n",
            "\n",
            "Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y).\n",
            "\n",
            "(1.14)\n",
            "Chunk 41: (1.14)\n",
            "\n",
            "\n",
            "This is an inﬁnite-dimensional linear program over a space of measures.\n",
            "\n",
            "Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14).\n",
            "\n",
            "Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "Chunk 42: On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\n",
            "\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below.\n",
            "\n",
            "Inspired by [ ?].\n",
            "is weak-* continuous.\n",
            "\n",
            "And the set of constraint is non empty, taking α⊗β.\n",
            "Chunk 43: On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "\n",
            "\n",
            "Wasserstein distances.\n",
            "\n",
            "An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties.\n",
            "\n",
            "Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "Chunk 44: We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare.\n",
            "\n",
            "The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "\n",
            "\n",
            "Proposition 2.\n",
            "Chunk 45: Proposition 2.\n",
            "\n",
            "We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0\n",
            "\n",
            "if and only if a=b,\n",
            "\n",
            "and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof.\n",
            "Chunk 46: Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "Chunk 47: To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure.\n",
            "\n",
            "In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple.\n",
            "\n",
            "Let a,b,c∈Σn.\n",
            "\n",
            "Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively.\n",
            "\n",
            "We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value).\n",
            "Chunk 48: We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "\n",
            "\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b).\n",
            "\n",
            "Similarly one veriﬁes that S⊤1n=c.\n",
            "\n",
            "\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "Chunk 49: min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "\n",
            "\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Chunk 50: Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "\n",
            "\n",
            "Proposition 3.\n",
            "\n",
            "We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y)\n",
            "\n",
            "+d(y,z).\n",
            "Chunk 51: +d(y,z).\n",
            "\n",
            "\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof.\n",
            "\n",
            "The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "Chunk 52: The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions.\n",
            "Chunk 53: In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work).\n",
            "\n",
            "In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y).\n",
            "Chunk 54: Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y).\n",
            "\n",
            "This shows that Wp(δx,δy)→0 ifx→y.\n",
            "\n",
            "This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "\n",
            "\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα.\n",
            "Chunk 55: This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "\n",
            "\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0\n",
            "\n",
            "[?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "\n",
            "\n",
            "Note that there exists alternative distances which also metrize weak convergence.\n",
            "\n",
            "The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k)\n",
            "\n",
            "=∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:\n",
            "\n",
            "X2→R.\n",
            "Chunk 56: X2→R.\n",
            "\n",
            "The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "\n",
            "\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "\n",
            "\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex.\n",
            "Chunk 57: In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set.\n",
            "\n",
            "In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity.\n",
            "Chunk 58: Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "\n",
            "\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem.\n",
            "Chunk 59: The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "\n",
            "\n",
            "Proposition 4.\n",
            "\n",
            "One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof.\n",
            "\n",
            "This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4].\n",
            "Chunk 60: The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??.\n",
            "\n",
            "For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality.\n",
            "\n",
            "The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "Chunk 61: For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ".\n",
            "Chunk 62: (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions).\n",
            "\n",
            "The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "\n",
            "\n",
            "Proposition 5.\n",
            "\n",
            "One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x)\n",
            "Chunk 63: +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x)\n",
            "\n",
            "+g(y)⩽c(x,y)}.\n",
            "\n",
            "(1.22)\n",
            "\n",
            "\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "\n",
            "\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)).\n",
            "\n",
            "The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x)\n",
            "Chunk 64: +g(y) =c(x,y)}.\n",
            "\n",
            "(1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive.\n",
            "\n",
            "Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Chunk 65: Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "\n",
            "\n",
            "Theorem 1 (Brenier) .In\n",
            "Chunk 66: Theorem 1 (Brenier) .In\n",
            "\n",
            "the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd.\n",
            "\n",
            "This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x).\n",
            "\n",
            "(1.24)\n",
            "\n",
            "\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x)\n",
            "Chunk 67: =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν.\n",
            "\n",
            "This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "\n",
            "\n",
            "Proof.\n",
            "\n",
            "We sketch the main ingredients of the proof, more details can be found for instance in [ ?].\n",
            "\n",
            "We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x)\n",
            "\n",
            "+∫\n",
            "||y||2dβ(y).\n",
            "Chunk 68: +∫\n",
            "||y||2dβ(y).\n",
            "\n",
            "Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ".\n",
            "\n",
            "(1.25)\n",
            "13\n",
            "\n",
            "\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g).\n",
            "\n",
            "One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x).\n",
            "\n",
            "(1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)).\n",
            "Chunk 69: Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??\n",
            "\n",
            "for a generalization of this idea to generic costs c(x,y).\n",
            "\n",
            "By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "\n",
            "\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x)\n",
            "Chunk 70: +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x).\n",
            "\n",
            "Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere.\n",
            "\n",
            "This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "Chunk 71: This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight).\n",
            "\n",
            "This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Chunk 72: Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one.\n",
            "\n",
            "Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Chunk 73: Note also that this theorem can be extended in many directions.\n",
            "\n",
            "The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces).\n",
            "\n",
            "One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "Chunk 74: For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x))\n",
            "\n",
            "=ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ.\n",
            "\n",
            "The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian.\n",
            "\n",
            "In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x))\n",
            "Chunk 75: = 1 +ε∆ϕ(x)\n",
            "\n",
            "+o(ε).\n",
            "\n",
            "\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "\n",
            "\n",
            "Special cases In general, computing OT distances is numerically involved.\n",
            "\n",
            "We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "\n",
            "\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm)\n",
            "Chunk 76: Remark 6 (Binary Cost Matrix and 1-Norm)\n",
            "\n",
            ".One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1.\n",
            "\n",
            "One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y.\n",
            "\n",
            "The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "Chunk 77: 14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "\n",
            "\n",
            "Top: empirical measures with same number of points (optimal matching).\n",
            "\n",
            "Bottom: generic case.\n",
            "\n",
            "This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX\n",
            "Chunk 78: =R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ.\n",
            "\n",
            "That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change.\n",
            "Chunk 79: That formula is a simple consequence of the more general\n",
            "remark given below.\n",
            "\n",
            "Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points.\n",
            "\n",
            "The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures.\n",
            "\n",
            "It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?].\n",
            "Chunk 80: Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "\n",
            "\n",
            "Remark 8 (1-D case – Generic case) .For\n",
            "\n",
            "a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "Chunk 81: That function is also called the generalized quantile function of α.\n",
            "\n",
            "For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr.\n",
            "\n",
            "(1.31)\n",
            "\n",
            "\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric.\n",
            "Chunk 82: This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??\n",
            "\n",
            "and more generally\n",
            "in§??.\n",
            "\n",
            "Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx.\n",
            "Chunk 83: (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions).\n",
            "Chunk 84: An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions.\n",
            "\n",
            "It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??.\n",
            "\n",
            "For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "\n",
            "\n",
            "Remark 9 (Distance between Gaussians) .Ifα\n",
            "Chunk 85: =N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ.\n",
            "\n",
            "Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x))\n",
            "Chunk 86: = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β.\n",
            "\n",
            "Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "\n",
            "\n",
            "thatTis optimal.\n",
            "Chunk 87: thatTis optimal.\n",
            "\n",
            "Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "\n",
            "\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "\n",
            "\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ".\n",
            "\n",
            "The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "Chunk 88: \u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians.\n",
            "\n",
            "Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "\n",
            "\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "Chunk 89: whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root.\n",
            "\n",
            "One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments.\n",
            "\n",
            "In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "Chunk 90: For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "\n",
            "\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "\n",
            "\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations.\n",
            "\n",
            "It operates by adding an entropic regularization penalty to\n",
            "the original problem.\n",
            "Chunk 91: This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "Chunk 92: 17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "\n",
            "\n",
            "Entropic Regularization.\n",
            "\n",
            "The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative.\n",
            "\n",
            "The function His 1-strongly concave, because its hessian is ∂2H(P)\n",
            "\n",
            "=−diag(1/Pi,j) and\n",
            "Pi,j⩽1.\n",
            "Chunk 93: =−diag(1/Pi,j) and\n",
            "Pi,j⩽1.\n",
            "\n",
            "The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P).\n",
            "\n",
            "(1.39)\n",
            "\n",
            "\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution.\n",
            "Chunk 94: The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem.\n",
            "\n",
            "Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11.\n",
            "Chunk 95: To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "\n",
            "\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D).\n",
            "\n",
            "Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle.\n",
            "\n",
            "The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle.\n",
            "Chunk 96: This is further detailed in the proposition below.\n",
            "\n",
            "The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "\n",
            "\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "\n",
            "\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "Chunk 97: One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof.\n",
            "\n",
            "We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0.\n",
            "\n",
            "We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b).\n",
            "\n",
            "We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "\n",
            "\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)).\n",
            "Chunk 98: (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities.\n",
            "\n",
            "Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "\n",
            "\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40).\n",
            "Chunk 99: Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40).\n",
            "\n",
            "Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "\n",
            "\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling.\n",
            "Chunk 100: In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions.\n",
            "\n",
            "A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp.\n",
            "\n",
            "1/ε)\n",
            "\n",
            "nearε= 0\n",
            "\n",
            "(respε= +∞).\n",
            "\n",
            "Figure 1.13\n",
            "shows visually the eﬀect of these two convergence.\n",
            "Chunk 101: A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Chunk 102: Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K).\n",
            "Chunk 103: (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "Chunk 104: dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ.\n",
            "\n",
            "It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "\n",
            "\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis\n",
            "\n",
            "the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y).\n",
            "Chunk 105: This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "\n",
            "\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "\n",
            "\n",
            "Sinkhorn’s\n",
            "Chunk 106: Sinkhorn’s\n",
            "\n",
            "Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables.\n",
            "\n",
            "That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "\n",
            "\n",
            "Proposition 7.\n",
            "\n",
            "The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof.\n",
            "Chunk 107: Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "\n",
            "\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "Chunk 108: The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "\n",
            "\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu)\n",
            "Chunk 109: =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors.\n",
            "\n",
            "That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein).\n",
            "\n",
            "An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side.\n",
            "Chunk 110: These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise.\n",
            "\n",
            "Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities.\n",
            "Chunk 111: Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "\n",
            "\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0).\n",
            "\n",
            "It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v).\n",
            "Chunk 112: Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations.\n",
            "\n",
            "It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "\n",
            "\n",
            "Remark 11 (Relation with iterative projections)\n",
            "\n",
            ".Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b.\n",
            "Chunk 113: One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)).\n",
            "\n",
            "(1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "\n",
            "\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "Chunk 114: In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??\n",
            "\n",
            "and??).\n",
            "\n",
            "\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "Chunk 115: This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”).\n",
            "\n",
            "This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?].\n",
            "\n",
            "The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance.\n",
            "\n",
            "It was introduced independently by [ ?] and\n",
            "Chunk 116: [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates.\n",
            "\n",
            "They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "\n",
            "\n",
            "21\n",
            "Theorem 2.\n",
            "Chunk 117: 21\n",
            "Theorem 2.\n",
            "\n",
            "Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "\n",
            "\n",
            "Theorem 3.\n",
            "\n",
            "One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ).\n",
            "Chunk 118: (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)).\n",
            "\n",
            "Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆)\n",
            "\n",
            "+dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "\n",
            "\n",
            "Proof.\n",
            "\n",
            "One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m)\n",
            "\n",
            "=dH(1m/v,1m/v′).\n",
            "\n",
            "\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "\n",
            "\n",
            "where we used Theorem 2.\n",
            "\n",
            "This shows (1.53).\n",
            "Chunk 119: This shows (1.53).\n",
            "\n",
            "One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ))\n",
            "\n",
            "+dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "\n",
            "\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "\n",
            "\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ))\n",
            "\n",
            "=P(ℓ)1m(the second one being similar).\n",
            "Chunk 120: =P(ℓ)1m(the second one being similar).\n",
            "\n",
            "The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "\n",
            "\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0.\n",
            "\n",
            "These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?].\n",
            "Chunk 121: Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??).\n",
            "\n",
            "This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations\n",
            "\n",
            "The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "\n",
            "\n",
            "Proposition 8.\n",
            "\n",
            "One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "Chunk 122: The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε).\n",
            "\n",
            "(1.57)\n",
            "Proof.\n",
            "\n",
            "We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε.\n",
            "\n",
            "Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)).\n",
            "\n",
            "(1.58)\n",
            "Chunk 123: (1.58)\n",
            "\n",
            "\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above.\n",
            "\n",
            "The remaining times\n",
            "are those displayed in (1.56).\n",
            "\n",
            "\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x)\n",
            "Chunk 124: +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "\n",
            "\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0.\n",
            "\n",
            "Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz).\n",
            "Chunk 125: Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "\n",
            "\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A\n",
            "\n",
            "simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56).\n",
            "Chunk 126: Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ".\n",
            "\n",
            "(1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ".\n",
            "\n",
            "(1.62)\n",
            "Chunk 127: (1.62)\n",
            "\n",
            "\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57).\n",
            "\n",
            "Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ))\n",
            "\n",
            "=ε(log(u(ℓ)),log(v(ℓ))).\n",
            "\n",
            "\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation.\n",
            "\n",
            "Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Chunk 128: Note that min ε(z) converges to min zfor any vector zasε→0.\n",
            "\n",
            "Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function.\n",
            "\n",
            "Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj.\n",
            "\n",
            "(1.64)\n",
            "\n",
            "\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤).\n",
            "Chunk 129: To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows.\n",
            "\n",
            "Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "\n",
            "\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "\n",
            "\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??\n",
            "\n",
            "(see in particu-\n",
            "lar (??)).\n",
            "\n",
            "Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "Chunk 130: g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT)\n",
            "\n",
            "+εlogb.\n",
            "\n",
            "(1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "\n",
            "\n",
            "Remark 17 (Log-domain Sinkhorn) .While\n",
            "\n",
            "mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε.\n",
            "Chunk 131: Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε.\n",
            "\n",
            "(1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings.\n",
            "\n",
            "This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)=\n",
            "\n",
            "Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "\n",
            "\n",
            "Ci,j−fi−gj)\n",
            "\n",
            "\n",
            "i,j.\n",
            "Chunk 132: Ci,j−fi−gj)\n",
            "\n",
            "\n",
            "i,j.\n",
            "\n",
            "\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations.\n",
            "\n",
            "The downside is that it\n",
            "requiresnmcomputations of exp at each step.\n",
            "Chunk 133: Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "Chunk 134: In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters.\n",
            "\n",
            "Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed.\n",
            "Chunk 135: A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "\n",
            "\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?].\n",
            "Chunk 136: They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "Chunk 137: The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "\n",
            "\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems.\n",
            "Chunk 138: One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs).\n",
            "\n",
            "(1.71)\n",
            "\n",
            "\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique.\n",
            "Chunk 139: Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures.\n",
            "\n",
            "Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??).\n",
            "\n",
            "Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s.\n",
            "Chunk 140: The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?].\n",
            "\n",
            "Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "\n",
            "\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0.\n",
            "Chunk 141: This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?].\n",
            "\n",
            "An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness).\n",
            "\n",
            "A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε.\n",
            "Chunk 142: Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S.\n",
            "\n",
            "As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection.\n",
            "\n",
            "This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??.\n",
            "Chunk 143: The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs.\n",
            "\n",
            "(1.77)\n",
            "\n",
            "\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "\n",
            "\n",
            "Proposition 9.\n",
            "Chunk 144: Proposition 9.\n",
            "\n",
            "The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ".\n",
            "\n",
            "(1.78)\n",
            "Proof.\n",
            "\n",
            "Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Chunk 145: Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks).\n",
            "Chunk 146: This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72).\n",
            "\n",
            "The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square.\n",
            "\n",
            "Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "\n",
            "\n",
            "which shows the desired formula.\n",
            "Chunk 147: which shows the desired formula.\n",
            "\n",
            "To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "\n",
            "\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75).\n",
            "\n",
            "Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "\n",
            "\n",
            "Figures ??\n",
            "Chunk 148: Figures ??\n",
            "\n",
            "and??show applications to 2-D and 3-D shapes interpolation.\n",
            "\n",
            "Figure ?\n",
            "\n",
            "?shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance.\n",
            "\n",
            "For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??.\n",
            "\n",
            "We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "\n",
            "\n",
            "Wasserstein Loss.\n",
            "Chunk 149: Wasserstein Loss.\n",
            "\n",
            "In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space.\n",
            "\n",
            "Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance.\n",
            "\n",
            "In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β).\n",
            "Chunk 150: (1.80)\n",
            "\n",
            "\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms.\n",
            "\n",
            "However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "Chunk 151: The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively.\n",
            "Chunk 152: Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]).\n",
            "\n",
            "However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "Chunk 153: 27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "\n",
            "\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting.\n",
            "\n",
            "Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis\n",
            "\n",
            "some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "Chunk 154: In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "\n",
            "\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "\n",
            "\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "Chunk 155: However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown).\n",
            "\n",
            "Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "Chunk 156: A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z\n",
            "\n",
            ")\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1.\n",
            "Chunk 157: The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure).\n",
            "\n",
            "Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d.\n",
            "\n",
            "samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "Chunk 158: In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x)\n",
            "\n",
            "+∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ".\n",
            "\n",
            "(1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "\n",
            "\n",
            "28\n",
            "Chunk 159: 28\n",
            "\n",
            "\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??.\n",
            "\n",
            "Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "\n",
            "\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "\n",
            "\n",
            "Gromov-Wasserstein.\n",
            "Chunk 160: Gromov-Wasserstein.\n",
            "\n",
            "Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost.\n",
            "\n",
            "To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned.\n",
            "Chunk 161: A typical scenario is when these matrices are (power\n",
            "of) distance matrices.\n",
            "\n",
            "The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "\n",
            "\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs.\n",
            "\n",
            "It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "Chunk 162: One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures.\n",
            "\n",
            "This distance was introduced and studied in details by Memoli\n",
            "in [?].\n",
            "\n",
            "An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?].\n",
            "\n",
            "See also [ ?] for applications in computer vision.\n",
            "Chunk 163: This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "\n",
            "\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces.\n",
            "\n",
            "One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′).\n",
            "Chunk 164: (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′))\n",
            "\n",
            "=dX(x,x′).\n",
            "\n",
            "\n",
            "Remark 20.Gromov-Wasserstein geodesics\n",
            "\n",
            "The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure.\n",
            "Chunk 165: [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "\n",
            "\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?].\n",
            "\n",
            "It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1.\n",
            "Chunk 166: A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "\n",
            "\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P).\n",
            "Chunk 167: (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86).\n",
            "\n",
            "Extracted from [ ?].\n",
            "\n",
            "\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85).\n",
            "Chunk 168: Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ))\n",
            "\n",
            "=−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?].\n",
            "\n",
            "Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ).\n",
            "\n",
            "Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "\n",
            "\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck.\n",
            "Chunk 169: 30\n",
            "Bibliography\n",
            "[1] Amir Beck.\n",
            "\n",
            "Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB.\n",
            "\n",
            "SIAM, 2014.\n",
            "\n",
            "\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.\n",
            "\n",
            "Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers.\n",
            "\n",
            "Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "\n",
            "\n",
            "[3] Stephen Boyd and Lieven Vandenberghe.\n",
            "\n",
            "Convex optimization .\n",
            "\n",
            "Cambridge university press, 2004.\n",
            "Chunk 170: Cambridge university press, 2004.\n",
            "\n",
            "\n",
            "[4] E. Cand` es and D. Donoho.\n",
            "\n",
            "New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities.\n",
            "\n",
            "Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 57(2):219–266, 2004.\n",
            "\n",
            "\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying.\n",
            "\n",
            "Fast discrete curvelet transforms.\n",
            "\n",
            "SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "\n",
            "\n",
            "[6] A. Chambolle.\n",
            "\n",
            "An algorithm for total variation minimization and applications.\n",
            "\n",
            "J. Math.\n",
            "Chunk 171: J. Math.\n",
            "\n",
            "Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "\n",
            "\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock.\n",
            "\n",
            "An intro-\n",
            "duction to total variation for image analysis.\n",
            "\n",
            "Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "\n",
            "\n",
            "[8] Antonin Chambolle and Thomas Pock.\n",
            "\n",
            "An introduction to continuous optimization for imaging.\n",
            "\n",
            "Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "\n",
            "\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders.\n",
            "Chunk 172: Atomic decomposition by basis pursuit.\n",
            "\n",
            "SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "\n",
            "\n",
            "[10] Philippe G Ciarlet.\n",
            "\n",
            "Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation.\n",
            "\n",
            "1982.\n",
            "\n",
            "\n",
            "[11] P. L. Combettes and V. R. Wajs.\n",
            "\n",
            "Signal recovery by proximal forward-backward splitting.\n",
            "\n",
            "SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "\n",
            "\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol.\n",
            "\n",
            "An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint.\n",
            "Chunk 173: Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 57:1413–1541, 2004.\n",
            "\n",
            "\n",
            "[13] D. Donoho and I. Johnstone.\n",
            "\n",
            "Ideal spatial adaptation via wavelet shrinkage.\n",
            "\n",
            "Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "\n",
            "\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer.\n",
            "\n",
            "Regularization of inverse problems , volume\n",
            "375.\n",
            "\n",
            "Springer Science & Business Media, 1996.\n",
            "\n",
            "\n",
            "[15] M. Figueiredo and R. Nowak.\n",
            "\n",
            "An EM Algorithm for Wavelet-Based Image Restoration.\n",
            "\n",
            "IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "Chunk 174: [16] Simon Foucart and Holger Rauhut.\n",
            "\n",
            "A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "\n",
            "\n",
            "31\n",
            "\n",
            "\n",
            "[17] Stephane Mallat.\n",
            "\n",
            "A wavelet tour of signal processing: the sparse way .\n",
            "\n",
            "Academic press, 2008.\n",
            "\n",
            "\n",
            "[18] D. Mumford and J. Shah.\n",
            "\n",
            "Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems.\n",
            "\n",
            "Commun.\n",
            "\n",
            "on Pure and Appl.\n",
            "\n",
            "Math. , 42:577–685, 1989.\n",
            "\n",
            "\n",
            "[19] Neal Parikh, Stephen Boyd, et al.\n",
            "\n",
            "Proximal algorithms.\n",
            "Chunk 175: Proximal algorithms.\n",
            "\n",
            "Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "\n",
            "\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier .\n",
            "\n",
            "Ellipses, 2004.\n",
            "\n",
            "\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain.\n",
            "\n",
            "IEEE Trans.\n",
            "\n",
            "Image Proc. , 12(11):1338–1351, November 2003.\n",
            "\n",
            "\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi.\n",
            "\n",
            "Nonlinear total variation based noise removal algorithms.\n",
            "Chunk 176: Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "\n",
            "\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "\n",
            "\n",
            "Variational methods in imaging .\n",
            "\n",
            "Springer, 2009.\n",
            "\n",
            "\n",
            "[24] C. E. Shannon.\n",
            "\n",
            "A mathematical theory of communication.\n",
            "\n",
            "The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "\n",
            "\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili.\n",
            "\n",
            "Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis .\n",
            "\n",
            "Cambridge university press, 2015.\n",
            "32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_nonoveralp(text, chunk_size=500,Recursive = True)\n",
        "print(\"Char count chunking _with_nonoverlap:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpHaI24h1sI0",
        "outputId": "cdf6b5e1-ada6-4f26-c292-c570e8969cce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_nonoverlap:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "Chunk 2: belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Chunk 3: x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "Chunk 4: to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "Chunk 5: Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Chunk 6: An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "Chunk 7: Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Chunk 8: +(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "Chunk 9: i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "Chunk 10: and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "Chunk 11: positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "Chunk 12: Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "Chunk 13: Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "Chunk 14: onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "Chunk 15: between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "Chunk 16: densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Chunk 17: |det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "Chunk 18: map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "Chunk 19: registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Chunk 20: (often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Chunk 21: variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "Chunk 22: 1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "Chunk 23: that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "Chunk 24: 5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Chunk 25: disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "Chunk 26: corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "Chunk 27: push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "Chunk 28: min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Chunk 29: constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "Chunk 30: target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "Chunk 31: (X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "Chunk 32: Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "Chunk 33: also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Chunk 34: constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "Chunk 35: to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "Chunk 36: commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "Chunk 37: of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Chunk 38: P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "Chunk 39: asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "Chunk 40: not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Chunk 41: Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "Chunk 42: 0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "Chunk 43: Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "Chunk 44: min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Chunk 45: problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "Chunk 46: Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Chunk 47: minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "Chunk 48: scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Chunk 49: couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "Chunk 50: +(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Chunk 51: U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "Chunk 52: α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "Chunk 53: and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "Chunk 54: 9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "Chunk 55: to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "Chunk 56: measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "Chunk 57: supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Chunk 58: Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "Chunk 59: elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "Chunk 60: gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "Chunk 61: and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "Chunk 62: The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "Chunk 63: jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "Chunk 64: Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Chunk 65: Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "Chunk 66: between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "Chunk 67: spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "Chunk 68: be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "Chunk 69: corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Chunk 70: This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "Chunk 71: ||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "Chunk 72: Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "Chunk 73: exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "Chunk 74: OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "Chunk 75: relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "Chunk 76: grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "Chunk 77: min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "Chunk 78: −∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "Chunk 79: are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Chunk 80: Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "Chunk 81: (fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "Chunk 82: trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "Chunk 83: Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Chunk 84: Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Chunk 85: ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "Chunk 86: ⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Chunk 87: . (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Chunk 88: minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "Chunk 89: Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "Chunk 90: diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "Chunk 91: of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "Chunk 92: be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "Chunk 93: Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "Chunk 94: strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Chunk 95: non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Chunk 96: Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "Chunk 97: the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Chunk 98: between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "Chunk 99: yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "Chunk 100: αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "Chunk 101: with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Chunk 102: of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "Chunk 103: ∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "Chunk 104: α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "Chunk 105: in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "Chunk 106: T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "Chunk 107: mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "Chunk 108: Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "Chunk 109: since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "Chunk 110: 2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "Chunk 111: 16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "Chunk 112: pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "Chunk 113: cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "Chunk 114: B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "Chunk 115: Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "Chunk 116: the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "Chunk 117: execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "Chunk 118: H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Chunk 119: to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "Chunk 120: transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Chunk 121: that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "Chunk 122: from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "Chunk 123: problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Chunk 124: Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "Chunk 125: ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Chunk 126: 0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "Chunk 127: betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Chunk 128: H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "Chunk 129: transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "Chunk 130: performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Chunk 131: turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "Chunk 132: Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "Chunk 133: regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "Chunk 134: dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "Chunk 135: Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "Chunk 136: Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "Chunk 137: which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Chunk 138: +×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "Chunk 139: which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "Chunk 140: correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "Chunk 141: community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "Chunk 142: u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Chunk 143: 20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "Chunk 144: in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "Chunk 145: the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "Chunk 146: Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "Chunk 147: Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "Chunk 148: one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "Chunk 149: Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "Chunk 150: +,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "Chunk 151: +,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "Chunk 152: cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "Chunk 153: show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "Chunk 154: ∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "Chunk 155: dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Chunk 156: The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Chunk 157: degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "Chunk 158: of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "Chunk 159: (u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "Chunk 160: LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "Chunk 161: ⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "Chunk 162: problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "Chunk 163: potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "Chunk 164: unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "Chunk 165: ∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "Chunk 166: . (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Chunk 167: using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "Chunk 168: rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Chunk 169: now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Chunk 170: lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "Chunk 171: regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "Chunk 172: minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "Chunk 173: ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "Chunk 174: requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "Chunk 175: In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "Chunk 176: Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "Chunk 177: solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "Chunk 178: has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "Chunk 179: min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "Chunk 180: Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "Chunk 181: barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "Chunk 182: ∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "Chunk 183: re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "Chunk 184: 25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "Chunk 185: where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "Chunk 186: generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Chunk 187: s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Chunk 188: (fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "Chunk 189: max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "Chunk 190: KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "Chunk 191: which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Chunk 192: form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "Chunk 193: the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Chunk 194: distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Chunk 195: term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "Chunk 196: in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Chunk 197: as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "Chunk 198: corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "Chunk 199: A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "Chunk 200: min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "Chunk 201: min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "Chunk 202: However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "Chunk 203: in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "Chunk 204: αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "Chunk 205: is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "Chunk 206: (f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "Chunk 207: solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "Chunk 208: was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "Chunk 209: namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "Chunk 210: GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "Chunk 211: for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "Chunk 212: in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "Chunk 213: Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "Chunk 214: GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "Chunk 215: thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "Chunk 216: ((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "Chunk 217: spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Chunk 218: To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Chunk 219: iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "Chunk 220: of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "Chunk 221: iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "Chunk 222: and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "Chunk 223: [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "Chunk 224: Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "Chunk 225: duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "Chunk 226: Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "Chunk 227: [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "Chunk 228: with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "Chunk 229: 375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "Chunk 230: Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "Chunk 231: tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "Chunk 232: [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "Chunk 233: D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPuKFgO0AS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_char_count = text_chunker.char_count_chunking_with_custom_delimiter( text, chunk_size=200, chunk_overlap=50, delimiter=\"\\n\\n\",Recursive = False)\n",
        "print(\"Char count chunking _with_ _with_custom_delimiter:\")\n",
        "for i, chunk in enumerate(chunks_char_count):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37xIl_nATRi",
        "outputId": "3db157e0-4d10-476f-d2be-c412af545388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char count chunking _with_ _with_custom_delimiter:\n",
            "Chunk 1: Mathematical Foundations of Data Sciences\n",
            "Gabriel Peyr´ e\n",
            "CNRS & DMA\n",
            "´Ecole Normale Sup´ erieure\n",
            "gabriel.peyre@ens.fr\n",
            "https://mathematical-tours.github.io\n",
            "www.numerical-tours.com\n",
            "August 14, 2019\n",
            "2\n",
            "Chapter 1\n",
            "Optimal Transport\n",
            "1.1 Radon Measures\n",
            "Measures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\n",
            "belongs to the probability simplex\n",
            "Σndef.={\n",
            "a∈Rn\n",
            "+;n∑\n",
            "i=1ai= 1}\n",
            ".\n",
            "A discrete measure with weights aand locations x1,...,xn∈X reads\n",
            "α=n∑\n",
            "i=1aiδxi (1.1)\n",
            "whereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\n",
            "x. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\n",
            "measure if each of the “weights” described in vector ais positive itself.\n",
            "Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\n",
            "“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\n",
            "to the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\n",
            "equipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\n",
            "it against continuous functions, denoted f∈C(X).\n",
            "Integration of f∈C(X) against a discrete measure αcomputes a sum\n",
            "∫\n",
            "Xf(x)dα(x) =n∑\n",
            "i=1aif(xi).\n",
            "More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\n",
            "dα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\n",
            "dx, which means that\n",
            "∀h∈C(Rd),∫\n",
            "Rdh(x)dα(x) =∫\n",
            "Rdh(x)ρα(x)dx.\n",
            "An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\n",
            "the fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\n",
            "Xf(x)dα(x)∈R.\n",
            "IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity.\n",
            "Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\n",
            "dual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\n",
            "set of all positive measures on X. The set of probability measures is denoted M1\n",
            "+(X), which means that\n",
            "anyα∈M1\n",
            "+(X) is positive, and that α(X) =∫\n",
            "Xdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\n",
            "classes of measures, beyond histograms, considered in this work.\n",
            "3\n",
            "Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\n",
            "Figure 1.1: Schematic display of discrete distributions α=∑n\n",
            "i=1aiδxi(red corresponds to empirical uniform\n",
            "distribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\n",
            "1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\n",
            "and in 2-D using point clouds (radius equal to ai).\n",
            "Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\n",
            "T♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\n",
            "positions of all the points in the support of the measure\n",
            "T♯αdef.=∑\n",
            "iaiδT(xi).\n",
            "For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\n",
            "mental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow.\n",
            "Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\n",
            "α∈M (X)reads\n",
            "∀h∈C(Y),∫\n",
            "Yh(y)dβ(y) =∫\n",
            "Xh(T(x))dα(x). (1.2)\n",
            "Equivalently, for any measurable set B⊂Y, one has\n",
            "β(B) =α({x∈X;T(x)∈B}). (1.3)\n",
            "Note thatT♯preserves positivity and total mass, so that if α∈M1\n",
            "+(X)thenT♯α∈M1\n",
            "+(Y).\n",
            "Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\n",
            "measurable space to another. The more general extension T♯can now “move” an entire probability measure\n",
            "onXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\n",
            "a measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\n",
            "new measure onY) writtenT♯α. Note that such a push-forward T♯:M1\n",
            "+(X)→M1\n",
            "+(Y) is a linear operator\n",
            "between measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2.\n",
            "Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\n",
            "with densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\n",
            "densities linearly as a change of variables in the integration formula, indeed\n",
            "ρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\n",
            "whereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\n",
            "ofT). This implies, denoting y=T(x)\n",
            "|det(T′(x))|=ρα(x)\n",
            "ρβ(y).\n",
            "4\n",
            "=Pi\u0000xiT↵T]↵def.=Pi\u0000T(xi)\n",
            "TT]gdef.=g\u0000TgPush-forward of measures Pull-back of functions\n",
            "Figure 1.2: Comparison of push-forward T♯and pull-back T♯.\n",
            "Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\n",
            "the pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\n",
            "map deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\n",
            "others, in the sense that\n",
            "∀(α,g)∈M (X)×C(Y),∫\n",
            "Ygd(T♯α) =∫\n",
            "X(T♯g)dα.\n",
            "It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\n",
            "the presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\n",
            "registration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\n",
            "between these push-forward and pull-back operators.\n",
            "Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\n",
            "butions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\n",
            "(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\n",
            "+(X) such\n",
            "thatP(X∈A) =α(A) =∫\n",
            "Adα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\n",
            "another push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\n",
            "variableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\n",
            "yfromYis thus simply achieved by computing y=T(x) wherexis drawn from X.\n",
            "Convergence of random variable. Convergence of random variable (in probability, almost sure, in law),\n",
            "convergence of measures (strong, weak).\n",
            "1.2 Monge Problem\n",
            "Given a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\n",
            "bijectionσin the set Perm( n) of permutations of nelements solving\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i). (1.5)\n",
            "One could naively evaluate the cost function above using all permutations in the set Perm( n). However,\n",
            "that set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\n",
            "10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\n",
            "algorithms to optimize that cost function over the set of permutations, which will be the subject of §??.\n",
            "5\n",
            "x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\n",
            "either matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\n",
            "the blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\n",
            "disk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\n",
            "4⩽i⩽7 we haveT(xi) =y1.\n",
            "Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions.\n",
            "Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\n",
            "corners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3. In that case\n",
            "only two assignments exist, and they share the same cost.\n",
            "For discrete measures\n",
            "α=n∑\n",
            "i=1aiδxiandβ=m∑\n",
            "j=1bjδyj (1.6)\n",
            "the Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\n",
            "push the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\n",
            "must verify that\n",
            "∀j∈JmK,bj=∑\n",
            "i:T(xi)=yjai (1.7)\n",
            "which we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\n",
            "parameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\n",
            "min\n",
            "T{∑\n",
            "ic(xi,T(xi)) ;T♯α=β}\n",
            ". (1.8)\n",
            "Such a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\n",
            "indicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\n",
            "∑\n",
            "i∈σ−1(j)ai=bj.\n",
            "In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\n",
            "constraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\n",
            "optimal matching problem (1.5) where the cost matrix is\n",
            "Ci,jdef.=c(xi,yj).\n",
            "Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\n",
            "to another. This happens when their weight vectors are not compatible, which is always the case when the\n",
            "target measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\n",
            "an (optimal) Monge map between αandβ, but there is no Monge map from βtoα.\n",
            "6\n",
            "Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\n",
            "(X,Y) as ﬁnding a map T:X→Y that minimizes\n",
            "min\n",
            "T{∫\n",
            "Xc(x,T(x))dα(x) ;T♯α=β}\n",
            "(1.9)\n",
            "The constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\n",
            "operator (1.2).\n",
            "1.3 Kantorovitch Problem\n",
            "The assignment problem has several limitations in practical settings, also encountered when using the\n",
            "Monge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\n",
            "be used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\n",
            "uniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\n",
            "also be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\n",
            "(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\n",
            "set for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\n",
            "constraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.\n",
            "Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\n",
            "ture of transportation, namely the fact that a source point xican only be assigned to another, or transported\n",
            "to one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\n",
            "dispatched across several locations. Kantorovich moves away from the idea that mass transportation should\n",
            "be “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\n",
            "commonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\n",
            "using, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\n",
            "+, where Pi,jdescribes the\n",
            "amount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\n",
            "of discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\n",
            "U(a,b)def.={\n",
            "P∈Rn×m\n",
            "+ ;P1m=aand PT1n=b}\n",
            ", (1.10)\n",
            "where we used the following matrix-vector notation\n",
            "P1m=\n",
            "∑\n",
            "jPi,j\n",
            "\n",
            "i∈Rnand PT1n=(∑\n",
            "iPi,j)\n",
            "j∈Rm.\n",
            "The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\n",
            "polytope (the convex hull of a ﬁnite set of matrices).\n",
            "Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\n",
            "asymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\n",
            "U(a,b) if and only if PTis inU(b,a).\n",
            "Kantorovich’s optimal transport problem now reads\n",
            "LC(a,b)def.= min\n",
            "P∈U(a,b)⟨C,P⟩def.=∑\n",
            "i,jCi,jPi,j. (1.11)\n",
            "This is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\n",
            "not necessarily unique.\n",
            "7\n",
            "↵\u0000\n",
            "↵\u0000Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\n",
            "indicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\n",
            "corresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points).\n",
            "Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\n",
            "associate two arbitrary discrete measures.\n",
            "Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\n",
            "ing permutation matrix,\n",
            "∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\n",
            "0 otherwise.(1.12)\n",
            "One can check that in that case\n",
            "⟨C,Pσ⟩=1\n",
            "nn∑\n",
            "i=1Ci,σi,\n",
            "which shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\n",
            "couplings Pare restricted to be exactly permutation matrices:\n",
            "min\n",
            "σ∈Perm(n)1\n",
            "nn∑\n",
            "i=1Ci,σ(i)= min\n",
            "σ∈Perm(n)⟨C,Pσ⟩.\n",
            "Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\n",
            "polytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\n",
            "1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\n",
            "min\n",
            "σ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n).\n",
            "The following proposition shows that these problems result in fact in the same optimum, namely that\n",
            "one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\n",
            "measures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\n",
            "problems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\n",
            "case.\n",
            "Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\n",
            "solution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\n",
            "Perm(n)for Problem (1.5) .\n",
            "Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\n",
            "permutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\n",
            "minimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\n",
            "polyhedron.\n",
            "8\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "⇡\u0000↵\u0000↵\n",
            "Discrete Semi-discrete Continuous\n",
            "Figure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\n",
            "scenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.\n",
            "⇡\u0000↵\n",
            "⇡\u0000↵\n",
            "Figure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\n",
            "coupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\n",
            "couplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\n",
            "display with a black disk at position ( i,j) with radius proportional to Ti,j.\n",
            "Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\n",
            "arbitrary measures by considering couplings π∈M1\n",
            "+(X×Y ) which are joint distributions over the product\n",
            "space. The discrete case is a special situation where one imposes this product measure to be of the form\n",
            "π=∑\n",
            "i,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\n",
            "marginal constraint on joint probability distributions\n",
            "U(α,β)def.={\n",
            "π∈M1\n",
            "+(X×Y ) ;PX♯π=αandPY♯π=β}\n",
            ". (1.13)\n",
            "HerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y.\n",
            "Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\n",
            "measures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\n",
            "α(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y.\n",
            "The Kantorovich problem (1.11) is then generalized as\n",
            "Lc(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y). (1.14)\n",
            "This is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\n",
            "and continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\n",
            "involving discrete and continuous marginals.\n",
            "On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\n",
            "weak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\n",
            "9\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "\u0000↵\u0000↵⇡\n",
            "↵\u0000↵⇡\u0000Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\n",
            "above (arrows) and couplings below. Inspired by [ ?].\n",
            "is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\n",
            "to impose moment condition on αandβ.\n",
            "Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\n",
            "and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\n",
            "understood as a canonical way to lift a ground distance between points to a distance between histogram or\n",
            "measures.\n",
            "We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\n",
            "is ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\n",
            "to compare. The following proposition states that OT provides a meaningful distance between histograms\n",
            "supported on these bins.\n",
            "Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\n",
            "i,j)i,j∈Rn×nwhere D∈Rn×n\n",
            "+\n",
            "is a distance on JnK,i.e.\n",
            "1.D∈Rn×n\n",
            "+ is symmetric;\n",
            "2.Di,j= 0if and only if i=j;\n",
            "3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k.\n",
            "Then\n",
            "Wp(a,b)def.= LDp(a,b)1/p(1.15)\n",
            "(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\n",
            "Wp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\n",
            "∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b).\n",
            "Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\n",
            "Wp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\n",
            "elements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\n",
            "a non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function.\n",
            "To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\n",
            "gluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\n",
            "the explicit constuction of this glued coupling is simple. Let a,b,c∈Σn. Let PandQbe two optimal\n",
            "solutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\n",
            "and set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\n",
            "Sdef.=Pdiag(1/¯b)Q∈Rn×n\n",
            "+.\n",
            "10\n",
            "We remark that S∈U(a,c) because\n",
            "S1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\n",
            "where we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\n",
            "because necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c.\n",
            "The triangle inequality follows from\n",
            "Wp(a,c) =(\n",
            "min\n",
            "P∈U(a,c)⟨P,Dp⟩)1/p\n",
            "⩽⟨S,Dp⟩1/p\n",
            "=\n",
            "∑\n",
            "ikDp\n",
            "ik∑\n",
            "jPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijk(Dij+Djk)pPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "⩽\n",
            "∑\n",
            "ijkDp\n",
            "ijPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "ijkDp\n",
            "jkPijQjk\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij∑\n",
            "kQjk\n",
            "¯bj\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk∑\n",
            "iPij\n",
            "¯bj\n",
            "1/p\n",
            "=\n",
            "∑\n",
            "ijDp\n",
            "ijPij\n",
            "1/p\n",
            "+\n",
            "∑\n",
            "jkDp\n",
            "jkQjk\n",
            "1/p\n",
            "= Wp(a,b) + Wp(b,b).\n",
            "The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\n",
            "inD, and the third comes from Minkowski’s inequality.\n",
            "Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.\n",
            "Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\n",
            "X,i.e.\n",
            "(i)d(x,y) =d(y,x)⩾0;\n",
            "(ii)d(x,y) = 0 if and only if x=y;\n",
            "(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z).\n",
            "Then\n",
            "Wp(α,β)def.=Ldp(α,β)1/p(1.16)\n",
            "(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\n",
            "Wp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\n",
            "∀(α,β,γ )∈M1\n",
            "+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ).\n",
            "Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\n",
            "between (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ).\n",
            "The Wasserstein distance Wphas many important properties, the most important one being that it is a\n",
            "weak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\n",
            "spatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\n",
            "are not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\n",
            "with a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\n",
            "be ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\n",
            "p(δx,δy) =d(x,y). Indeed, it suﬃces\n",
            "to notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\n",
            "Wp\n",
            "p(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\n",
            "corresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne.\n",
            "11\n",
            "Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\n",
            "+(X)(denotedαk⇀α ) if and only if\n",
            "for any continuous function g∈C(X),∫\n",
            "Xgdαk→∫\n",
            "Xgdα. This notion of weak convergence corresponds to\n",
            "the convergence in law of random vectors.\n",
            "This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\n",
            "convergence of the moments up to order pfor unbounded metric spaces).\n",
            "Note that there exists alternative distances which also metrize weak convergence. The simplest one are\n",
            "Hilbertian norms, deﬁned as\n",
            "||α||2\n",
            "kdef.=Eα⊗α(k) =∫\n",
            "X×Xk(x,y)dα(x)dα(y)\n",
            "for a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\n",
            "e−||x−y||2\n",
            "2σ2for some choice of bandwidth σ>0.\n",
            "This convergence should not be confounded with the strong convergence of measures, which is metrized\n",
            "by the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure.\n",
            "Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\n",
            "as interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\n",
            "pivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\n",
            "exists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\n",
            "the auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\n",
            "which is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\n",
            "OT problem.\n",
            "1.4 Duality\n",
            "The Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\n",
            "naturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\n",
            "following fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\n",
            "relationship between the primal and dual problems.\n",
            "Proposition 4. One has\n",
            "LC(a,b) = max\n",
            "(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\n",
            "where the set of admissible potentials is\n",
            "R(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\n",
            "Proof. This result is a direct consequence of the more general result on the strong duality for linear pro-\n",
            "grams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\n",
            "is a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\n",
            "with the use of Lagrangian duality. The Lagangian associate to (1.11) reads\n",
            "min\n",
            "P⩾0max\n",
            "(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\n",
            "For linear program, one can always exchange the min and the max and get the same value of the linear\n",
            "program, and one thus consider\n",
            "max\n",
            "(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\n",
            "P⩾0⟨C−f1⊤\n",
            "m−1ng⊤,P⟩.\n",
            "We conclude by remarking that\n",
            "min\n",
            "P⩾0⟨Q,P⟩={0 if Q⩾0\n",
            "−∞ otherwise\n",
            "so that the constraint reads C−f1⊤\n",
            "m−1ng⊤=C−f⊕g⩾0.\n",
            "12\n",
            "The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\n",
            "transport plan\n",
            "Supp( P)⊂{\n",
            "(i,j)∈JnK×JmK;fi+gj=Ci,j}\n",
            ". (1.20)\n",
            "To extend this primal-dual construction to arbitrary measures, it is important to realize that measures\n",
            "are naturally paired in duality with continuous functions (a measure can only be accessed through integration\n",
            "against continuous functions). The duality is formalized in the following proposition, which boils down to\n",
            "Proposition 4 when dealing with discrete measures.\n",
            "Proposition 5. One has\n",
            "Lc(α,β) = max\n",
            "(f,g)∈R(c)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(y)dβ(y), (1.21)\n",
            "where the set of admissible dual potentials is\n",
            "R(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\n",
            "Here, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”.\n",
            "The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e.\n",
            "(fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\n",
            "and (1.20) is generalized as\n",
            "Supp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\n",
            "Note that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\n",
            "trivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\n",
            "machinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\n",
            "Lipschitz regular, which enable to replace the constraint by a compact one.\n",
            "Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\n",
            "Rdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\n",
            "are equivalent.\n",
            "Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\n",
            "measures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\n",
            "Kantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\n",
            "Rd→Rd. This means that π= (Id,T)♯µ,i.e.\n",
            "∀h∈C(X×Y ),∫\n",
            "X×Yh(x,y)dπ(x,y) =∫\n",
            "Xh(x,T(x))dµ(x). (1.24)\n",
            "Furthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\n",
            "ϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\n",
            "related to the dual potential fsolving (1.21) asϕ(x) =||x||2\n",
            "2−f(x).\n",
            "Proof. We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\n",
            "that∫\n",
            "cdπ=Cα,β−2∫\n",
            "⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\n",
            "||x||2dα(x) +∫\n",
            "||y||2dβ(y). Instead of\n",
            "solving (1.14), one can thus consider the following problem\n",
            "max\n",
            "π∈U(α,β)∫\n",
            "X×Y⟨x, y⟩dπ(x,y),\n",
            "whose dual reads\n",
            "min\n",
            "(ϕ,ψ){∫\n",
            "Xϕdα+∫\n",
            "Yψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\n",
            ". (1.25)\n",
            "13\n",
            "The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\n",
            "2−f,||·||2\n",
            "2−g). One can replace the\n",
            "constraint by\n",
            "∀y, ψ (y)⩾ϕ∗(y)def.= sup\n",
            "x⟨x, y⟩−ϕ(x). (1.26)\n",
            "Hereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\n",
            "also ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\n",
            "minimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\n",
            "min\n",
            "ϕ∫\n",
            "Xϕdα+∫\n",
            "Yϕ∗dβ, (1.27)\n",
            "see also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\n",
            "twice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex.\n",
            "Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\n",
            "such anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\n",
            "y∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\n",
            "diﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\n",
            "everywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α.\n",
            "This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\n",
            "and its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\n",
            "of Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\n",
            "problem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map).\n",
            "Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\n",
            "be examined under the light that a convex function is the natural generalization of the notion of increasing\n",
            "functions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\n",
            "functions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?].\n",
            "Note also that this theorem can be extended in many directions. The condition that αhas a density can\n",
            "be weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\n",
            "thand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\n",
            "strictly convex function.\n",
            "For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\n",
            "constant) convex function which solves the following Monge-Amp ˜A¨re-type equation\n",
            "det(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\n",
            "where∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\n",
            "non-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\n",
            "Laplacian ∆ as a linearization since for smooth maps\n",
            "det(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε).\n",
            "The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.\n",
            "Special cases In general, computing OT distances is numerically involved. We review special favorable\n",
            "cases where the resolution of the OT problem is easy.\n",
            "Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\n",
            "the diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\n",
            "the 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\n",
            "discrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\n",
            "between two discrete measures αandβis equal to their total variation distance.\n",
            "14\n",
            "\u0000\u0000↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling.\n",
            "Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\n",
            "corresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\n",
            "yj⩽yj′.\n",
            "Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\n",
            "n∑n\n",
            "i=1δxiandβ=1\n",
            "n∑n\n",
            "j=1δyj,\n",
            "and assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\n",
            "y1⩽y2⩽...⩽yn, then one has the simple formula\n",
            "Wp(α,β)p=p∑\n",
            "i=1|xi−yi|p, (1.29)\n",
            "i.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\n",
            "αandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\n",
            "might change whenever some of the values change. That formula is a simple consequence of the more general\n",
            "remark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\n",
            "with the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\n",
            "discrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\n",
            "circle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\n",
            "of the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case.\n",
            "Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\n",
            "∀x∈R,Cα(x)def.=∫x\n",
            "−∞dα, (1.30)\n",
            "which is a function Cα:R→[0,1], and its pseudo-inverse C−1\n",
            "α: [0,1]→R∪{−∞}\n",
            "∀r∈[0,1],C−1\n",
            "α(r) = min\n",
            "x{x∈R∪{−∞} ;Cα(x)⩾r}.\n",
            "That function is also called the generalized quantile function of α. For anyp⩾1, one has\n",
            "Wp(α,β)p=||C−1\n",
            "α−C−1\n",
            "β||p\n",
            "Lp([0,1])=∫1\n",
            "0|C−1\n",
            "α(r)−C−1\n",
            "β(r)|pdr. (1.31)\n",
            "This means that through the map α↦→C−1\n",
            "α, the Wasserstein distance is isometric to a linear space equipped\n",
            "with theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\n",
            "metric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\n",
            "geometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\n",
            "in§??. Forp= 1, one even has the simpler formula\n",
            "W1(α,β) =||Cα−Cβ||L1(R)=∫\n",
            "R|Cα(x)−Cβ(x)|dx (1.32)\n",
            "=∫\n",
            "R⏐⏐⏐⏐∫x\n",
            "−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\n",
            "15\n",
            "µ ν (tT+ (1−t)Id)♯µ\n",
            "0 0.5 10.5Cµ\n",
            "Cν\n",
            "0 0.5 100.51\n",
            "Cµ-1\n",
            "Cν-1\n",
            "0 0.5 100.51\n",
            "T\n",
            "T-1\n",
            "0 0.5 100.51\n",
            "(Cα,Cβ) (C−1\n",
            "α,C−1\n",
            "β) ( T,T−1) (1−t)C−1\n",
            "α+tC−1\n",
            "β\n",
            "Figure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\n",
            "function as detailed in (1.34).\n",
            "which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\n",
            "mapTsuch thatT♯α=βis then deﬁned by\n",
            "T=C−1\n",
            "β◦Cα. (1.34)\n",
            "Figure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\n",
            "interpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\n",
            "optimal transport in 1-D, we refer the reader to [ ?, Chapter 2].\n",
            "Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\n",
            "then one can show that the following map\n",
            "T:x↦→mβ+A(x−mα), (1.35)\n",
            "where\n",
            "A=Σ−1\n",
            "2α(\n",
            "Σ1\n",
            "2αΣβΣ1\n",
            "2α)1\n",
            "2Σ−1\n",
            "2α=AT,\n",
            "is such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\n",
            "since\n",
            "ρβ(T(x)) = det(2πΣβ)−1\n",
            "2exp(−⟨T(x)−mβ,Σ−1\n",
            "β(T(x)−mβ)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα, ATΣ−1\n",
            "βA(x−mα)⟩)\n",
            "= det(2πΣβ)−1\n",
            "2exp(−⟨x−mα,Σ−1\n",
            "α(x−mα)⟩),\n",
            "and sinceTis a linear map we have that\n",
            "|detT′(x)|= detA=(detΣβ\n",
            "detΣα)1\n",
            "2\n",
            "and we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\n",
            "functionψ:x↦→1\n",
            "2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\n",
            "thatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??\n",
            "16\n",
            "-4 -2 0 2 4 6-3-2-101234\n",
            "ρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\n",
            "mean and variance matrices mα= (−2,0),Σα=1\n",
            "2(\n",
            "1−1\n",
            "2;−1\n",
            "21)\n",
            "andmβ= (3,1),Σβ=(\n",
            "2,1\n",
            "2;1\n",
            "2,1)\n",
            ". The\n",
            "arrows originate at random points xtaken on the plane and end at the corresponding mappings of those\n",
            "pointsT(x) =mβ+A(x−mα).\n",
            "\u0000m\n",
            "Figure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\n",
            "1√\n",
            "2πse−(x−m)2\n",
            "2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1.\n",
            "With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\n",
            "cost of that map is\n",
            "W2\n",
            "2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\n",
            "whereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\n",
            "B(Σα,Σβ)2def.= tr(\n",
            "Σα+Σβ−2(Σ1/2\n",
            "αΣβΣ1/2\n",
            "α)1/2)\n",
            ", (1.37)\n",
            "where Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\n",
            "B2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\n",
            "diagonals, the Bures metric is the Hellinger distance\n",
            "B(Σα,Σβ) =||√r−√s||2.\n",
            "For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\n",
            "Σ), as illustrated in Figure 1.11.\n",
            "For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?].\n",
            "1.5 Sinkhorn\n",
            "This section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\n",
            "of optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\n",
            "the original problem. This regularization has several important advantages, but a few stand out particularly:\n",
            "The minimization of the regularized problen can be solved using a simple alternate minimization scheme;\n",
            "that scheme translates into iterations that are simple matrix products, making them particularly suited to\n",
            "execution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\n",
            "and positions of the Diracs.\n",
            "17\n",
            "c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\n",
            "argminP∈Σ3⟨C,P⟩−εH(P) for a varying ε.\n",
            "Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\n",
            "H(P)def.=−∑\n",
            "i,jPi,j(log(Pi,j)−1), (1.38)\n",
            "with an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\n",
            "0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\n",
            "Pi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\n",
            "to obtain approximate solutions to the original transport problem (1.11):\n",
            "Lε\n",
            "C(a,b)def.= min\n",
            "P∈U(a,b)⟨P,C⟩−εH(P). (1.39)\n",
            "Since the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\n",
            "to regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\n",
            "transportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\n",
            "solution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\n",
            "to rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\n",
            "that, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\n",
            "more “blurred” traﬃc prediction.\n",
            "Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\n",
            "can thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\n",
            "from the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\n",
            "triangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\n",
            "problem towards an optimal solution of the original linear program has been studied by [ ?].\n",
            "Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\n",
            "with maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\n",
            "Pεε→0−→argmin\n",
            "P{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\n",
            "so that in particular\n",
            "Lε\n",
            "C(a,b)ε→0−→LC(a,b).\n",
            "One has\n",
            "Pεε→∞−→abT= (aibj)i,j. (1.41)\n",
            "Proof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\n",
            "ε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\n",
            "such that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b).\n",
            "By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\n",
            "0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\n",
            "18\n",
            "⇡\"↵\u0000\n",
            "\"\u0000↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6.\n",
            "Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\n",
            "n=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\n",
            "betweenxiandyj).\n",
            "Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\n",
            "P⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\n",
            "H(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\n",
            "0to this program is unique\n",
            "by strict convexity of −H, one has P⋆=P⋆\n",
            "0, and the whole sequence is converging.\n",
            "Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\n",
            "transport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\n",
            "coupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\n",
            "two independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\n",
            "performed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\n",
            "shows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\n",
            "becomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\n",
            "turn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\n",
            "faster statistical convergence (as exposed in §??).\n",
            "Deﬁning the Kullback-Leibler divergence between couplings as\n",
            "KL(P|K)def.=∑\n",
            "i,jPi,jlog(Pi,j\n",
            "Ki,j)\n",
            "−Pi,j+Ki,j, (1.43)\n",
            "the unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\n",
            "Cas\n",
            "Ki,jdef.=e−Ci,j\n",
            "ε\n",
            "Indeed one has that using the deﬁnition above\n",
            "Pε= ProjKL\n",
            "U(a,b)(K)def.= argmin\n",
            "P∈U(a,b)KL(P|K). (1.44)\n",
            "Remark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\n",
            "by the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\n",
            "regularized counterpart to (1.14) using\n",
            "Lε\n",
            "c(α,β)def.= min\n",
            "π∈U(α,β)∫\n",
            "X×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\n",
            "where the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\n",
            "KL(π|ξ)def.=∫\n",
            "X×Ylog(dπ\n",
            "dξ(x,y))\n",
            "dπ(x,y)+\n",
            "∫\n",
            "X×Y(dξ(x,y)−dπ(x,y)),(1.46)\n",
            "19\n",
            "and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\n",
            "dξwith respect to ξ. It is important to\n",
            "realize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\n",
            "plays no speciﬁc role, only its support matters.\n",
            "Formula (1.45) can be re-factored as a projection problem\n",
            "min\n",
            "π∈U(α,β)KL(π|K) (1.47)\n",
            "whereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\n",
            "εdµ(x)dν(y). This problem is often referred to as the\n",
            "“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?].\n",
            "Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§??\n",
            "details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\n",
            "the points of two measures.\n",
            "Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\n",
            "which can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\n",
            "the sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints.\n",
            "Proposition 7. The solution to (1.39) is unique and has the form\n",
            "∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\n",
            "for two (unknown) scaling variable (u,v)∈Rn\n",
            "+×Rm\n",
            "+.\n",
            "Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\n",
            "reads\n",
            "E(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩.\n",
            "Considering ﬁrst order conditions, we have\n",
            "∂E(P,f,g)\n",
            "∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj.\n",
            "which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\n",
            "which can be rewritten in the form provided in the proposition using non-negative vectors uandv.\n",
            "The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\n",
            "matrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\n",
            "correspond to the mass conservation constraints inherent to U(a,b),\n",
            "diag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\n",
            "These two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\n",
            "times Kvis\n",
            "u⊙(Kv) =aand v⊙(KTu) =b (1.50)\n",
            "where⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\n",
            "community as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\n",
            "these equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\n",
            "Equation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\n",
            "u(ℓ+1)def.=a\n",
            "Kv(ℓ)and v(ℓ+1)def.=b\n",
            "KTu(ℓ+1), (1.51)\n",
            "initialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\n",
            "vectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\n",
            "20\n",
            "`⇡(`)\"\n",
            "1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\n",
            "ε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\n",
            "Sinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\n",
            "in term of marginal constraint violation log( ||πℓ\n",
            "ε1m−b||1).\n",
            "solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\n",
            "so doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\n",
            "a justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\n",
            "the same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\n",
            "diag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\n",
            "optimal coupling solving (1.39) by progressively shifting the mass away from the diagonal.\n",
            "Remark 11 (Relation with iterative projections) .Denoting\n",
            "C1\n",
            "adef.={P;P1m=a}andC2\n",
            "bdef.={\n",
            "P;PT1m=b}\n",
            "the rows and columns constraints, one has U(a,b) =C1\n",
            "a∩C2\n",
            "b. One can use Bregman iterative projections [ ?]\n",
            "P(ℓ+1) def.= ProjKL\n",
            "C1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\n",
            "C2\n",
            "b(P(ℓ+1)). (1.52)\n",
            "Since the setsC1\n",
            "aandC2\n",
            "bare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?].\n",
            "These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\n",
            "P(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\n",
            "one has\n",
            "P(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\n",
            "and P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\n",
            "In practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\n",
            "multiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??).\n",
            "Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\n",
            "greatly simpliﬁed using Hilbert projective metric on Rn\n",
            "+,∗(positive vectors), deﬁned as\n",
            "∀(u,u′)∈(Rn\n",
            "+,∗)2, dH(u,u′)def.= log max\n",
            "i,i′uiu′\n",
            "i′\n",
            "ui′u′\n",
            "i.\n",
            "This can be shows to be a distance on the projective cone Rn\n",
            "+,∗/∼, where u∼u′means that∃s>0,u=su′\n",
            "(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\n",
            "triangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\n",
            "distance on bounded open convex sets [ ?]. The projective cone Rn\n",
            "+,∗/∼is a complete metric space for this\n",
            "distance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\n",
            "theorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\n",
            "proved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\n",
            "cone of positive vectors.\n",
            "21\n",
            "Theorem 2. Let K∈Rn×m\n",
            "+,∗, then for (v,v′)∈(Rm\n",
            "+,∗)2\n",
            "dH(Kv,Kv′)⩽λ(K)dH(v,v′)where\n",
            "\n",
            "λ(K)def.=√\n",
            "η(K)−1√\n",
            "η(K)+1<1\n",
            "η(K)def.= max\n",
            "i,j,k,ℓKi,kKj,ℓ\n",
            "Kj,kKi,ℓ.\n",
            "Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\n",
            "show the linear convergence of Sinkhorn’s iterations.\n",
            "Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\n",
            "dH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\n",
            "One also has\n",
            "dH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\n",
            "1−λ(K)\n",
            "dH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\n",
            "1−λ(K)(1.54)\n",
            "where we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\n",
            "∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\n",
            "where P⋆is the unique solution of (1.39) .\n",
            "Proof. One notice that for any ( v,v′)∈(Rm\n",
            "+,∗)2, one has\n",
            "dH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′).\n",
            "This shows that\n",
            "dH(u(ℓ+1),u⋆) =dH(a\n",
            "Kv(ℓ),a\n",
            "Kv⋆)\n",
            "=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆).\n",
            "where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\n",
            "dH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\n",
            "⩽dH(a\n",
            "Kv(ℓ),u(ℓ))\n",
            "+λ(K)dH(u(ℓ),u⋆)\n",
            "=dH(\n",
            "a,u(ℓ)⊙(Kv(ℓ)))\n",
            "+λ(K)dH(u(ℓ),u⋆),\n",
            "which gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\n",
            "of (1.55) follows from [ ?, Lemma 3]\n",
            "The bound (1.54) shows that some error measures on the marginal constraints violation, for instance\n",
            "∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.\n",
            "Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\n",
            "degrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\n",
            "Theory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\n",
            "convergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\n",
            "of the scaled coupling matrix.\n",
            "22\n",
            "Regularized Dual and Log-domain Computations The following proposition details the dual problem\n",
            "associated to (1.39).\n",
            "Proposition 8. One has\n",
            "Lε\n",
            "C(a,b) = max\n",
            "f∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\n",
            "The optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\n",
            "(u,v) = (ef/ε,eg/ε). (1.57)\n",
            "Proof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\n",
            "and dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\n",
            "LagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\n",
            "dual function equals\n",
            "f,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\n",
            "The entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\n",
            "⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\n",
            "=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\n",
            "therefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\n",
            "are those displayed in (1.56).\n",
            "Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\n",
            "problem (1.56) reads\n",
            "sup\n",
            "f,g∈C(X)×C(Y)∫\n",
            "Xf(x)dα(x) +∫\n",
            "Yg(x)dβ(x)−ε∫\n",
            "X×Ye−c(x,y)+f(x)+g(y)\n",
            "ε dα(x)dβ(y)\n",
            "This corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\n",
            "is retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\n",
            "potentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\n",
            "usec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\n",
            "convergence of Sinkhorn iterations, see [ ?] for more details.\n",
            "Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\n",
            "unconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\n",
            "update alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\n",
            "can easily notice that, writing Q(f,g) for the objective of (1.56) that\n",
            "∇|fQ(f,g) =a−ef/ε⊙(\n",
            "Keg/ε)\n",
            ", (1.59)\n",
            "∇|gQ(f,g) =b−eg/ε⊙(\n",
            "KTef/ε)\n",
            ". (1.60)\n",
            "Block coordinate ascent can therefore be implemented in a closed form by applying successively the following\n",
            "updates, starting from any arbitrary g(0), forl⩾0,\n",
            "f(ℓ+1)=εloga−εlog(\n",
            "Keg(ℓ)/ε)\n",
            ", (1.61)\n",
            "g(ℓ+1)=εlogb−εlog(\n",
            "KTef(ℓ+1)/ε)\n",
            ". (1.62)\n",
            "Such iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\n",
            "dual relations highlighted in (1.57). Indeed, we recover that at any iteration\n",
            "(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))).\n",
            "23\n",
            "Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\n",
            "using the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\n",
            "coordinates, namely\n",
            "minεz=−εlog∑\n",
            "ie−zi/ε.\n",
            "Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\n",
            "diﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\n",
            "rewritten\n",
            "(f(ℓ+1))i= minε(Cij−g(ℓ)\n",
            "j)j+εlogai, (1.63)\n",
            "(g(ℓ+1))j= minε(Cij−f(ℓ)\n",
            "i)i+εlogbj. (1.64)\n",
            "Here the term min ε(Cij−g(ℓ)\n",
            "j)jdenotes the soft-minimum of all values of the j-th column of matrix\n",
            "(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\n",
            "now a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\n",
            "we deﬁne\n",
            "Minrow\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)j)\n",
            "i∈Rn,\n",
            "Mincol\n",
            "ε(A)def.=(\n",
            "minε(Ai,j)i)\n",
            "j∈Rm.\n",
            "Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\n",
            "lar (??)). Using these notations, Sinkhorn’s iterates read\n",
            "f(ℓ+1)= Minrow\n",
            "ε(C−1ng(ℓ)T) +εloga, (1.65)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(C−f(ℓ)1mT) +εlogb. (1.66)\n",
            "Note that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\n",
            "because alternate minimization does not converge for constrained problems (which is the case for the un-\n",
            "regularized dual (1.17)).\n",
            "Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\n",
            "tions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\n",
            "ofε. Writing z = min z, that trick suggests to evaluate min εzas\n",
            "minεz= z−εlog∑\n",
            "ie−(zi−z)/ε. (1.67)\n",
            "Instead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\n",
            "previously computed scalings. This leads to the following stabilized iteration\n",
            "f(ℓ+1)= Minrow\n",
            "ε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\n",
            "g(ℓ+1)= Mincol\n",
            "ε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\n",
            "where we deﬁned\n",
            "S(f,g) =(\n",
            "Ci,j−fi−gj)\n",
            "i,j.\n",
            "In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\n",
            "arbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\n",
            "requiresnmcomputations of exp at each step. Computing a Minrow\n",
            "εor Mincol\n",
            "εis typically substantially\n",
            "slower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\n",
            "therefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously.\n",
            "In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\n",
            "εstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].\n",
            "24\n",
            "1.6 Extensions\n",
            "Wasserstein Barycenters. Given input histogram {bs}S\n",
            "s=1, wherebs∈Σns, and weights λ∈ΣS, a\n",
            "Wasserstein barycenter is computed by minimizing\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLCs(a,bs) (1.70)\n",
            "where the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\n",
            "barycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\n",
            "solves\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsWp\n",
            "p(a,bs).\n",
            "This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\n",
            "in particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\n",
            "has a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\n",
            "one guaranteeing the existence of a Monge map, see Remark ??).\n",
            "The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\n",
            "couplings ( Ps)sbetween each input and the barycenter itself\n",
            "min\n",
            "a∈Σn,(Ps∈Rn×ns)s{S∑\n",
            "s=1λs⟨Ps,Cs⟩;∀s,P⊤\n",
            "s1ns=a,P⊤\n",
            "s1n=bs}\n",
            ".\n",
            "Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\n",
            "can therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?].\n",
            "Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\n",
            "the barycenter problem becomes\n",
            "min\n",
            "α∈M1\n",
            "+(X)S∑\n",
            "s=1λsLc(α,βs). (1.71)\n",
            "In the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\n",
            "then this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\n",
            "barycenters of points ( xs)S\n",
            "s=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\n",
            "solution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\n",
            "of the barycenter α⋆is necessarily the barycenter of the mean, i.e.\n",
            "∫\n",
            "Xxdα⋆(x) =∑\n",
            "sλs∫\n",
            "Xxdαs(x),\n",
            "and the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\n",
            "approximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\n",
            "using discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\n",
            "re-cast (1.71) as a multi-marginal OT problem, see Remark ??.\n",
            "One can use entropic smoothing and approximate the solution of (1.70) using\n",
            "min\n",
            "a∈ΣnS∑\n",
            "s=1λsLε\n",
            "Cs(a,bs) (1.72)\n",
            "for someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\n",
            "descent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\n",
            "25\n",
            "useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\n",
            "but eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\n",
            "min\n",
            "(Ps)s{∑\n",
            "sλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\n",
            "(1.73)\n",
            "where we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\n",
            "the couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\n",
            "this problem, which also corresponds to iterative projection. This can also be seen as a special case of the\n",
            "generalized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\n",
            "form as\n",
            "Ps= diag( us)Kdiag(vs), (1.74)\n",
            "and the scalings are sequentially updated as\n",
            "∀s∈J1,SK,v(ℓ+1)\n",
            "sdef.=bs\n",
            "KT\n",
            "su(ℓ)\n",
            "s, (1.75)\n",
            "∀s∈J1,SK,u(ℓ+1)\n",
            "sdef.=a(ℓ+1)\n",
            "Ksv(ℓ+1)\n",
            "s, (1.76)\n",
            "where a(ℓ+1)def.=∏\n",
            "s(Ksv(ℓ+1)\n",
            "s)λs. (1.77)\n",
            "An alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\n",
            "problem, which detailed in the following proposition.\n",
            "Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\n",
            "(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\n",
            "max\n",
            "(fs,gs)s{∑\n",
            "sλs(\n",
            "⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\n",
            ";∑\n",
            "sλsfs= 0}\n",
            ". (1.78)\n",
            "Proof. Introducing Lagrange multipliers in (1.73) leads to\n",
            "min\n",
            "(Ps)s,amax\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "εKL(Ps|Ks) +⟨a−Ps1m,fs⟩\n",
            "+⟨bs−PsT1m,gs⟩)\n",
            ".\n",
            "Strong duality holds, so that one can exchange the min and the max, and gets\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs(\n",
            "⟨gs,bs⟩+ min\n",
            "PsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\n",
            "+ min\n",
            "a⟨∑\n",
            "sλsfs,a⟩.\n",
            "The explicit minimization on agives the constraint∑\n",
            "sλsfs= 0 together with\n",
            "max\n",
            "(fs,gs)s∑\n",
            "sλs⟨gs,bs⟩−εKL∗(fs⊕gs\n",
            "ε|Ks)\n",
            "where KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\n",
            "KL∗(U|K) =∑\n",
            "i,jKi,j(eUi,j−1), (1.79)\n",
            "26\n",
            "Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\n",
            "(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\n",
            "are uniform within the boundaries of the shape and null outside.\n",
            "which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\n",
            "∀(u,k)∈R2\n",
            "+,KL∗(u|k)def.= max\n",
            "rur−(rlog(r/k)−r+k)\n",
            "whose optimality condition reads u= log(r/k), i.e.r=keu, hence the result.\n",
            "Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\n",
            "form by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\n",
            "to the expression (1.76).\n",
            "Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\n",
            "of barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\n",
            "the computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\n",
            "to [?] for more details and other applications to computer graphics and imaging sciences.\n",
            "Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\n",
            "distribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\n",
            "Θ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\n",
            "term, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\n",
            "suitable parameter θis obtained by minimizing directly\n",
            "min\n",
            "θ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\n",
            "Of course, one can consider more complicated problems: for instance, the barycenter problem described\n",
            "in§??consists in a sum of such terms. However, most of these more advanced problems can be usually\n",
            "solved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\n",
            "or using automatic diﬀerentiation.\n",
            "The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\n",
            "as shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\n",
            "Σnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\n",
            "i=1θiαi\n",
            "is a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\n",
            "corresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\n",
            "a Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\n",
            "not convex.\n",
            "27\n",
            "g✓XZ⇣xz\u0000↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81.\n",
            "A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\n",
            "some discrete samples ( xi)n\n",
            "i=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\n",
            "θ↦→αθ∈M (X) to the observed empirical input measure β\n",
            "min\n",
            "θ∈ΘL(αθ,β) where β=1\n",
            "n∑\n",
            "iδxi, (1.81)\n",
            "whereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\n",
            "ure 1.16).\n",
            "In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\n",
            "reference measure), the maximum likelihood estimator (MLE) is obtained by solving\n",
            "min\n",
            "θLMLE(αθ,β)def.=−∑\n",
            "ilog(ρθ(xi)).\n",
            "This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d.\n",
            "samples of some ¯β, then\n",
            "LMLE(α,β)n→+∞−→ KL(α|¯β)\n",
            "This MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]).\n",
            "However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\n",
            "(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\n",
            "the same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\n",
            "in several cases of practical interest, the density ρθis inaccessible (or too hard to compute).\n",
            "A typical setup where both problems (singular and unknown densities) occur is for so-called generative\n",
            "models, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\n",
            "αθ=hθ,♯ζwherehθ:Z→X\n",
            "where the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\n",
            "that the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\n",
            "singular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\n",
            "is usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\n",
            "(zi)iare i.i.d. samples from ζ.\n",
            "In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\n",
            "LMLE, which needs to be written in dual form as\n",
            "L(α,β)def.= max\n",
            "(f,g)∈C(X)2{∫\n",
            "Xf(x)dα(x) +∫\n",
            "Xg(x)dβ(x) ; (f,g)∈R}\n",
            ". (1.82)\n",
            "Dual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\n",
            "setsR=R(c) as deﬁned in (1.22).\n",
            "28\n",
            "For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\n",
            "solving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\n",
            "respect toθis much more involved, and is typically highly non-convex.\n",
            "The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\n",
            "was initially introduced in [ ?], see also [ ?].\n",
            "Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\n",
            "thus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\n",
            "these spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\n",
            "namely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\n",
            "between the points on which the histograms are deﬁned. A typical scenario is when these matrices are (power\n",
            "of) distance matrices. The Gromov-Wasserstein problem reads\n",
            "GW(( a,D),(b,D′))2def.= min\n",
            "P∈U(a,b)ED,D′(P)def.=∑\n",
            "i,j,i′,j′|Di,i′−D′\n",
            "j,j′|2Pi,jPi′,j′. (1.83)\n",
            "This is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\n",
            "full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\n",
            "for a particular cost.\n",
            "One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\n",
            "metric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\n",
            "up to isometries preserving the measures. This distance was introduced and studied in details by Memoli\n",
            "in [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\n",
            "in [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\n",
            "Gromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?].\n",
            "Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\n",
            "metric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\n",
            "on their respective spaces. One deﬁnes\n",
            "GW((αX,dX),(αY,dY))2def.= min\n",
            "π∈U(αX,αY)∫\n",
            "X2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\n",
            "GW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\n",
            "(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′).\n",
            "Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\n",
            "thisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\n",
            "(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\n",
            "((x0,x1),(x′\n",
            "0,x′\n",
            "1))∈(X0×X 1)2,\n",
            "dt((x0,x1),(x′\n",
            "0,x′\n",
            "1))def.= (1−t)dX0(x0,x′\n",
            "0) +tdX1(x1,x′\n",
            "1).\n",
            "This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\n",
            "spaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\n",
            "spaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\n",
            "spaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\n",
            "detailed below.\n",
            "To approximate the computation of GW, and to help convergence of minimization schemes to better\n",
            "minima, one can consider the entropic regularized variant\n",
            "min\n",
            "P∈U(a,b)ED,D′(P)−εH(P). (1.85)\n",
            "29\n",
            "Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\n",
            "iterations (1.86). Extracted from [ ?].\n",
            "As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\n",
            "Sinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\n",
            "of the objective function lead to consider the succession of updates\n",
            "P(ℓ+1) def.= min\n",
            "P∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\n",
            "C(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\n",
            "which can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\n",
            "iterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\n",
            "compute soft maps between domains.\n",
            "30\n",
            "Bibliography\n",
            "[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\n",
            "LAB. SIAM, 2014.\n",
            "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\n",
            "and statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\n",
            "in Machine Learning , 3(1):1–122, 2011.\n",
            "[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.\n",
            "[4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\n",
            "piecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004.\n",
            "[5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\n",
            "Multiscale Modeling and Simulation , 5:861–899, 2005.\n",
            "[6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\n",
            "20:89–97, 2004.\n",
            "[7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\n",
            "duction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\n",
            "recovery , 9(263-340):227, 2010.\n",
            "[8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\n",
            "Numerica , 25:161–319, 2016.\n",
            "[9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\n",
            "on Scientiﬁc Computing , 20(1):33–61, 1999.\n",
            "[10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982.\n",
            "[11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\n",
            "Multiscale Modeling and Simulation , 4(4), 2005.\n",
            "[12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\n",
            "with a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004.\n",
            "[13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\n",
            "Dec 1994.\n",
            "[14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\n",
            "375. Springer Science & Business Media, 1996.\n",
            "[15] M. Figueiredo and R. Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans.\n",
            "Image Proc. , 12(8):906–916, 2003.\n",
            "[16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1.\n",
            "Birkh¨ auser Basel, 2013.\n",
            "31\n",
            "[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.\n",
            "[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\n",
            "tional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989.\n",
            "[19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\n",
            "1(3):127–239, 2014.\n",
            "[20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004.\n",
            "[21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\n",
            "Gaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003.\n",
            "[22] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys.\n",
            "D, 60(1-4):259–268, 1992.\n",
            "[23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich.\n",
            "Variational methods in imaging . Springer, 2009.\n",
            "[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\n",
            "27(3):379–423, 1948.\n",
            "[25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\n",
            "related geometric multiscale analysis . Cambridge university press, 2015.\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYfQlZCjArl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a1069fce9e934db9a89bc14a3471837c",
            "6067b8adfb4f4d9c875acec37cd64849",
            "88b29eb9d502499aa70da8fdb26d8436",
            "23f523f2397a4e819ba906210d45a691",
            "64f412fe8190482288122de8f57fb9d2",
            "efb2d3c5f8f748b181d0536f160695d3",
            "09d3c4a0dca2404bafc2552bd21120a7",
            "be25cf42cbbc4f4898367fd1cc25c03e",
            "73dbe7750def4092a338184f95c82da3",
            "f800ff3840a54a3cbf2f701d62491c5f",
            "cc1c941f66814cbb9839d32225bbca7f",
            "e516bd5b1e504e34a3ce87b92a092114",
            "2222b07218da4006babfb7171da5b4ca",
            "140e1f35c3e64c26971b78c4e75aff0a",
            "e90d1b1eda2346d7a7f77c258ce19870",
            "b0daf348bd51405ba302e6e4b8c4323a",
            "96a3ca38383743919d364746544f9cb3",
            "67ae62987f504a72aae5d00e09d9bf3e",
            "354f4a4c71b34f7cb014c01b5c512b12",
            "377a2228557440ff96c816f76628f8ce",
            "c07c17f5788d43cb9c6c782dd2341ba4",
            "8d6f86ce1956474ea318b7f4a5e05440",
            "9773f36d82674c79890acc7d6c4af03b",
            "5c97ea3c194445cc80d5e4e506754685",
            "50bcc0a50d824b0e872ba9550b56f0e3",
            "9e67e4146fa94458b085da12fabab9c1",
            "cfd106cf14b24812a29fd4e271c5fa60",
            "fec1f0005ecf49d5a4f0ba084664274e",
            "73da1153194248f8834c5758d6ab2554",
            "159d24e8884a4d8cbba7e2e72c204f74",
            "6f3c2853d1fe4894a0b3f0068018c066",
            "044aea42e0864684ad64eecbc8c55f0d",
            "78f873aff64f4201813bb688557b998f",
            "c845b76c77584130ae4e6e3ed075e738",
            "8b4f9d7c77de42a88497939ed3a5a919",
            "888dcb3602c44f7da3388584ecc1404c",
            "4c4c8e3fc828429ead99de422046724c",
            "586a7e0198c9432b96387a8ab7d92ff4",
            "5aea687509f64dadaf7e2c820d45e50e",
            "64d0813edd924a54829adc8d86f4f3f0",
            "8c28e26ee6204af6b7a7868530b7946a",
            "a3680627758b407996558e6d2f40ad41",
            "e0a0ecae417d45a1aeab645f9e14207e",
            "6f3f0b1d06e644fb86a5e19ebb6175b0",
            "8887b5d2aacf444abbe150814735327c",
            "d9a57d5b66ea44fea458adf4229a00b5",
            "d1a90ff9aa08444188a56142a309f990",
            "cf663225320341278bdd355d51bee786",
            "510a4287650c421fbc086a64b144222a",
            "b8c0733864454c8996c763b086254fb8",
            "ad90060e8e894a769916f0b30fda7ee7",
            "b1d9f05f08874ae6a48d68f429b298ee",
            "1cc2fa800d824d4e80737c936ff03f48",
            "89267f7e68aa460e91a7d48b5ba9093a",
            "9b5d552704244faba3142df8cd4d1bf0",
            "30627dc2ed8e479ba79498228145d529",
            "93220d7a5bcf46aab6921d56e38a76c0",
            "a1a6b23aaea743f79aa784e7e936fc9f",
            "fdcffc7a65b84c14a3f9631f348647a8",
            "53d8d621b5414394a5117c7cfdee51f7",
            "879ef5190bf84644971245b1a43c8d2c",
            "4deed6c6930b4fe3a6f59f20704b074c",
            "8910a6d6f5874cd99adc7c6b5ea4ad36",
            "b76d93732fd14c0895a49e1785880783",
            "4402883ff24c4d918b198ec4b5b37d54",
            "4a4a22ec38b94f94bed3f1d685b54ab2"
          ]
        },
        "id": "7oiY0B-aAro-",
        "outputId": "20228bee-c4eb-412d-bd80-e434e2799f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1069fce9e934db9a89bc14a3471837c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e516bd5b1e504e34a3ce87b92a092114"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9773f36d82674c79890acc7d6c4af03b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c845b76c77584130ae4e6e3ed075e738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8887b5d2aacf444abbe150814735327c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_optimized.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30627dc2ed8e479ba79498228145d529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "semantic chunking with text embeddings:\n",
            "Chunk 1: page_content='Mathematical Foundations of Data Sciences\\nGabriel Peyr´ e\\nCNRS & DMA\\n´Ecole Normale Sup´ erieure\\ngabriel.peyre@ens.fr\\nhttps://mathematical-tours.github.io\\nwww.numerical-tours.com\\nAugust 14, 2019'\n",
            "Chunk 2: page_content='2'\n",
            "Chunk 3: page_content='Chapter 1\\nOptimal Transport\\n1.1 Radon Measures\\nMeasures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\\nbelongs to the probability simplex\\nΣndef.={\\na∈Rn\\n+;n∑\\ni=1ai= 1}\\n. A discrete measure with weights aand locations x1,...,xn∈X reads\\nα=n∑\\ni=1aiδxi (1.1)\\nwhereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\\nx. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\\nmeasure if each of the “weights” described in vector ais positive itself. Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\\n“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\\nto the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\\nequipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\\nit against continuous functions, denoted f∈C(X). Integration of f∈C(X) against a discrete measure αcomputes a sum\\n∫\\nXf(x)dα(x) =n∑\\ni=1aif(xi). More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\\ndα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\\ndx, which means that\\n∀h∈C(Rd),∫\\nRdh(x)dα(x) =∫\\nRdh(x)ρα(x)dx. An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\\nthe fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\\nXf(x)dα(x)∈R. IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity. Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\\ndual to smooth functions). For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\\nset of all positive measures on X. The set of probability measures is denoted M1\\n+(X), which means that\\nanyα∈M1\\n+(X) is positive, and that α(X) =∫\\nXdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\\nclasses of measures, beyond histograms, considered in this work.'\n",
            "Chunk 4: page_content='3'\n",
            "Chunk 5: page_content='Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\\nFigure 1.1: Schematic display of discrete distributions α=∑n\\ni=1aiδxi(red corresponds to empirical uniform\\ndistribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\\n1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\\nand in 2-D using point clouds (radius equal to ai). Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\\nT♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\\npositions of all the points in the support of the measure\\nT♯αdef.=∑\\niaiδT(xi). For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\\nmental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow. Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\\nα∈M (X)reads\\n∀h∈C(Y),∫\\nYh(y)dβ(y) =∫\\nXh(T(x))dα(x). (1.2)\\nEquivalently, for any measurable set B⊂Y, one has\\nβ(B) =α({x∈X;T(x)∈B}). (1.3)\\nNote thatT♯preserves positivity and total mass, so that if α∈M1\\n+(X)thenT♯α∈M1\\n+(Y). Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\\nmeasurable space to another. The more general extension T♯can now “move” an entire probability measure\\nonXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\\na measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\\nnew measure onY) writtenT♯α. Note that such a push-forward T♯:M1\\n+(X)→M1\\n+(Y) is a linear operator\\nbetween measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2. Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\\nwith densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\\ndensities linearly as a change of variables in the integration formula, indeed\\nρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\\nwhereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\\nofT). This implies, denoting y=T(x)\\n|det(T′(x))|=ρα(x)\\nρβ(y).'\n",
            "Chunk 6: page_content='4'\n",
            "Chunk 7: page_content='=Pi\\x00xiT↵T]↵def.=Pi\\x00T(xi)\\nTT]gdef.=g\\x00TgPush-forward of measures Pull-back of functions\\nFigure 1.2: Comparison of push-forward T♯and pull-back T♯. Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\\nthe pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\\nmap deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\\nothers, in the sense that\\n∀(α,g)∈M (X)×C(Y),∫\\nYgd(T♯α) =∫\\nX(T♯g)dα. It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\\nthe presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\\nregistration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\\nbetween these push-forward and pull-back operators. Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\\nbutions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\\n(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\\n+(X) such\\nthatP(X∈A) =α(A) =∫\\nAdα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\\nanother push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\\nvariableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\\nyfromYis thus simply achieved by computing y=T(x) wherexis drawn from X. Convergence of random variable.'\n",
            "Chunk 8: page_content='Convergence of random variable (in probability, almost sure, in law),\\nconvergence of measures (strong, weak). 1.2 Monge Problem\\nGiven a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\\nbijectionσin the set Perm( n) of permutations of nelements solving\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i). (1.5)\\nOne could naively evaluate the cost function above using all permutations in the set Perm( n). However,\\nthat set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\\n10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\\nalgorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5'\n",
            "Chunk 9: page_content='x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\\neither matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\\nthe blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\\ndisk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\\n4⩽i⩽7 we haveT(xi) =y1. Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions. Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\\ncorners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3.'\n",
            "Chunk 10: page_content='In that case\\nonly two assignments exist, and they share the same cost. For discrete measures\\nα=n∑\\ni=1aiδxiandβ=m∑\\nj=1bjδyj (1.6)\\nthe Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\\npush the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\\nmust verify that\\n∀j∈JmK,bj=∑\\ni:T(xi)=yjai (1.7)\\nwhich we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\\nparameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\\nmin\\nT{∑\\nic(xi,T(xi)) ;T♯α=β}\\n. (1.8)\\nSuch a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\\nindicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\\n∑\\ni∈σ−1(j)ai=bj. In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\\nconstraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\\noptimal matching problem (1.5) where the cost matrix is\\nCi,jdef.=c(xi,yj). Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\\nto another. This happens when their weight vectors are not compatible, which is always the case when the\\ntarget measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\\nan (optimal) Monge map between αandβ, but there is no Monge map from βtoα. 6'\n",
            "Chunk 11: page_content='Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\\n(X,Y) as ﬁnding a map T:X→Y that minimizes\\nmin\\nT{∫\\nXc(x,T(x))dα(x) ;T♯α=β}\\n(1.9)\\nThe constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\\noperator (1.2). 1.3 Kantorovitch Problem\\nThe assignment problem has several limitations in practical settings, also encountered when using the\\nMonge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\\nbe used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\\nuniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\\nalso be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\\n(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\\nset for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\\nconstraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation.'\n",
            "Chunk 12: page_content='Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\\nture of transportation, namely the fact that a source point xican only be assigned to another, or transported\\nto one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\\ndispatched across several locations. Kantorovich moves away from the idea that mass transportation should\\nbe “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\\ncommonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\\nusing, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\\n+, where Pi,jdescribes the\\namount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\\nof discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\\nU(a,b)def.={\\nP∈Rn×m\\n+ ;P1m=aand PT1n=b}\\n, (1.10)\\nwhere we used the following matrix-vector notation\\nP1m=\\uf8eb\\n\\uf8ed∑\\njPi,j\\uf8f6\\n\\uf8f8\\ni∈Rnand PT1n=(∑\\niPi,j)\\nj∈Rm. The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\\npolytope (the convex hull of a ﬁnite set of matrices). Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\\nasymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\\nU(a,b) if and only if PTis inU(b,a). Kantorovich’s optimal transport problem now reads\\nLC(a,b)def.= min\\nP∈U(a,b)⟨C,P⟩def.=∑\\ni,jCi,jPi,j. (1.11)\\nThis is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\\nnot necessarily unique. 7'\n",
            "Chunk 13: page_content='↵\\x00\\n↵\\x00Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\\nindicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\\ncorresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points). Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\\nassociate two arbitrary discrete measures. Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\\ning permutation matrix,\\n∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\\n0 otherwise.(1.12)\\nOne can check that in that case\\n⟨C,Pσ⟩=1\\nnn∑\\ni=1Ci,σi,\\nwhich shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\\ncouplings Pare restricted to be exactly permutation matrices:\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i)= min\\nσ∈Perm(n)⟨C,Pσ⟩. Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\\npolytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\\n1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\\nmin\\nσ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n). The following proposition shows that these problems result in fact in the same optimum, namely that\\none can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\\nmeasures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\\nproblems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\\ncase. Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\\nsolution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\\nPerm(n)for Problem (1.5) . Proof.'\n",
            "Chunk 14: page_content='Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\\npermutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\\nminimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\\npolyhedron. 8'\n",
            "Chunk 15: page_content='⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\nDiscrete Semi-discrete Continuous\\nFigure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\\nscenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup.'\n",
            "Chunk 16: page_content='⇡\\x00↵\\n⇡\\x00↵\\nFigure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\\ncoupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\\ncouplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\\ndisplay with a black disk at position ( i,j) with radius proportional to Ti,j. Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\\narbitrary measures by considering couplings π∈M1\\n+(X×Y ) which are joint distributions over the product\\nspace. The discrete case is a special situation where one imposes this product measure to be of the form\\nπ=∑\\ni,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\\nmarginal constraint on joint probability distributions\\nU(α,β)def.={\\nπ∈M1\\n+(X×Y ) ;PX♯π=αandPY♯π=β}\\n. (1.13)\\nHerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y. Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\\nmeasures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\\nα(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y. The Kantorovich problem (1.11) is then generalized as\\nLc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y). (1.14)\\nThis is an inﬁnite-dimensional linear program over a space of measures. Figure 1.6 shows examples of discrete\\nand continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\\ninvolving discrete and continuous marginals. On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\\nweak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\\n9'\n",
            "Chunk 17: page_content='\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n↵\\x00↵⇡\\x00Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\\nabove (arrows) and couplings below. Inspired by [ ?].'\n",
            "Chunk 18: page_content='is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\\nto impose moment condition on αandβ. Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\\nand probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\\nunderstood as a canonical way to lift a ground distance between points to a distance between histogram or\\nmeasures. We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\\nis ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\\nto compare. The following proposition states that OT provides a meaningful distance between histograms\\nsupported on these bins. Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\\ni,j)i,j∈Rn×nwhere D∈Rn×n\\n+\\nis a distance on JnK,i.e. 1.D∈Rn×n\\n+ is symmetric;\\n2.Di,j= 0if and only if i=j;\\n3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k. Then\\nWp(a,b)def.= LDp(a,b)1/p(1.15)\\n(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\\nWp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\\n∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b). Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\\nWp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\\nelements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\\na non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function. To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\\ngluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\\nthe explicit constuction of this glued coupling is simple.'\n",
            "Chunk 19: page_content='Let a,b,c∈Σn. Let PandQbe two optimal\\nsolutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\\nand set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\\nSdef.=Pdiag(1/¯b)Q∈Rn×n\\n+. 10'\n",
            "Chunk 20: page_content='We remark that S∈U(a,c) because\\nS1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\\nwhere we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\\nbecause necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c. The triangle inequality follows from\\nWp(a,c) =(\\nmin\\nP∈U(a,c)⟨P,Dp⟩)1/p\\n⩽⟨S,Dp⟩1/p\\n=\\uf8eb\\n\\uf8ed∑\\nikDp\\nik∑\\njPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijk(Dij+Djk)pPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijkDp\\nijPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\nijkDp\\njkPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij∑\\nkQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk∑\\niPij\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk\\uf8f6\\n\\uf8f81/p\\n= Wp(a,b) + Wp(b,b). The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\\ninD, and the third comes from Minkowski’s inequality. Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.'\n",
            "Chunk 21: page_content='Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\\nX,i.e. (i)d(x,y) =d(y,x)⩾0;\\n(ii)d(x,y) = 0 if and only if x=y;\\n(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z). Then\\nWp(α,β)def.=Ldp(α,β)1/p(1.16)\\n(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\\nWp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\\n∀(α,β,γ )∈M1\\n+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ). Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\\nbetween (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ). The Wasserstein distance Wphas many important properties, the most important one being that it is a\\nweak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\\nspatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\\nare not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\\nwith a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\\nbe ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\\np(δx,δy) =d(x,y). Indeed, it suﬃces\\nto notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\\nWp\\np(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\\ncorresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne. 11'\n",
            "Chunk 22: page_content='Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\\n+(X)(denotedαk⇀α ) if and only if\\nfor any continuous function g∈C(X),∫\\nXgdαk→∫\\nXgdα. This notion of weak convergence corresponds to\\nthe convergence in law of random vectors. This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\\nconvergence of the moments up to order pfor unbounded metric spaces). Note that there exists alternative distances which also metrize weak convergence. The simplest one are\\nHilbertian norms, deﬁned as\\n||α||2\\nkdef.=Eα⊗α(k) =∫\\nX×Xk(x,y)dα(x)dα(y)\\nfor a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\\ne−||x−y||2\\n2σ2for some choice of bandwidth σ>0. This convergence should not be confounded with the strong convergence of measures, which is metrized\\nby the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure. Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\\nas interior point methods or simplex.'\n",
            "Chunk 23: page_content='In practice, the network simplex is an eﬃcient option, and it used\\npivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\\nexists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\\nthe auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\\nwhich is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\\nOT problem. 1.4 Duality\\nThe Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\\nnaturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\\nfollowing fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\\nrelationship between the primal and dual problems. Proposition 4. One has\\nLC(a,b) = max\\n(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\\nwhere the set of admissible potentials is\\nR(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\\nProof. This result is a direct consequence of the more general result on the strong duality for linear pro-\\ngrams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\\nis a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\\nwith the use of Lagrangian duality. The Lagangian associate to (1.11) reads\\nmin\\nP⩾0max\\n(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\\nFor linear program, one can always exchange the min and the max and get the same value of the linear\\nprogram, and one thus consider\\nmax\\n(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\\nP⩾0⟨C−f1⊤\\nm−1ng⊤,P⟩. We conclude by remarking that\\nmin\\nP⩾0⟨Q,P⟩={0 if Q⩾0\\n−∞ otherwise\\nso that the constraint reads C−f1⊤\\nm−1ng⊤=C−f⊕g⩾0. 12'\n",
            "Chunk 24: page_content='The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\\ntransport plan\\nSupp( P)⊂{\\n(i,j)∈JnK×JmK;fi+gj=Ci,j}\\n. (1.20)\\nTo extend this primal-dual construction to arbitrary measures, it is important to realize that measures\\nare naturally paired in duality with continuous functions (a measure can only be accessed through integration\\nagainst continuous functions). The duality is formalized in the following proposition, which boils down to\\nProposition 4 when dealing with discrete measures. Proposition 5. One has\\nLc(α,β) = max\\n(f,g)∈R(c)∫\\nXf(x)dα(x) +∫\\nYg(y)dβ(y), (1.21)\\nwhere the set of admissible dual potentials is\\nR(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\\nHere, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”. The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e. (fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\\nand (1.20) is generalized as\\nSupp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\\nNote that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\\ntrivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\\nmachinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\\nLipschitz regular, which enable to replace the constraint by a compact one. Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\\nRdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\\nare equivalent. Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\\nmeasures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\\nKantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\\nRd→Rd. This means that π= (Id,T)♯µ,i.e. ∀h∈C(X×Y ),∫\\nX×Yh(x,y)dπ(x,y) =∫\\nXh(x,T(x))dµ(x). (1.24)\\nFurthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\\nϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\\nrelated to the dual potential fsolving (1.21) asϕ(x) =||x||2\\n2−f(x). Proof.'\n",
            "Chunk 25: page_content='We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\\nthat∫\\ncdπ=Cα,β−2∫\\n⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\\n||x||2dα(x) +∫\\n||y||2dβ(y). Instead of\\nsolving (1.14), one can thus consider the following problem\\nmax\\nπ∈U(α,β)∫\\nX×Y⟨x, y⟩dπ(x,y),\\nwhose dual reads\\nmin\\n(ϕ,ψ){∫\\nXϕdα+∫\\nYψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\\n. (1.25)\\n13'\n",
            "Chunk 26: page_content='The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\\n2−f,||·||2\\n2−g). One can replace the\\nconstraint by\\n∀y, ψ (y)⩾ϕ∗(y)def.= sup\\nx⟨x, y⟩−ϕ(x). (1.26)\\nHereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\\nalso ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\\nminimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\\nmin\\nϕ∫\\nXϕdα+∫\\nYϕ∗dβ, (1.27)\\nsee also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\\ntwice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex. Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\\nsuch anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\\ny∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\\ndiﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\\neverywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α. This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\\nand its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\\nof Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\\nproblem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map). Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\\nbe examined under the light that a convex function is the natural generalization of the notion of increasing\\nfunctions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\\nfunctions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?]. Note also that this theorem can be extended in many directions.'\n",
            "Chunk 27: page_content='The condition that αhas a density can\\nbe weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\\nthand−1 (e.g. hypersurfaces).'\n",
            "Chunk 28: page_content='One can also consider costs of the form c(x,y) =h(x−y) wherehis a\\nstrictly convex function. For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\\nconstant) convex function which solves the following Monge-Amp ˜A¨re-type equation\\ndet(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\\nwhere∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\\nnon-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\\nLaplacian ∆ as a linearization since for smooth maps\\ndet(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε). The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution. Special cases In general, computing OT distances is numerically involved. We review special favorable\\ncases where the resolution of the OT problem is easy. Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\\nthe diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\\nthe 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\\ndiscrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\\nbetween two discrete measures αandβis equal to their total variation distance. 14'\n",
            "Chunk 29: page_content='\\x00\\x00↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling. Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\\ncorresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\\nyj⩽yj′. Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\\nn∑n\\ni=1δxiandβ=1\\nn∑n\\nj=1δyj,\\nand assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\\ny1⩽y2⩽...⩽yn, then one has the simple formula\\nWp(α,β)p=p∑\\ni=1|xi−yi|p, (1.29)\\ni.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\\nαandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\\nmight change whenever some of the values change. That formula is a simple consequence of the more general\\nremark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\\nwith the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\\ndiscrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\\ncircle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\\nof the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case. Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\\n∀x∈R,Cα(x)def.=∫x\\n−∞dα, (1.30)\\nwhich is a function Cα:R→[0,1], and its pseudo-inverse C−1\\nα: [0,1]→R∪{−∞}\\n∀r∈[0,1],C−1\\nα(r) = min\\nx{x∈R∪{−∞} ;Cα(x)⩾r}. That function is also called the generalized quantile function of α. For anyp⩾1, one has\\nWp(α,β)p=||C−1\\nα−C−1\\nβ||p\\nLp([0,1])=∫1\\n0|C−1\\nα(r)−C−1\\nβ(r)|pdr. (1.31)\\nThis means that through the map α↦→C−1\\nα, the Wasserstein distance is isometric to a linear space equipped\\nwith theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\\nmetric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\\ngeometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\\nin§??. Forp= 1, one even has the simpler formula\\nW1(α,β) =||Cα−Cβ||L1(R)=∫\\nR|Cα(x)−Cβ(x)|dx (1.32)\\n=∫\\nR⏐⏐⏐⏐∫x\\n−∞d(α−β)⏐⏐⏐⏐dx.'\n",
            "Chunk 30: page_content='(1.33)\\n15'\n",
            "Chunk 31: page_content='µ ν (tT+ (1−t)Id)♯µ\\n0 0.5 10.5Cµ\\nCν\\n0 0.5 100.51\\nCµ-1\\nCν-1\\n0 0.5 100.51\\nT\\nT-1\\n0 0.5 100.51\\n(Cα,Cβ) (C−1\\nα,C−1\\nβ) ( T,T−1) (1−t)C−1\\nα+tC−1\\nβ\\nFigure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\\nfunction as detailed in (1.34). which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\\nmapTsuch thatT♯α=βis then deﬁned by\\nT=C−1\\nβ◦Cα. (1.34)\\nFigure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\\ninterpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\\noptimal transport in 1-D, we refer the reader to [ ?, Chapter 2]. Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\\nthen one can show that the following map\\nT:x↦→mβ+A(x−mα), (1.35)\\nwhere\\nA=Σ−1\\n2α(\\nΣ1\\n2αΣβΣ1\\n2α)1\\n2Σ−1\\n2α=AT,\\nis such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\\nsince\\nρβ(T(x)) = det(2πΣβ)−1\\n2exp(−⟨T(x)−mβ,Σ−1\\nβ(T(x)−mβ)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα, ATΣ−1\\nβA(x−mα)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα,Σ−1\\nα(x−mα)⟩),\\nand sinceTis a linear map we have that\\n|detT′(x)|= detA=(detΣβ\\ndetΣα)1\\n2\\nand we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\\nfunctionψ:x↦→1\\n2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\\nthatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??'\n",
            "Chunk 32: page_content='16'\n",
            "Chunk 33: page_content='-4 -2 0 2 4 6-3-2-101234\\nρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\\nmean and variance matrices mα= (−2,0),Σα=1\\n2(\\n1−1\\n2;−1\\n21)\\nandmβ= (3,1),Σβ=(\\n2,1\\n2;1\\n2,1)\\n. The\\narrows originate at random points xtaken on the plane and end at the corresponding mappings of those\\npointsT(x) =mβ+A(x−mα). \\x00m\\nFigure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\\n1√\\n2πse−(x−m)2\\n2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1. With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\\ncost of that map is\\nW2\\n2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\\nwhereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\\nB(Σα,Σβ)2def.= tr(\\nΣα+Σβ−2(Σ1/2\\nαΣβΣ1/2\\nα)1/2)\\n, (1.37)\\nwhere Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\\nB2is convex with respect to both its arguments.'\n",
            "Chunk 34: page_content='In the case where Σα= diag(ri)iandΣβ= diag(si)iare\\ndiagonals, the Bures metric is the Hellinger distance\\nB(Σα,Σβ) =||√r−√s||2. For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\\nΣ), as illustrated in Figure 1.11. For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?]. 1.5 Sinkhorn\\nThis section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\\nof optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\\nthe original problem. This regularization has several important advantages, but a few stand out particularly:\\nThe minimization of the regularized problen can be solved using a simple alternate minimization scheme;\\nthat scheme translates into iterations that are simple matrix products, making them particularly suited to\\nexecution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\\nand positions of the Diracs. 17'\n",
            "Chunk 35: page_content='c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\\nargminP∈Σ3⟨C,P⟩−εH(P) for a varying ε. Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\\nH(P)def.=−∑\\ni,jPi,j(log(Pi,j)−1), (1.38)\\nwith an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\\n0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\\nPi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\\nto obtain approximate solutions to the original transport problem (1.11):\\nLε\\nC(a,b)def.= min\\nP∈U(a,b)⟨P,C⟩−εH(P). (1.39)\\nSince the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\\nto regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\\ntransportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\\nsolution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\\nto rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\\nthat, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\\nmore “blurred” traﬃc prediction. Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\\ncan thus be visualized as a triangle in 2-D). Note how the entropy pushes the original LP solution away\\nfrom the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\\ntriangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\\nproblem towards an optimal solution of the original linear program has been studied by [ ?]. Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\\nwith maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\\nPεε→0−→argmin\\nP{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\\nso that in particular\\nLε\\nC(a,b)ε→0−→LC(a,b). One has\\nPεε→∞−→abT= (aibj)i,j.'\n",
            "Chunk 36: page_content='(1.41)\\nProof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\\nε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\\nsuch that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b).'\n",
            "Chunk 37: page_content='We consider any Psuch that⟨C,P⟩= LC(a,b). By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\\n0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\\n18'\n",
            "Chunk 38: page_content='⇡\"↵\\x00\\n\"\\x00↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6. Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\\nn=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\\nbetweenxiandyj). Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\\nP⋆is a feasible point of (1.40).'\n",
            "Chunk 39: page_content='Furthermore, dividing by εℓin (1.42) and taking the limit shows that\\nH(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\\n0to this program is unique\\nby strict convexity of −H, one has P⋆=P⋆\\n0, and the whole sequence is converging. Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\\ntransport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\\ncoupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\\ntwo independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\\nperformed in [ ?], including a ﬁrst order expansion in ε(resp. 1/ε) nearε= 0 (respε= +∞). Figure 1.13\\nshows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\\nbecomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\\nturn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\\nfaster statistical convergence (as exposed in §??). Deﬁning the Kullback-Leibler divergence between couplings as\\nKL(P|K)def.=∑\\ni,jPi,jlog(Pi,j\\nKi,j)\\n−Pi,j+Ki,j, (1.43)\\nthe unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\\nCas\\nKi,jdef.=e−Ci,j\\nε\\nIndeed one has that using the deﬁnition above\\nPε= ProjKL\\nU(a,b)(K)def.= argmin\\nP∈U(a,b)KL(P|K). (1.44)\\nRemark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\\nby the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\\nregularized counterpart to (1.14) using\\nLε\\nc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\\nwhere the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\\nKL(π|ξ)def.=∫\\nX×Ylog(dπ\\ndξ(x,y))\\ndπ(x,y)+\\n∫\\nX×Y(dξ(x,y)−dπ(x,y)),(1.46)\\n19'\n",
            "Chunk 40: page_content='and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\\ndξwith respect to ξ. It is important to\\nrealize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\\nplays no speciﬁc role, only its support matters. Formula (1.45) can be re-factored as a projection problem\\nmin\\nπ∈U(α,β)KL(π|K) (1.47)\\nwhereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\\nεdµ(x)dν(y). This problem is often referred to as the\\n“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?]. Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§?? details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\\nthe points of two measures. Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\\nwhich can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\\nthe sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints. Proposition 7. The solution to (1.39) is unique and has the form\\n∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\\nfor two (unknown) scaling variable (u,v)∈Rn\\n+×Rm\\n+. Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\\nreads\\nE(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩. Considering ﬁrst order conditions, we have\\n∂E(P,f,g)\\n∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj. which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\\nwhich can be rewritten in the form provided in the proposition using non-negative vectors uandv. The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\\nmatrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\\ncorrespond to the mass conservation constraints inherent to U(a,b),\\ndiag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\\nThese two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\\ntimes Kvis\\nu⊙(Kv) =aand v⊙(KTu) =b (1.50)\\nwhere⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\\ncommunity as the matrix scaling problem (see [ ?] and references therein).'\n",
            "Chunk 41: page_content='An intuitive way to try to solve\\nthese equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\\nEquation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\\nu(ℓ+1)def.=a\\nKv(ℓ)and v(ℓ+1)def.=b\\nKTu(ℓ+1), (1.51)\\ninitialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\\nvectors is to be understood entry-wise. Note that a diﬀerent initialization will likely lead to a diﬀerent\\n20'\n",
            "Chunk 42: page_content='`⇡(`)\"\\n1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\\nε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\\nSinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\\nin term of marginal constraint violation log( ||πℓ\\nε1m−b||1). solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\\nso doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\\na justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\\nthe same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\\ndiag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\\noptimal coupling solving (1.39) by progressively shifting the mass away from the diagonal. Remark 11 (Relation with iterative projections) .Denoting\\nC1\\nadef.={P;P1m=a}andC2\\nbdef.={\\nP;PT1m=b}\\nthe rows and columns constraints, one has U(a,b) =C1\\na∩C2\\nb.'\n",
            "Chunk 43: page_content='One can use Bregman iterative projections [ ?]\\nP(ℓ+1) def.= ProjKL\\nC1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\\nC2\\nb(P(ℓ+1)). (1.52)\\nSince the setsC1\\naandC2\\nbare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?]. These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\\nP(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\\none has\\nP(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\\nand P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\\nIn practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\\nmultiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??). Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\\ngreatly simpliﬁed using Hilbert projective metric on Rn\\n+,∗(positive vectors), deﬁned as\\n∀(u,u′)∈(Rn\\n+,∗)2, dH(u,u′)def.= log max\\ni,i′uiu′\\ni′\\nui′u′\\ni. This can be shows to be a distance on the projective cone Rn\\n+,∗/∼, where u∼u′means that∃s>0,u=su′\\n(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\\ntriangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\\ndistance on bounded open convex sets [ ?]. The projective cone Rn\\n+,∗/∼is a complete metric space for this\\ndistance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\\ntheorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\\nproved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\\ncone of positive vectors. 21'\n",
            "Chunk 44: page_content='Theorem 2. Let K∈Rn×m\\n+,∗, then for (v,v′)∈(Rm\\n+,∗)2\\ndH(Kv,Kv′)⩽λ(K)dH(v,v′)where\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3λ(K)def.=√\\nη(K)−1√\\nη(K)+1<1\\nη(K)def.= max\\ni,j,k,ℓKi,kKj,ℓ\\nKj,kKi,ℓ. Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\\nshow the linear convergence of Sinkhorn’s iterations. Theorem 3.'\n",
            "Chunk 45: page_content='One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\\ndH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\\nOne also has\\ndH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\\n1−λ(K)\\ndH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\\n1−λ(K)(1.54)\\nwhere we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\\n∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\\nwhere P⋆is the unique solution of (1.39) . Proof. One notice that for any ( v,v′)∈(Rm\\n+,∗)2, one has\\ndH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′). This shows that\\ndH(u(ℓ+1),u⋆) =dH(a\\nKv(ℓ),a\\nKv⋆)\\n=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆). where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\\ndH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\\n⩽dH(a\\nKv(ℓ),u(ℓ))\\n+λ(K)dH(u(ℓ),u⋆)\\n=dH(\\na,u(ℓ)⊙(Kv(ℓ)))\\n+λ(K)dH(u(ℓ),u⋆),\\nwhich gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\\nof (1.55) follows from [ ?, Lemma 3]\\nThe bound (1.54) shows that some error measures on the marginal constraints violation, for instance\\n∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence. Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\\ndegrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\\nTheory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\\nconvergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\\nof the scaled coupling matrix. 22'\n",
            "Chunk 46: page_content='Regularized Dual and Log-domain Computations The following proposition details the dual problem\\nassociated to (1.39). Proposition 8.'\n",
            "Chunk 47: page_content='One has\\nLε\\nC(a,b) = max\\nf∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\\nThe optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\\n(u,v) = (ef/ε,eg/ε). (1.57)\\nProof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\\nand dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\\nLagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\\ndual function equals\\nf,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\\nThe entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\\n⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\\n=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\\ntherefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\\nare those displayed in (1.56). Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\\nproblem (1.56) reads\\nsup\\nf,g∈C(X)×C(Y)∫\\nXf(x)dα(x) +∫\\nYg(x)dβ(x)−ε∫\\nX×Ye−c(x,y)+f(x)+g(y)\\nε dα(x)dβ(y)\\nThis corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\\nis retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\\npotentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\\nusec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\\nconvergence of Sinkhorn iterations, see [ ?] for more details. Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\\nunconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\\nupdate alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\\ncan easily notice that, writing Q(f,g) for the objective of (1.56) that\\n∇|fQ(f,g) =a−ef/ε⊙(\\nKeg/ε)\\n, (1.59)\\n∇|gQ(f,g) =b−eg/ε⊙(\\nKTef/ε)\\n. (1.60)\\nBlock coordinate ascent can therefore be implemented in a closed form by applying successively the following\\nupdates, starting from any arbitrary g(0), forl⩾0,\\nf(ℓ+1)=εloga−εlog(\\nKeg(ℓ)/ε)\\n, (1.61)\\ng(ℓ+1)=εlogb−εlog(\\nKTef(ℓ+1)/ε)\\n. (1.62)\\nSuch iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\\ndual relations highlighted in (1.57). Indeed, we recover that at any iteration\\n(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))). 23'\n",
            "Chunk 48: page_content='Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\\nusing the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\\ncoordinates, namely\\nminεz=−εlog∑\\nie−zi/ε. Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\\ndiﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\\nrewritten\\n(f(ℓ+1))i= minε(Cij−g(ℓ)\\nj)j+εlogai, (1.63)\\n(g(ℓ+1))j= minε(Cij−f(ℓ)\\ni)i+εlogbj. (1.64)\\nHere the term min ε(Cij−g(ℓ)\\nj)jdenotes the soft-minimum of all values of the j-th column of matrix\\n(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\\nnow a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\\nwe deﬁne\\nMinrow\\nε(A)def.=(\\nminε(Ai,j)j)\\ni∈Rn,\\nMincol\\nε(A)def.=(\\nminε(Ai,j)i)\\nj∈Rm. Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\\nlar (??)). Using these notations, Sinkhorn’s iterates read\\nf(ℓ+1)= Minrow\\nε(C−1ng(ℓ)T) +εloga, (1.65)\\ng(ℓ+1)= Mincol\\nε(C−f(ℓ)1mT) +εlogb. (1.66)\\nNote that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\\nbecause alternate minimization does not converge for constrained problems (which is the case for the un-\\nregularized dual (1.17)). Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\\ntions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\\nofε. Writing z = min z, that trick suggests to evaluate min εzas\\nminεz= z−εlog∑\\nie−(zi−z)/ε. (1.67)\\nInstead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\\npreviously computed scalings. This leads to the following stabilized iteration\\nf(ℓ+1)= Minrow\\nε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\\ng(ℓ+1)= Mincol\\nε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\\nwhere we deﬁned\\nS(f,g) =(\\nCi,j−fi−gj)\\ni,j. In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\\narbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\\nrequiresnmcomputations of exp at each step.'\n",
            "Chunk 49: page_content='Computing a Minrow\\nεor Mincol\\nεis typically substantially\\nslower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\\ntherefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously. In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\\nεstrategy to signiﬁcantly speed up the computation using sparse grids [ ?]. 24'\n",
            "Chunk 50: page_content='1.6 Extensions\\nWasserstein Barycenters. Given input histogram {bs}S\\ns=1, wherebs∈Σns, and weights λ∈ΣS, a\\nWasserstein barycenter is computed by minimizing\\nmin\\na∈ΣnS∑\\ns=1λsLCs(a,bs) (1.70)\\nwhere the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\\nbarycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\\nsolves\\nmin\\na∈ΣnS∑\\ns=1λsWp\\np(a,bs). This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\\nin particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\\nhas a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\\none guaranteeing the existence of a Monge map, see Remark ??). The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\\ncouplings ( Ps)sbetween each input and the barycenter itself\\nmin\\na∈Σn,(Ps∈Rn×ns)s{S∑\\ns=1λs⟨Ps,Cs⟩;∀s,P⊤\\ns1ns=a,P⊤\\ns1n=bs}\\n. Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\\ncan therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?]. Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\\nthe barycenter problem becomes\\nmin\\nα∈M1\\n+(X)S∑\\ns=1λsLc(α,βs). (1.71)\\nIn the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\\nthen this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\\nbarycenters of points ( xs)S\\ns=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\\nsolution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\\nof the barycenter α⋆is necessarily the barycenter of the mean, i.e.'\n",
            "Chunk 51: page_content='∫\\nXxdα⋆(x) =∑\\nsλs∫\\nXxdαs(x),\\nand the support of α⋆is located in the convex hull of the supports of the ( αs)s. The consistency of the\\napproximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\\nusing discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\\nre-cast (1.71) as a multi-marginal OT problem, see Remark ??. One can use entropic smoothing and approximate the solution of (1.70) using\\nmin\\na∈ΣnS∑\\ns=1λsLε\\nCs(a,bs) (1.72)\\nfor someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\\ndescent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\\n25'\n",
            "Chunk 52: page_content='useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\\nbut eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\\nmin\\n(Ps)s{∑\\nsλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\\n(1.73)\\nwhere we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\\nthe couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\\nthis problem, which also corresponds to iterative projection. This can also be seen as a special case of the\\ngeneralized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\\nform as\\nPs= diag( us)Kdiag(vs), (1.74)\\nand the scalings are sequentially updated as\\n∀s∈J1,SK,v(ℓ+1)\\nsdef.=bs\\nKT\\nsu(ℓ)\\ns, (1.75)\\n∀s∈J1,SK,u(ℓ+1)\\nsdef.=a(ℓ+1)\\nKsv(ℓ+1)\\ns, (1.76)\\nwhere a(ℓ+1)def.=∏\\ns(Ksv(ℓ+1)\\ns)λs. (1.77)\\nAn alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\\nproblem, which detailed in the following proposition.'\n",
            "Chunk 53: page_content='Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\\n(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\\nmax\\n(fs,gs)s{∑\\nsλs(\\n⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\\n;∑\\nsλsfs= 0}\\n. (1.78)\\nProof. Introducing Lagrange multipliers in (1.73) leads to\\nmin\\n(Ps)s,amax\\n(fs,gs)s∑\\nsλs(\\nεKL(Ps|Ks) +⟨a−Ps1m,fs⟩\\n+⟨bs−PsT1m,gs⟩)\\n. Strong duality holds, so that one can exchange the min and the max, and gets\\nmax\\n(fs,gs)s∑\\nsλs(\\n⟨gs,bs⟩+ min\\nPsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\\n+ min\\na⟨∑\\nsλsfs,a⟩. The explicit minimization on agives the constraint∑\\nsλsfs= 0 together with\\nmax\\n(fs,gs)s∑\\nsλs⟨gs,bs⟩−εKL∗(fs⊕gs\\nε|Ks)\\nwhere KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\\nKL∗(U|K) =∑\\ni,jKi,j(eUi,j−1), (1.79)\\n26'\n",
            "Chunk 54: page_content='Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\\n(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\\nare uniform within the boundaries of the shape and null outside. which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\\n∀(u,k)∈R2\\n+,KL∗(u|k)def.= max\\nrur−(rlog(r/k)−r+k)\\nwhose optimality condition reads u= log(r/k), i.e.r=keu, hence the result. Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\\nform by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\\nto the expression (1.76). Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\\nof barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\\nthe computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\\nto [?] for more details and other applications to computer graphics and imaging sciences. Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\\ndistribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\\nΘ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\\nterm, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\\nsuitable parameter θis obtained by minimizing directly\\nmin\\nθ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\\nOf course, one can consider more complicated problems: for instance, the barycenter problem described\\nin§??consists in a sum of such terms.'\n",
            "Chunk 55: page_content='However, most of these more advanced problems can be usually\\nsolved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\\nor using automatic diﬀerentiation. The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\\nas shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\\nΣnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\\ni=1θiαi\\nis a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\\ncorresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\\na Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\\nnot convex. 27'\n",
            "Chunk 56: page_content='g✓XZ⇣xz\\x00↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81. A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\\nsome discrete samples ( xi)n\\ni=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\\nθ↦→αθ∈M (X) to the observed empirical input measure β\\nmin\\nθ∈ΘL(αθ,β) where β=1\\nn∑\\niδxi, (1.81)\\nwhereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\\nure 1.16). In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\\nreference measure), the maximum likelihood estimator (MLE) is obtained by solving\\nmin\\nθLMLE(αθ,β)def.=−∑\\nilog(ρθ(xi)). This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d. samples of some ¯β, then\\nLMLE(α,β)n→+∞−→ KL(α|¯β)\\nThis MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]). However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\\n(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\\nthe same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\\nin several cases of practical interest, the density ρθis inaccessible (or too hard to compute). A typical setup where both problems (singular and unknown densities) occur is for so-called generative\\nmodels, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\\nαθ=hθ,♯ζwherehθ:Z→X\\nwhere the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\\nthat the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\\nsingular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\\nis usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\\n(zi)iare i.i.d.'\n",
            "Chunk 57: page_content='samples from ζ. In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\\nLMLE, which needs to be written in dual form as\\nL(α,β)def.= max\\n(f,g)∈C(X)2{∫\\nXf(x)dα(x) +∫\\nXg(x)dβ(x) ; (f,g)∈R}\\n. (1.82)\\nDual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\\nsetsR=R(c) as deﬁned in (1.22). 28'\n",
            "Chunk 58: page_content='For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\\nsolving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\\nrespect toθis much more involved, and is typically highly non-convex. The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\\nwas initially introduced in [ ?], see also [ ?]. Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\\nthus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\\nthese spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\\nnamely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\\nbetween the points on which the histograms are deﬁned.'\n",
            "Chunk 59: page_content='A typical scenario is when these matrices are (power\\nof) distance matrices. The Gromov-Wasserstein problem reads\\nGW(( a,D),(b,D′))2def.= min\\nP∈U(a,b)ED,D′(P)def.=∑\\ni,j,i′,j′|Di,i′−D′\\nj,j′|2Pi,jPi′,j′. (1.83)\\nThis is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\\nfull generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\\nfor a particular cost. One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\\nmetric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\\nup to isometries preserving the measures. This distance was introduced and studied in details by Memoli\\nin [?].'\n",
            "Chunk 60: page_content='An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\\nin [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\\nGromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?]. Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\\nmetric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\\non their respective spaces. One deﬁnes\\nGW((αX,dX),(αY,dY))2def.= min\\nπ∈U(αX,αY)∫\\nX2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\\nGW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\\n(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′). Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\\nthisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\\n(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\\n((x0,x1),(x′\\n0,x′\\n1))∈(X0×X 1)2,\\ndt((x0,x1),(x′\\n0,x′\\n1))def.= (1−t)dX0(x0,x′\\n0) +tdX1(x1,x′\\n1). This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\\nspaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\\nspaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\\nspaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\\ndetailed below. To approximate the computation of GW, and to help convergence of minimization schemes to better\\nminima, one can consider the entropic regularized variant\\nmin\\nP∈U(a,b)ED,D′(P)−εH(P). (1.85)\\n29'\n",
            "Chunk 61: page_content='Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\\niterations (1.86). Extracted from [ ?].'\n",
            "Chunk 62: page_content='As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\\nSinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\\nof the objective function lead to consider the succession of updates\\nP(ℓ+1) def.= min\\nP∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\\nC(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\\nwhich can be interpreted as a mirror-descent scheme [ ?]. Each update can thus be solved using Sinkhorn\\niterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\\ncompute soft maps between domains. 30'\n",
            "Chunk 63: page_content='Bibliography\\n[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\\nLAB. SIAM, 2014. [2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\\nand statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\\nin Machine Learning , 3(1):1–122, 2011. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004. [4] E. Cand` es and D.'\n",
            "Chunk 64: page_content='Donoho. New tight frames of curvelets and optimal representations of objects with\\npiecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004. [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L.'\n",
            "Chunk 65: page_content='Ying. Fast discrete curvelet transforms. SIAM\\nMultiscale Modeling and Simulation , 5:861–899, 2005. [6] A. Chambolle. An algorithm for total variation minimization and applications. J. Math. Imaging Vis. ,\\n20:89–97, 2004. [7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\\nduction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\\nrecovery , 9(263-340):227, 2010. [8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\\nNumerica , 25:161–319, 2016. [9] S.S. Chen, D.L. Donoho, and M.A.'\n",
            "Chunk 66: page_content='Saunders. Atomic decomposition by basis pursuit. SIAM Journal\\non Scientiﬁc Computing , 20(1):33–61, 1999. [10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982. [11] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM\\nMultiscale Modeling and Simulation , 4(4), 2005. [12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\\nwith a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004. [13] D. Donoho and I.'\n",
            "Chunk 67: page_content='Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\\nDec 1994. [14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\\n375. Springer Science & Business Media, 1996. [15] M. Figueiredo and R.'\n",
            "Chunk 68: page_content='Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans. Image Proc. , 12(8):906–916, 2003. [16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1. Birkh¨ auser Basel, 2013. 31'\n",
            "Chunk 69: page_content='[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008.'\n",
            "Chunk 70: page_content='[18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\\ntional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989. [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\\n1(3):127–239, 2014. [20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004. [21] J. Portilla, V. Strela, M.J.'\n",
            "Chunk 71: page_content='Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\\nGaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003. [22] L. I. Rudin, S. Osher, and E.'\n",
            "Chunk 72: page_content='Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4):259–268, 1992. [23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich. Variational methods in imaging . Springer, 2009. [24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\\n27(3):379–423, 1948. [25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\\nrelated geometric multiscale analysis . Cambridge university press, 2015. 32'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "semantic_chunks = text_chunker.semantic_section_chunking( documents , text_embedding_model_name,  breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "\n",
        "print(\"\\nsemantic chunking with text embeddings:\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2001590e83e441338fa6ab39900910a7",
            "b56465047f324a41b672c2cbc3fc9abf",
            "180f77d723864fecaae9f44038beda59",
            "2c6b2c125c584d868130e675847fcf7c",
            "dcc4f5a5a2cd44fb822a2203f1ac4bd4",
            "143f09b783ba4af7b87a87688814ffc7",
            "d5d654fefae64341a0902cb823578b0e",
            "2636cd6f393e41eea314cc6c5569eed1",
            "6ab170d8365f4f818d15125a81499ab3",
            "fd7e0e0e7c2340f2810e4d052514b55e",
            "35fd7789f1f34e20b5166976f9fbf752",
            "a9e39c64bf80418c96cad5a01b513863",
            "42225bed3352494db2d1eefc418e7218",
            "27288aa967f74a96813061deb0f05e0a",
            "35761e232dd44dd78e1784d8d3ac789c",
            "da2c95a227154ac0819e0a1a55072eeb",
            "82f1e5dff20a4f819de25effaa7ba643",
            "1a04eaa4ddea4f3b9491e64c0497caa3",
            "4cf024361c414fd0959d0bc277efe70f",
            "8f5a93f308f94952b868d3d663c7608d",
            "d41bf9989ecb4d5390b7c8689125d559",
            "7d08618d1b3144f8a6466e197ebd13a0",
            "78aa8bad210a487c95be0322bbde14ef",
            "bc12fba260c140f49996117198010b0a",
            "6584287005d04856b1f6da458b1bad3c",
            "888da9be3d8048c2ada5a181ffc733ed",
            "eeb5144a5e174eaca2448637e7e60a76",
            "f6877040fc9c41998c70a35e549c1ce3",
            "4e6bbac3688d4cd9b25c90c809883afc",
            "29c277723a8b4c63bfa552c94d17d41d",
            "9d0e16e49d0b401780266e42db655ef8",
            "c6fdd0c1342e4a67b90f41394e6ba39e",
            "ec18bd7d5d0e4b6b92551f96c800c96c",
            "7cfa564fee914a90a9741097f1dfa372",
            "2ca25e29e7a945e2a29ffbac11acaa71",
            "4b4b4a707c0c4711bacfbd517ca79d35",
            "d705ca96460243acbcd036ae67061db4",
            "06018d2b98414b92b8e0a16557afe36b",
            "309a9d196bb4432697f5901f2d4fb85c",
            "ae20dd13b94b4c248f97d4f6309114f4",
            "89d2c556a89542beb5d74a43c529fa6b",
            "58678a2dc8f1403b8fc7a6016fc07d33",
            "b4e87963281747ecae932e21aa29484f",
            "5677087e133741269ff98459ca6e2693",
            "4d3b926ffe43428ab02cf485b8f47911",
            "e5dbc75e76f3429889569131ee8a2a84",
            "3bc4a79362724414b60b81f779715ed2",
            "d78137fac74b47eb92ff1880bf08c8e1",
            "ca97c78b2db14024abb0a35b55c9817f",
            "cec1ef8545144dfa917836d79c5acb08",
            "acb2e84dbc404e9cbad68cddec375b42",
            "3aa19fdf106a4e2ea53c18b5ba385c16",
            "f7be4571d6584befa0cc284c4c3a4eb9",
            "9498e71ab68e408982c87a6ec2d495e9",
            "6774b980d08349c4a95618dba0765825",
            "99c44087be8e444e919129a792c75208",
            "f174128f0ea341b18fc9ccc23eb34c15",
            "82934c9796ea41fab3a3292ad1a3e76c",
            "a85e723b6b614c6b9b4de3e2ae6ebf6e",
            "4d296eee6d4b40b3aeffa81d2d7f2ff3",
            "68e74173f71d4dc59d850a45ca8dae98",
            "465880c8b54549a3a89d79df215f6be8",
            "2cc855e95f784c1988ecaa74387c2a23",
            "58c2c3479a444ec8829a641ea0d0742b",
            "1c7eba7890514e3b9120bcb85bc86d90",
            "6557a684698844cf8d323c0fc78c5521"
          ]
        },
        "id": "SldDgWylAsFB",
        "outputId": "de67ad8f-331d-4892-a2b0-30261d9d3d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2001590e83e441338fa6ab39900910a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9e39c64bf80418c96cad5a01b513863"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78aa8bad210a487c95be0322bbde14ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cfa564fee914a90a9741097f1dfa372"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d3b926ffe43428ab02cf485b8f47911"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99c44087be8e444e919129a792c75208"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "semantic chunking with text embeddings:\n",
            "Chunk 1: page_content='Mathematical Foundations of Data Sciences\\nGabriel Peyr´ e\\nCNRS & DMA\\n´Ecole Normale Sup´ erieure\\ngabriel.peyre@ens.fr\\nhttps://mathematical-tours.github.io\\nwww.numerical-tours.com\\nAugust 14, 2019'\n",
            "Chunk 2: page_content='2'\n",
            "Chunk 3: page_content='Chapter 1\\nOptimal Transport\\n1.1 Radon Measures\\nMeasures. We will interchangeably the term histogram or probability vector for any element a∈Σnthat\\nbelongs to the probability simplex\\nΣndef.={\\na∈Rn\\n+;n∑\\ni=1ai= 1}\\n. A discrete measure with weights aand locations x1,...,xn∈X reads\\nα=n∑\\ni=1aiδxi (1.1)\\nwhereδxis the Dirac at position x, intuitively a unit of mass which is inﬁnitely concentrated at location\\nx. Such as measure describes a probability measure if, additionally, a∈Σn, and more generally a positive\\nmeasure if each of the “weights” described in vector ais positive itself. Remark 1 (General measures) .A convenient feature of OT is that it can deal with discrete and continuous\\n“objects” within the same framework. Such objects only need to be modelled as measures. This corresponds\\nto the notion of Radon measures M(X) on the spaceX. The formal deﬁnition of that set requires that Xis\\nequipped with a distance, usually denoted d, because one can only access a measure by “testing” (integrating)\\nit against continuous functions, denoted f∈C(X). Integration of f∈C(X) against a discrete measure αcomputes a sum\\n∫\\nXf(x)dα(x) =n∑\\ni=1aif(xi). More general measures, for instance on X=Rd(whered∈N∗is the dimension), can have a density\\ndα(x) =ρα(x)dxw.r.t. the Lebesgue measure, often denoted ρα=dα\\ndx, which means that\\n∀h∈C(Rd),∫\\nRdh(x)dα(x) =∫\\nRdh(x)ρα(x)dx. An arbitrary measure α∈M (X) (which needs not to have a density nor be a sum of Diracs) is deﬁned by\\nthe fact that it can be integrated agains any continuous function f∈C(X) and obtain∫\\nXf(x)dα(x)∈R. IfXis not compact, one should also impose that fhas compact support or at least as 0 limit at inﬁnity. Measure as thus in some sense “less regular” than functions, but more regular than distributions (which are\\ndual to smooth functions).'\n",
            "Chunk 4: page_content='For instance, the derivative of a Dirac is not a measure. We denote M+(X) the\\nset of all positive measures on X. The set of probability measures is denoted M1\\n+(X), which means that\\nanyα∈M1\\n+(X) is positive, and that α(X) =∫\\nXdα= 1. Figure 1.1 oﬀers a visualization of the diﬀerent\\nclasses of measures, beyond histograms, considered in this work. 3'\n",
            "Chunk 5: page_content='Discreted= 1 Discrete d= 2 Density d= 1 Density d= 2\\nFigure 1.1: Schematic display of discrete distributions α=∑n\\ni=1aiδxi(red corresponds to empirical uniform\\ndistribution ai= 1/n, and blue to arbitrary distributions) and densities d α(x) =ρα(x)dx(in violet), in both\\n1-D and 2-D. Discrete distributions in 1-D are displayed using vertical segments (with length equal to ai)\\nand in 2-D using point clouds (radius equal to ai). Operators on measures. For some continuous map T:X →Y , we deﬁne the pushforward operator\\nT♯:M(X)→M (Y). For discrete measures (1.1), the pushforward operation consists simply in moving the\\npositions of all the points in the support of the measure\\nT♯αdef.=∑\\niaiδT(xi). For more general measures, for instance for those with a density, the notion of push-forward plays a funda-\\nmental to describe spatial modiﬁcations of probability measures. The formal deﬁnition reads as follow. Deﬁnition 1 (Push-forward) .ForT:X → Y , the push forward measure β=T♯α∈ M (Y)of some\\nα∈M (X)reads\\n∀h∈C(Y),∫\\nYh(y)dβ(y) =∫\\nXh(T(x))dα(x). (1.2)\\nEquivalently, for any measurable set B⊂Y, one has\\nβ(B) =α({x∈X;T(x)∈B}). (1.3)\\nNote thatT♯preserves positivity and total mass, so that if α∈M1\\n+(X)thenT♯α∈M1\\n+(Y). Intuitively, a measurable map T:X→Y , can be interpreted as a function “moving” a single point from a\\nmeasurable space to another. The more general extension T♯can now “move” an entire probability measure\\nonXtowards a new probability measure on Y. The operator T♯“pushes forward” each elementary mass of\\na measureαonXby applying the map Tto obtain then an elementary mass in Y, to build on aggregate a\\nnew measure onY) writtenT♯α. Note that such a push-forward T♯:M1\\n+(X)→M1\\n+(Y) is a linear operator\\nbetween measures in the sense that for two measures α1,α2onX,T♯(α1+α2) =T♯α1+T♯α2. Remark 2 (Push-forward for densities) .Explicitly doing the change of variable in formula (1.2) for measures\\nwith densities ( ρα,ρβ) onRd(assumingTis smooth and a bijection) shows that a push-forward acts on\\ndensities linearly as a change of variables in the integration formula, indeed\\nρα(x) =|det(T′(x))|ρβ(T(x)) (1.4)\\nwhereT′(x)∈Rd×dis the Jacobian matrix of T(the matrix formed by taking the gradient of each coordinate\\nofT). This implies, denoting y=T(x)\\n|det(T′(x))|=ρα(x)\\nρβ(y).'\n",
            "Chunk 6: page_content='4'\n",
            "Chunk 7: page_content='=Pi\\x00xiT↵T]↵def.=Pi\\x00T(xi)\\nTT]gdef.=g\\x00TgPush-forward of measures Pull-back of functions\\nFigure 1.2: Comparison of push-forward T♯and pull-back T♯. Remark 3 (Push-forward vs. pull-back) .The push-forward T♯of measures should not be confounded with\\nthe pull-back of function T♯:C(Y)→C(X) which corresponds to the “warping” of functions. It is the linear\\nmap deﬁned, for g∈C(Y) byT♯g=g◦T. Push-forward and pull-back are actually adjoint one from each\\nothers, in the sense that\\n∀(α,g)∈M (X)×C(Y),∫\\nYgd(T♯α) =∫\\nX(T♯g)dα. It is important to realize that even if ( α,β) have densities ( ρα,ρβ),T♯αis not equal to T♯ρβ, because of\\nthe presence of the Jacobian in (1.4). This explains why OT should be used with caution to perform image\\nregistration, because it does not operate as an image warping method. Figure 1.2 illustrate the distinction\\nbetween these push-forward and pull-back operators. Remark 4 (Measures and random variables) .Radon measures can also be viewed as representing the distri-\\nbutions of random variables. A random variable XonXis actually a map X: Ω→X from some abstract\\n(often un-speciﬁed) probabized space (Ω ,P), and its distribution αis the Radon measure X∈M1\\n+(X) such\\nthatP(X∈A) =α(A) =∫\\nAdα(x). Equivalently, it is the push-forward of PbyX,α=X♯P. Applying\\nanother push-forward β=T♯αforT:X →Y , following (1.2), is equivalent to deﬁning another random\\nvariableY=T(X) :ω∈Ω→T(X(ω))∈Y, so thatβis the distribution of Y. Drawing a random sample\\nyfromYis thus simply achieved by computing y=T(x) wherexis drawn from X. Convergence of random variable.'\n",
            "Chunk 8: page_content='Convergence of random variable (in probability, almost sure, in law),\\nconvergence of measures (strong, weak). 1.2 Monge Problem\\nGiven a cost matrix ( Ci,j)i∈JnK,j∈JmK, assuming n=m, the optimal assignment problem seeks for a\\nbijectionσin the set Perm( n) of permutations of nelements solving\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i). (1.5)\\nOne could naively evaluate the cost function above using all permutations in the set Perm( n). However,\\nthat set has size n!, which is gigantic even for small n. Consider for instance that such a set has more than\\n10100elements [ ?] whennis as small as 70. That problem can therefore only be solved if there exist eﬃcient\\nalgorithms to optimize that cost function over the set of permutations, which will be the subject of §??. 5'\n",
            "Chunk 9: page_content='x1x2y1y2x1x2y1y2x4x5x6x3y3x7Figure 1.3: (left) blue dots from measure αand red dots from measure βare pairwise equidistant. Hence,\\neither matching σ= (1,2) (full line) or σ= (2,1) (dotted line) is optimal. (right) a Monge map can associate\\nthe blue measure αto the red measure β. The weights αiare displayed proportionally to the area of the\\ndisk marked at each location. The mapping here is such that T(x1) =T(x2) =y2,T(x3) =y3, whereas for\\n4⩽i⩽7 we haveT(xi) =y1. Remark 5 (Uniqueness) .Note that the optimal assignment problem may have several optimal solutions. Suppose for instance that n=m= 2 and that the matrix Cis the pairwise distance matrix between the 4\\ncorners of a 2-dimensional square of side length 1, as represented in the left plot in Figure 1.3.'\n",
            "Chunk 10: page_content='In that case\\nonly two assignments exist, and they share the same cost. For discrete measures\\nα=n∑\\ni=1aiδxiandβ=m∑\\nj=1bjδyj (1.6)\\nthe Monge problem [ ?] seeks for a map that associates to each point xia single point yj, and which must\\npush the mass of αtoward the mass of β, which is to say that such a map T:{x1,...,xn}→{y1,...,ym}\\nmust verify that\\n∀j∈JmK,bj=∑\\ni:T(xi)=yjai (1.7)\\nwhich we write in compact form as T♯α=β. This map should minimize some transportation cost, which is\\nparameterized by a function c(x,y) deﬁned for points ( x,y)∈X×Y\\nmin\\nT{∑\\nic(xi,T(xi)) ;T♯α=β}\\n. (1.8)\\nSuch a map between discrete points can be of course encoded, assuming all x’s andy’s are distinct, using\\nindicesσ:JnK→JmKso thatj=σ(i), and the mass conservation is written as\\n∑\\ni∈σ−1(j)ai=bj. In the special case when n=mand all weights are uniform, that is ai=bj= 1/n, then the mass conservation\\nconstraint implies that Tis a bijection, such that T(xi) =yσ(i), and the Monge problem is equivalent to the\\noptimal matching problem (1.5) where the cost matrix is\\nCi,jdef.=c(xi,yj). Whenn̸=m, note that, optimality aside, Monge maps may not even exist between an empirical measure\\nto another. This happens when their weight vectors are not compatible, which is always the case when the\\ntarget measure has more points than the source measure. For instance, the right plot in Figure 1.3 shows\\nan (optimal) Monge map between αandβ, but there is no Monge map from βtoα. 6'\n",
            "Chunk 11: page_content='Monge problem (1.8) is extended to the setting of two arbitrary probability measures ( α,β) on two spaces\\n(X,Y) as ﬁnding a map T:X→Y that minimizes\\nmin\\nT{∫\\nXc(x,T(x))dα(x) ;T♯α=β}\\n(1.9)\\nThe constraint T♯α=βmeans that Tpushes forward the mass of αtoβ, and makes use of the push-forward\\noperator (1.2). 1.3 Kantorovitch Problem\\nThe assignment problem has several limitations in practical settings, also encountered when using the\\nMonge problem. Indeed, because the assignment problem is formulated as a permutation problem, it can only\\nbe used to compare two points clouds of the same size. A direct generalization to discrete measures with non-\\nuniform weights can be carried out using Monge’s formalism of pushforward maps, but that formulation may\\nalso be degenerate it there does not exist feasible solutions satisfying the mass conservation constraint (1.7)\\n(see the end of Remark ??). Additionally, the assignment Problem (1.8) is combinatorial, whereas the feasible\\nset for the Monge Problem (1.9), consisting in all push-forward measures that satisfy the mass conservation\\nconstraint, is non-convex . Both are therefore diﬃcult to solve in their original formulation. Kantorovitch formulation for discrete measures. The key idea of [ ?] is to relax the deterministic na-\\nture of transportation, namely the fact that a source point xican only be assigned to another, or transported\\nto one and one location T(xi) only. Kantorovich proposes instead that the mass at any point xibe potentially\\ndispatched across several locations. Kantorovich moves away from the idea that mass transportation should\\nbe “deterministic” to consider instead a “probabilistic” (or “fuzzy”) transportation, which allows what is\\ncommonly known now as “mass splitting” from a source towards several targets. This ﬂexibility is encoded\\nusing, in place of a permutation σor a mapT, a coupling matrix P∈Rn×m\\n+, where Pi,jdescribes the\\namount of mass ﬂowing from bin i(or pointxi) towards bin j(or pointxj),xitowardsyjin the formalism\\nof discrete measures (1.6). Admissible couplings admit a far simpler characterization than Monge maps:\\nU(a,b)def.={\\nP∈Rn×m\\n+ ;P1m=aand PT1n=b}\\n, (1.10)\\nwhere we used the following matrix-vector notation\\nP1m=\\uf8eb\\n\\uf8ed∑\\njPi,j\\uf8f6\\n\\uf8f8\\ni∈Rnand PT1n=(∑\\niPi,j)\\nj∈Rm. The set of matrices U(a,b) is bounded, deﬁned by n+mequality constraints, and therefore a convex\\npolytope (the convex hull of a ﬁnite set of matrices). Additionally, whereas the Monge formulation (as illustrated in the right plot of Figure 1.3) was intrisically\\nasymmetric, Kantorovich’s relaxed formulation is always symmetric, in the sense that a coupling Pis in\\nU(a,b) if and only if PTis inU(b,a). Kantorovich’s optimal transport problem now reads\\nLC(a,b)def.= min\\nP∈U(a,b)⟨C,P⟩def.=∑\\ni,jCi,jPi,j. (1.11)\\nThis is a linear program (see Chapter ??), and as is usually the case with such programs, its solutions are\\nnot necessarily unique.'\n",
            "Chunk 12: page_content='7'\n",
            "Chunk 13: page_content='↵\\x00\\n↵\\x00Figure 1.4: Comparison of optimal matching and generic couplings. A black segment between xiandyj\\nindicates a non-zero element in the displayed optimal coupling Pi,jsolving (1.11). Left: optimal matching,\\ncorresponding to the setting of Proposition (1) (empirical measures with the same number n=mof points). Right: these two weighted point clouds cannot be matched; instead a Kantorovich coupling can be used to\\nassociate two arbitrary discrete measures. Permutation Matrices as Couplings For a permutation σ∈Perm(n), we write Pσfor the correspond-\\ning permutation matrix,\\n∀(i,j)∈JnK2,(Pσ)i,j={1/n ifj=σi,\\n0 otherwise.(1.12)\\nOne can check that in that case\\n⟨C,Pσ⟩=1\\nnn∑\\ni=1Ci,σi,\\nwhich shows that the assignment problem (1.5) can be recast as a Kantorovich problem (1.11) where the\\ncouplings Pare restricted to be exactly permutation matrices:\\nmin\\nσ∈Perm(n)1\\nnn∑\\ni=1Ci,σ(i)= min\\nσ∈Perm(n)⟨C,Pσ⟩. Next, one can easily check that the set of permutation matrices is strictly included in the so-called Birkhoﬀ\\npolytope U(1n/n,1n,n). Indeed, for any permutation σwe have Pσ1=1nandPσT1=1n, whereas\\n1n1nT/n2is a valid coupling but not a permutation matrix. Therefore, one has naturally that\\nmin\\nσ∈Perm(n)⟨C,Pσ⟩⩽LC(1n/n,1n/n). The following proposition shows that these problems result in fact in the same optimum, namely that\\none can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (1.11) between two uniform\\nmeasures a=b=1n/n, which shows that the Kantorovich relaxation is tight when considered on assignment\\nproblems. Figure 1.4 shows on the left a 2-D example of optimal matching corresponding to this special\\ncase. Proposition 1 (Kantorovich for matching) .Ifm=nanda=b=1n/n, then there exists an optimal\\nsolution for Problem (1.11) Pσ⋆, which is a permutation matrix associated to an optimal permutation σ⋆∈\\nPerm(n)for Problem (1.5) . Proof.'\n",
            "Chunk 14: page_content='Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n,1n/n) is equal to the set of\\npermutation matrices. A fundamental theorem of linear programming [ ?, Theorem 2.7] states that the\\nminimum of a linear objective in a non-empty polyhedron, if ﬁnite, is reached at an extremal point of the\\npolyhedron. 8'\n",
            "Chunk 15: page_content='⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\n⇡\\x00↵\\x00↵\\nDiscrete Semi-discrete Continuous\\nFigure 1.5: Schematic viewed of input measures ( α,β) and couplingsU(α,β) encountered in the three main\\nscenario for Kantorovich OT. Chapter ??is dedicated to the semi-discrete setup. ⇡\\x00↵\\n⇡\\x00↵\\nFigure 1.6: Left: “continuous” coupling πsolving (1.13) between two 1-D measure with density. The\\ncoupling is localized along the graph of the Monge map ( x,T(x)) (displayed in black). Right: “discrete”\\ncouplingTsolving (1.11) between two discrete measures of the form (1.6). The non-zero entries Ti,jare\\ndisplay with a black disk at position ( i,j) with radius proportional to Ti,j. Kantorovitch formulation for arbitrary measures. The deﬁnition of Lcin (??) can be extended to\\narbitrary measures by considering couplings π∈M1\\n+(X×Y ) which are joint distributions over the product\\nspace. The discrete case is a special situation where one imposes this product measure to be of the form\\nπ=∑\\ni,jPi,jδ(xi,yj). In the general case, the mass conservation constraint (1.10) should be rewritten as a\\nmarginal constraint on joint probability distributions\\nU(α,β)def.={\\nπ∈M1\\n+(X×Y ) ;PX♯π=αandPY♯π=β}\\n. (1.13)\\nHerePX♯andPY♯are the push-forward (see Deﬁnition 1) by the projections PX(x,y) =xandPY(x,y) =y. Figure 1.5 shows a schematic visualization of the coupling constraints for diﬀerent class of problem (discrete\\nmeasures and densities). Using (1.3), these marginal constraints are equivalent to imposing that π(A×Y) =\\nα(A) andπ(X×B) =β(B) for setsA⊂X andB⊂Y. The Kantorovich problem (1.11) is then generalized as\\nLc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y). (1.14)\\nThis is an inﬁnite-dimensional linear program over a space of measures.'\n",
            "Chunk 16: page_content='Figure 1.6 shows examples of discrete\\nand continuous optimal coupling solving (1.14). Figure 1.7 shows other examples of optimal 1-D couplings,\\ninvolving discrete and continuous marginals. On compact domain ( X,Y), (1.14) always has a solution, because using the weak-* topology (so called\\nweak topology of measures), the set of measure is compact, and a linear function with a continuous c(x,y)\\n9'\n",
            "Chunk 17: page_content='\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n\\x00↵\\x00↵⇡\\n↵\\x00↵⇡\\x00Figure 1.7: Four simple examples of optimal couplings between 1-D distributions, represented as maps\\nabove (arrows) and couplings below. Inspired by [ ?].'\n",
            "Chunk 18: page_content='is weak-* continuous. And the set of constraint is non empty, taking α⊗β. On non compact domain, needs\\nto impose moment condition on αandβ. Wasserstein distances. An important feature of OT is that it deﬁnes a distance between histograms\\nand probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed, OT can be\\nunderstood as a canonical way to lift a ground distance between points to a distance between histogram or\\nmeasures. We ﬁrst consider the case where, using a term ﬁrst introduce by [ ?], the “ground metric” matrix C\\nis ﬁxed, representing substitution costs between bins, and shared across several histograms we would like\\nto compare. The following proposition states that OT provides a meaningful distance between histograms\\nsupported on these bins. Proposition 2. We suppose n=m, and that for some p⩾1,C=Dp= (Dp\\ni,j)i,j∈Rn×nwhere D∈Rn×n\\n+\\nis a distance on JnK,i.e. 1.D∈Rn×n\\n+ is symmetric;\\n2.Di,j= 0if and only if i=j;\\n3.∀(i,j,k )∈JnK3,Di,k⩽Di,j+Dj,k. Then\\nWp(a,b)def.= LDp(a,b)1/p(1.15)\\n(note that Wpdepends on D) deﬁnes the p-Wasserstein distance on Σn,i.e. Wpis symmetric, positive,\\nWp(a,b) = 0 if and only if a=b, and it satisﬁes the triangle inequality\\n∀a,a′,b∈Σn,Wp(a,b)⩽Wp(a,a′) + Wp(a′,b). Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C=Dphas a null diagonal,\\nWp(a,a) = 0, with corresponding optimal transport matrix P⋆= diag( a); by the positivity of all oﬀ-diagonal\\nelements of Dp, Wp(a,b)>0 whenever a̸=b(because in this case, an admissible coupling necessarily has\\na non-zero element outside the diagonal); by symmetry of Dp, Wp(a,b) = 0 is itself a symmetric function. To prove the triangle inequality of Wasserstein distances for arbitrary measures, [ ?, Theorem 7.3] uses the\\ngluing lemma, which stresses the existence of couplings with a prescribed structure. In the discrete setting,\\nthe explicit constuction of this glued coupling is simple.'\n",
            "Chunk 19: page_content='Let a,b,c∈Σn. Let PandQbe two optimal\\nsolutions of the transport problems between aandb, and bandcrespectively. We deﬁne ¯bjdef.=bjifbj>0\\nand set otherwise ¯bj= 1 (or actually any other value). We then deﬁne\\nSdef.=Pdiag(1/¯b)Q∈Rn×n\\n+. 10'\n",
            "Chunk 20: page_content='We remark that S∈U(a,c) because\\nS1n=Pdiag(1/¯b)Q1n=P(b/¯b) =P1Supp( b)=a\\nwhere we denoted 1Supp( b)the indicator of the support of b, and we use the fact that P1Supp( b)=P1=b\\nbecause necessarily Pi,j= 0 forj /∈Supp( b). Similarly one veriﬁes that S⊤1n=c. The triangle inequality follows from\\nWp(a,c) =(\\nmin\\nP∈U(a,c)⟨P,Dp⟩)1/p\\n⩽⟨S,Dp⟩1/p\\n=\\uf8eb\\n\\uf8ed∑\\nikDp\\nik∑\\njPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijk(Dij+Djk)pPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n⩽\\uf8eb\\n\\uf8ed∑\\nijkDp\\nijPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\nijkDp\\njkPijQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij∑\\nkQjk\\n¯bj\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk∑\\niPij\\n¯bj\\uf8f6\\n\\uf8f81/p\\n=\\uf8eb\\n\\uf8ed∑\\nijDp\\nijPij\\uf8f6\\n\\uf8f81/p\\n+\\uf8eb\\n\\uf8ed∑\\njkDp\\njkQjk\\uf8f6\\n\\uf8f81/p\\n= Wp(a,b) + Wp(b,b). The ﬁrst inequality is due to the suboptimality of S, the second is the usual triangle inequality for elements\\ninD, and the third comes from Minkowski’s inequality. Proposition 2 generalizes from histogram to arbitrary measures that need not be discrete.'\n",
            "Chunk 21: page_content='Proposition 3. We assumeX=Y, and that for some p⩾1,c(x,y) =d(x,y)pwheredis a distance on\\nX,i.e. (i)d(x,y) =d(y,x)⩾0;\\n(ii)d(x,y) = 0 if and only if x=y;\\n(ii)∀(x,y,z )∈X3,d(x,z)⩽d(x,y) +d(y,z). Then\\nWp(α,β)def.=Ldp(α,β)1/p(1.16)\\n(note thatWpdepends on d) deﬁnes the p-Wasserstein distance on X,i.e.Wpis symmetric, positive,\\nWp(α,β) = 0 if and only if α=β, and it satisﬁes the triangle inequality\\n∀(α,β,γ )∈M1\\n+(X)3,Wp(α,γ)⩽Wp(α,β) +Wp(β,γ). Proof. The proof follows the same approach as that for Proposition 2 and relies on the existence of a coupling\\nbetween (α,γ) obtained by “guying” optimal couplings between ( α,β) and (β,γ). The Wasserstein distance Wphas many important properties, the most important one being that it is a\\nweak distance, i.e.it allows to compare singular distributions (for instance discrete ones) and to quantify\\nspatial shift between the supports of the distributions. In particular, “classical” distances (or divergences)\\nare not even deﬁned between discrete distributions (the L2norm can only be applied to continuous measures\\nwith a density with respect to a base measure, and the discrete ℓ2norm requires the positions ( xi,yj) to\\nbe ﬁxed to work). In sharp contrast, one has that for any p >0,Wp\\np(δx,δy) =d(x,y). Indeed, it suﬃces\\nto notice thatU(δx,δy) ={δx,y}and therefore the Kantorovich problem having only one feasible solution,\\nWp\\np(δx,δy) is necessarily ( d(x,y)p)1/p=d(x,y). This shows that Wp(δx,δy)→0 ifx→y. This property\\ncorresponds to the fact that Wpis a way to quantify the weak convergence as we now deﬁne. 11'\n",
            "Chunk 22: page_content='Deﬁnition 2 (Weak convergence) .(αk)kconverges weakly to αinM1\\n+(X)(denotedαk⇀α ) if and only if\\nfor any continuous function g∈C(X),∫\\nXgdαk→∫\\nXgdα. This notion of weak convergence corresponds to\\nthe convergence in law of random vectors. This convergence can be shown to be equivalent to Wp(αk,α)→0 [?, Theorem 6.8] (together with a\\nconvergence of the moments up to order pfor unbounded metric spaces). Note that there exists alternative distances which also metrize weak convergence. The simplest one are\\nHilbertian norms, deﬁned as\\n||α||2\\nkdef.=Eα⊗α(k) =∫\\nX×Xk(x,y)dα(x)dα(y)\\nfor a suitable choice of kernel k:X2→R. The most famous of such kernel is the Gaussian one k(x,y) =\\ne−||x−y||2\\n2σ2for some choice of bandwidth σ>0. This convergence should not be confounded with the strong convergence of measures, which is metrized\\nby the TV norm ||α||TVdef.=|α|(X), which is the total mass of the absolute value of the measure. Algorithms Since ( ??)ˆA is a linear program, it is possible to use any classical linear program solver, such\\nas interior point methods or simplex. In practice, the network simplex is an eﬃcient option, and it used\\npivoting rule adapted to the OT constraint set. In the case of the assignment problem, a=b=1n/n, there\\nexists faster combinatorial optimization scheme, the most famous ones being the Hungarian algorithm and\\nthe auction algorithm, which have roughly O(n3) complexity. Section 1.5 details an approximate algorithm,\\nwhich is typically faster, and amenable to parallelisation, but do not compute exactly the solution to the\\nOT problem. 1.4 Duality\\nThe Kantorovich problem (1.11) is a constrained convex minimization problem, and as such, it can be\\nnaturally paired with a so-called dual problem, which is a constrained concave maximization problem. The\\nfollowing fundamental proposition, which is a special case of Fenchel-Rockafellar duality theory, explains the\\nrelationship between the primal and dual problems. Proposition 4. One has\\nLC(a,b) = max\\n(f,g)∈R(a,b)⟨f,a⟩+⟨g,b⟩ (1.17)\\nwhere the set of admissible potentials is\\nR(a,b)def.={(f,g)∈Rn×Rm;∀(i,j)∈JnK×JmK,f⊕g⩽C} (1.18)\\nProof. This result is a direct consequence of the more general result on the strong duality for linear pro-\\ngrams [ ?, p.148,Theo.4.4]. The easier part of that result, namely that the right-hand side of Equation (1.17)\\nis a lower bound on L C(a,b) is discussed in ??. For the sake of completeness, let us derive this dual problem\\nwith the use of Lagrangian duality. The Lagangian associate to (1.11) reads\\nmin\\nP⩾0max\\n(f,g)∈Rn×Rm⟨C,P⟩+⟨a−P1m,f⟩+⟨b−P⊤1n,g⟩. (1.19)\\nFor linear program, one can always exchange the min and the max and get the same value of the linear\\nprogram, and one thus consider\\nmax\\n(f,g)∈Rn×Rm⟨a,f⟩+⟨b,g⟩+ min\\nP⩾0⟨C−f1⊤\\nm−1ng⊤,P⟩. We conclude by remarking that\\nmin\\nP⩾0⟨Q,P⟩={0 if Q⩾0\\n−∞ otherwise\\nso that the constraint reads C−f1⊤\\nm−1ng⊤=C−f⊕g⩾0.'\n",
            "Chunk 23: page_content='12'\n",
            "Chunk 24: page_content='The primal-dual optimality relation for the Lagrangian (1.19) allows to locate the support of the optimal\\ntransport plan\\nSupp( P)⊂{\\n(i,j)∈JnK×JmK;fi+gj=Ci,j}\\n. (1.20)\\nTo extend this primal-dual construction to arbitrary measures, it is important to realize that measures\\nare naturally paired in duality with continuous functions (a measure can only be accessed through integration\\nagainst continuous functions). The duality is formalized in the following proposition, which boils down to\\nProposition 4 when dealing with discrete measures. Proposition 5. One has\\nLc(α,β) = max\\n(f,g)∈R(c)∫\\nXf(x)dα(x) +∫\\nYg(y)dβ(y), (1.21)\\nwhere the set of admissible dual potentials is\\nR(c)def.={(f,g)∈C(X)×C(Y) ;∀(x,y),f(x) +g(y)⩽c(x,y)}. (1.22)\\nHere, (f,g)is a pair of continuous functions, and are often called “Kantorovich potentials”. The discrete case (1.17) corresponds to the dual vectors being samples of the continuous potentials, i.e. (fi,gj) = (f(xi),g(yj)). The primal-dual optimality conditions allow to track the support of optimal plan,\\nand (1.20) is generalized as\\nSupp(π)⊂{(x,y)∈X×Y ;f(x) +g(y) =c(x,y)}. (1.23)\\nNote that in contrast to the primal problem (1.14), showing the existence of solutions to (1.21) is non-\\ntrivial, because the constraint set R(c) is not compact and the function to minimize non-coercive. Using the\\nmachinery of c-transform detailed in Section ??, one can however show that optimal ( f,g) are necessarily\\nLipschitz regular, which enable to replace the constraint by a compact one. Benier’s Theorem and Monge-Amp` ere PDE The following celebrated theorem of [ ?] ensures that in\\nRdforp= 2, if at least one of the two inputs measures has a density, then Kantorovitch and Monge problems\\nare equivalent. Theorem 1 (Brenier) .In the caseX=Y=Rdandc(x,y) =||x−y||2, if at least one of the two inputs\\nmeasures (denoted α) has a density ραwith respect to the Lebesgue measure, then the optimal πin the\\nKantorovich formulation (1.14) is unique, and is supported on the graph (x,T(x))of a “Monge map” T:\\nRd→Rd. This means that π= (Id,T)♯µ,i.e. ∀h∈C(X×Y ),∫\\nX×Yh(x,y)dπ(x,y) =∫\\nXh(x,T(x))dµ(x). (1.24)\\nFurthermore, this map Tis uniquely deﬁned as the gradient of a convex function ϕ,T(x) =∇ϕ(x), where\\nϕis the unique (up to an additive constant) convex function such that (∇ϕ)♯µ=ν. This convex function is\\nrelated to the dual potential fsolving (1.21) asϕ(x) =||x||2\\n2−f(x). Proof.'\n",
            "Chunk 25: page_content='We sketch the main ingredients of the proof, more details can be found for instance in [ ?]. We remark\\nthat∫\\ncdπ=Cα,β−2∫\\n⟨x, y⟩dπ(x,y) where the constant is Cα,β=∫\\n||x||2dα(x) +∫\\n||y||2dβ(y). Instead of\\nsolving (1.14), one can thus consider the following problem\\nmax\\nπ∈U(α,β)∫\\nX×Y⟨x, y⟩dπ(x,y),\\nwhose dual reads\\nmin\\n(ϕ,ψ){∫\\nXϕdα+∫\\nYψdβ;∀(x,y), ϕ (x) +ψ(y)⩾⟨x, y⟩}\\n. (1.25)\\n13'\n",
            "Chunk 26: page_content='The relation between these variables and those of (1.22) is ( ϕ,ψ) = (||·||2\\n2−f,||·||2\\n2−g). One can replace the\\nconstraint by\\n∀y, ψ (y)⩾ϕ∗(y)def.= sup\\nx⟨x, y⟩−ϕ(x). (1.26)\\nHereϕ∗is the Legendre transform of ϕand is a convex function as a supremum of linear forms (see\\nalso ( ??)). Since the objective appearing in (1.27) is linear and the integrating measures positive, one can\\nminimize explicitly with respect to ϕand setψ=ϕ∗in order to consider the unconstraint problem\\nmin\\nϕ∫\\nXϕdα+∫\\nYϕ∗dβ, (1.27)\\nsee also Section ??for a generalization of this idea to generic costs c(x,y). By iterating this argument\\ntwice, one can replace ϕbyϕ∗∗, which is a convex function, and thus impose in (1.27) that ϕis convex. Condition (1.23) shows that an optimal πis supported on{(x,y) ;ϕ(x) +ϕ∗(y) =⟨x, y⟩}which shows that\\nsuch anyis optimal for the minimization (1.26) of the Legendre transform, whose optimality condition reads\\ny∈∂ϕ(x). Sinceϕis convex, it is diﬀerentiable almost everywhere, and since αhas a density, it is also\\ndiﬀerentiable α-almost everywhere. This shows that for each x, the associated yis uniquely deﬁned α-almost\\neverywhere as y=∇ϕ(x), and shows that necessarily π= (Id,∇ϕ)♯α. This results shows that in the setting of W2with non-singular densities, the Monge problem (1.9)\\nand its Kantorovich relaxation (1.14) are equal (the relaxation is tight). This is the continuous analog\\nof Proposition 1 for the assignment case (1), which states that the minimum of the optimal transport\\nproblem is achieved, when the marginals are equal and uniform, at a permutation matrix (a discrete map). Brenier’s theorem, stating that an optimal transport map must be the gradient of a convex function, should\\nbe examined under the light that a convex function is the natural generalization of the notion of increasing\\nfunctions in dimension more than one. Optimal transport can thus plays an important role to deﬁne quantile\\nfunctions in arbitrary dimensions, which in turn is useful for applications to quantile regression problems [ ?]. Note also that this theorem can be extended in many directions.'\n",
            "Chunk 27: page_content='The condition that αhas a density can\\nbe weakened to the condition that it does not give mass to “small sets” having Hausdorﬀ dimension smaller\\nthand−1 (e.g. hypersurfaces). One can also consider costs of the form c(x,y) =h(x−y) wherehis a\\nstrictly convex function. For measures with densities, using (1.4), one obtains that ϕis the unique (up to the addition of a\\nconstant) convex function which solves the following Monge-Amp ˜A¨re-type equation\\ndet(∂2ϕ(x))ρβ(∇ϕ(x)) =ρα(x) (1.28)\\nwhere∂2ϕ(x)∈Rd×dis the hessian of ϕ. The Monge-Amp` ere operator det( ∂2ϕ(x)) can be understood as a\\nnon-linear degenerate Laplacian. In the limit of small displacements, ϕ= Id +εϕ, one indeed recovers the\\nLaplacian ∆ as a linearization since for smooth maps\\ndet(∂2ϕ(x)) = 1 +ε∆ϕ(x) +o(ε). The convexity constraint forces det( ∂2ϕ(x))⩾0 and is necessary for this equation to have a solution.'\n",
            "Chunk 28: page_content='Special cases In general, computing OT distances is numerically involved. We review special favorable\\ncases where the resolution of the OT problem is easy. Remark 6 (Binary Cost Matrix and 1-Norm) .One can easily check that when the cost matrix Cis zero on\\nthe diagonal and 1 elsewhere, namely when C=1n×n−In, the OT distance between aandbis equal to\\nthe 1-norm of their diﬀerence, L C(a,b) =||a−b||1. One can also easily check that this result extends to\\ndiscrete and discrete measures in the case where c(x,y) is 0 ifx=yand 1 when x̸=y. The OT distance\\nbetween two discrete measures αandβis equal to their total variation distance. 14'\n",
            "Chunk 29: page_content='\\x00\\x00↵Figure 1.8: 1-D optimal couplings: each arrow xi→yjindicate a non-zero Pi,jin the optimal coupling. Top: empirical measures with same number of points (optimal matching). Bottom: generic case. This\\ncorresponds to monotone rearrangements, if xi⩽xi′are such that Pi,j̸= 0,Pi′,j′̸= 0, then necessarily\\nyj⩽yj′. Remark 7 (1-D case – Empirical measures) .HereX=R. Assuming α=1\\nn∑n\\ni=1δxiandβ=1\\nn∑n\\nj=1δyj,\\nand assuming (without loss of generality) that the points are ordered, i.e.x1⩽x2⩽...⩽xnand\\ny1⩽y2⩽...⩽yn, then one has the simple formula\\nWp(α,β)p=p∑\\ni=1|xi−yi|p, (1.29)\\ni.e.locally (if one assumes distinct points), Wp(α,β) is theℓpnorm between two vectors of ordered values of\\nαandβ. That statement is only valid locally, in the sense that the order (and those vector representations)\\nmight change whenever some of the values change.'\n",
            "Chunk 30: page_content='That formula is a simple consequence of the more general\\nremark given below. Figure 1.8, top row, illustrates the 1-D transportation map between empirical measures\\nwith the same number of points. The bottom row shows how this monotone map generalizes to arbitrary\\ndiscrete measures. It is possible to leverage this 1-D computation to also compute eﬃciently OT on the\\ncircle, see [ ?]. Note that in the case of concave cost of the distance, for instance when p<1, the behaviour\\nof the optimal transport plan is very diﬀerent, see [ ?], which describes an eﬃcient solver in this case. Remark 8 (1-D case – Generic case) .For a measure αonR, we introduce the cumulative function\\n∀x∈R,Cα(x)def.=∫x\\n−∞dα, (1.30)\\nwhich is a function Cα:R→[0,1], and its pseudo-inverse C−1\\nα: [0,1]→R∪{−∞}\\n∀r∈[0,1],C−1\\nα(r) = min\\nx{x∈R∪{−∞} ;Cα(x)⩾r}. That function is also called the generalized quantile function of α. For anyp⩾1, one has\\nWp(α,β)p=||C−1\\nα−C−1\\nβ||p\\nLp([0,1])=∫1\\n0|C−1\\nα(r)−C−1\\nβ(r)|pdr. (1.31)\\nThis means that through the map α↦→C−1\\nα, the Wasserstein distance is isometric to a linear space equipped\\nwith theLpnorm, or, equivalently, that the Wasserstein distance for measures on the real line is a Hilbertian\\nmetric. This makes the geometry of 1-D optimal transport very simple, but also very diﬀerent from its\\ngeometry in higher dimensions, which is not Hilbertian as discussed in Proposition ??and more generally\\nin§??. Forp= 1, one even has the simpler formula\\nW1(α,β) =||Cα−Cβ||L1(R)=∫\\nR|Cα(x)−Cβ(x)|dx (1.32)\\n=∫\\nR⏐⏐⏐⏐∫x\\n−∞d(α−β)⏐⏐⏐⏐dx. (1.33)\\n15'\n",
            "Chunk 31: page_content='µ ν (tT+ (1−t)Id)♯µ\\n0 0.5 10.5Cµ\\nCν\\n0 0.5 100.51\\nCµ-1\\nCν-1\\n0 0.5 100.51\\nT\\nT-1\\n0 0.5 100.51\\n(Cα,Cβ) (C−1\\nα,C−1\\nβ) ( T,T−1) (1−t)C−1\\nα+tC−1\\nβ\\nFigure 1.9: Computation of OT and displacement interpolation between two 1-D measures, using cumulant\\nfunction as detailed in (1.34). which shows that W1is a norm (see§??for the generalization to arbitrary dimensions). An optimal Monge\\nmapTsuch thatT♯α=βis then deﬁned by\\nT=C−1\\nβ◦Cα. (1.34)\\nFigure 1.9 illustrates the computation of 1-D OT through cumulative functions. It also displays displacement\\ninterpolations, computed as detailed in ( ??), see also Remark ??. For a detailed survey of the properties of\\noptimal transport in 1-D, we refer the reader to [ ?, Chapter 2]. Remark 9 (Distance between Gaussians) .Ifα=N(mα,Σα) andβ=N(mβ,Σβ) are two Gaussians in Rd,\\nthen one can show that the following map\\nT:x↦→mβ+A(x−mα), (1.35)\\nwhere\\nA=Σ−1\\n2α(\\nΣ1\\n2αΣβΣ1\\n2α)1\\n2Σ−1\\n2α=AT,\\nis such that T♯ρα=ρβ. Indeed, one simply has to notice that the change of variables formula (1.4) is satisﬁed\\nsince\\nρβ(T(x)) = det(2πΣβ)−1\\n2exp(−⟨T(x)−mβ,Σ−1\\nβ(T(x)−mβ)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα, ATΣ−1\\nβA(x−mα)⟩)\\n= det(2πΣβ)−1\\n2exp(−⟨x−mα,Σ−1\\nα(x−mα)⟩),\\nand sinceTis a linear map we have that\\n|detT′(x)|= detA=(detΣβ\\ndetΣα)1\\n2\\nand we therefore recover ρα=|detT′|ρβmeaningT♯α=β. Notice now that Tis the gradient of the convex\\nfunctionψ:x↦→1\\n2⟨x−mα, A(x−mα)⟩+⟨mβ, x⟩to conclude, using Brenier’s theorem [ ?] (see Remark ??)\\nthatTis optimal. Both that map Tand the corresponding potential ψare illustrated in Figures 1.10 and ??'\n",
            "Chunk 32: page_content='16'\n",
            "Chunk 33: page_content='-4 -2 0 2 4 6-3-2-101234\\nρβραFigure 1.10: Two Gaussians ραandρβ, represented using the contour plots of their densities, with respective\\nmean and variance matrices mα= (−2,0),Σα=1\\n2(\\n1−1\\n2;−1\\n21)\\nandmβ= (3,1),Σβ=(\\n2,1\\n2;1\\n2,1)\\n. The\\narrows originate at random points xtaken on the plane and end at the corresponding mappings of those\\npointsT(x) =mβ+A(x−mα). \\x00m\\nFigure 1.11: Computation of displacement interpolation between two 1-D Gaussians. Denoting Gm,σ(x)def.=\\n1√\\n2πse−(x−m)2\\n2s2the Gaussian density, it thus shows the interpolation G(1−t)m0+tm1,(1−t)σ0+tσ1. With additional calculations involving ﬁrst and second order moments of ρα, we obtain that the transport\\ncost of that map is\\nW2\\n2(α,β) =||mα−mβ||2+B(Σα,Σβ)2(1.36)\\nwhereBis the so-called Bures’ metric [ ?] between positive deﬁnite matrices (see also [ ?,?]),\\nB(Σα,Σβ)2def.= tr(\\nΣα+Σβ−2(Σ1/2\\nαΣβΣ1/2\\nα)1/2)\\n, (1.37)\\nwhere Σ1/2is the matrix square root. One can show that Bis a distance on covariance matrices, and that\\nB2is convex with respect to both its arguments. In the case where Σα= diag(ri)iandΣβ= diag(si)iare\\ndiagonals, the Bures metric is the Hellinger distance\\nB(Σα,Σβ) =||√r−√s||2. For 1-D Gaussians, W2is thus the Euclidean distance on the 2-D plane ( m,√\\nΣ), as illustrated in Figure 1.11. For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to [ ?]. 1.5 Sinkhorn\\nThis section introduces a family of numerical scheme to approximate solutions to Kantorovich formulation\\nof optimal transport and its many generalizations. It operates by adding an entropic regularization penalty to\\nthe original problem.'\n",
            "Chunk 34: page_content='This regularization has several important advantages, but a few stand out particularly:\\nThe minimization of the regularized problen can be solved using a simple alternate minimization scheme;\\nthat scheme translates into iterations that are simple matrix products, making them particularly suited to\\nexecution of GPU; the resulting approximate distance is smooth with respect to input histogram weights\\nand positions of the Diracs. 17'\n",
            "Chunk 35: page_content='c\"P\"Figure 1.12: Impact of εon the optimization of a linear function on the simplex, solving Pε=\\nargminP∈Σ3⟨C,P⟩−εH(P) for a varying ε. Entropic Regularization. The discrete entropy of a coupling matrix is deﬁned as\\nH(P)def.=−∑\\ni,jPi,j(log(Pi,j)−1), (1.38)\\nwith an analogous deﬁnition for vectors, with the convention that H(a) =−∞ if one of the entries ajis\\n0 or negative. The function His 1-strongly concave, because its hessian is ∂2H(P) =−diag(1/Pi,j) and\\nPi,j⩽1. The idea of the entropic regularization of optimal transport is to use −Has a regularizing function\\nto obtain approximate solutions to the original transport problem (1.11):\\nLε\\nC(a,b)def.= min\\nP∈U(a,b)⟨P,C⟩−εH(P). (1.39)\\nSince the objective is a ε-strongly convex function, problem 1.39 has a unique optimal solution. The idea\\nto regularize the optimal transport problem by an entropic term can be traced back to modeling ideas in\\ntransportation theory [ ?]: Actual traﬃc patterns in a network do not agree with those predicted by the\\nsolution of the optimal transport problem. Indeed, the former are more diﬀuse than the latter, which tend\\nto rely on a few routes as a result of the sparsity of optimal couplings to the solution of 1.11. To balance for\\nthat, researchers in transportation proposed a model, called the “gravity” model [ ?], that is able to form a\\nmore “blurred” traﬃc prediction. Figure 1.12 illustrates the eﬀect of the entropy to regularize a linear program over the simples Σ 3(which\\ncan thus be visualized as a triangle in 2-D).'\n",
            "Chunk 36: page_content='Note how the entropy pushes the original LP solution away\\nfrom the boundary of the triangle. The optimal Pεprogressively moves toward an “entropic center” of the\\ntriangle. This is further detailed in the proposition below. The convergence of the solution of that regularized\\nproblem towards an optimal solution of the original linear program has been studied by [ ?]. Proposition 6 (Convergence with ε).The unique solution Pεof(1.39) converges to the optimal solution\\nwith maximal entropy within the set of all optimal solutions of the Kantorovich problem, namely\\nPεε→0−→argmin\\nP{−H(P) ;P∈U(a,b),⟨P,C⟩= LC(a,b)} (1.40)\\nso that in particular\\nLε\\nC(a,b)ε→0−→LC(a,b). One has\\nPεε→∞−→abT= (aibj)i,j.'\n",
            "Chunk 37: page_content='(1.41)\\nProof. We consider a sequence ( εℓ)ℓsuch thatεℓ→0 andεℓ>0. We denote Pℓthe solution of (1.39) for\\nε=εℓ. Since U(a,b) is bounded, we can extract a sequence (that we do not relabel for sake of simplicity)\\nsuch that Pℓ→P⋆. Since U(a,b) is closed, P⋆∈U(a,b). We consider any Psuch that⟨C,P⟩= LC(a,b). By optimality of PandPℓfor their respective optimization problems (for ε= 0 andε=εℓ), one has\\n0⩽⟨C,Pℓ⟩−⟨C,P⟩⩽εℓ(H(Pℓ)−H(P)). (1.42)\\n18'\n",
            "Chunk 38: page_content='⇡\"↵\\x00\\n\"\\x00↵Figure 1.13: Impact of εon coupling between densities and discrete distributions, illustrating Proposition 6. Left: between two 1-D densities. Right: between two 2-D discrete empirical densities with same number\\nn=mof points (only entries of the optimal ( Pi,j)i,jabove a small threshold are displayed as segments\\nbetweenxiandyj). Since His continuous, taking the limit ℓ→+∞in this expression shows that ⟨C,P⋆⟩=⟨C,P⟩so that\\nP⋆is a feasible point of (1.40). Furthermore, dividing by εℓin (1.42) and taking the limit shows that\\nH(P)⩽H(P⋆), which shows that P⋆is a solution of (1.40). Since the solution P⋆\\n0to this program is unique\\nby strict convexity of −H, one has P⋆=P⋆\\n0, and the whole sequence is converging. Formula (1.40) states that for low regularization, the solution converges to the maximum entropy optimal\\ntransport coupling. In sharp contrast, (1.41) shows that for large regularization, the solution converges to the\\ncoupling with maximal entropy between two prescribed marginals a,b, namely the joint probability between\\ntwo independent random variables with prescribed distributions. A reﬁned analysis of this convergence is\\nperformed in [ ?], including a ﬁrst order expansion in ε(resp.'\n",
            "Chunk 39: page_content='1/ε) nearε= 0 (respε= +∞). Figure 1.13\\nshows visually the eﬀect of these two convergence. A key insight is that, as εincreases, the optimal coupling\\nbecomes less and less sparse (in the sense of having entries larger than a prescribed thresholds), which in\\nturn as the eﬀect of both accelerating computational algorithms (as we study in §1.5) but also leading to\\nfaster statistical convergence (as exposed in §??). Deﬁning the Kullback-Leibler divergence between couplings as\\nKL(P|K)def.=∑\\ni,jPi,jlog(Pi,j\\nKi,j)\\n−Pi,j+Ki,j, (1.43)\\nthe unique solution Pεof (1.39) is a projection onto U(a,b) of the Gibbs kernel associated to the cost matrix\\nCas\\nKi,jdef.=e−Ci,j\\nε\\nIndeed one has that using the deﬁnition above\\nPε= ProjKL\\nU(a,b)(K)def.= argmin\\nP∈U(a,b)KL(P|K). (1.44)\\nRemark 10 (General formulation) .One can consider arbitrary measures by replacing the discrete entropy\\nby the relative entropy with respect to the product measure d α⊗dβ(x,y)def.= dα(x)dβ(y), and propose a\\nregularized counterpart to (1.14) using\\nLε\\nc(α,β)def.= min\\nπ∈U(α,β)∫\\nX×Yc(x,y)dπ(x,y) +εKL(π|α⊗β) (1.45)\\nwhere the relative entropy is a generalization of the discrete Kullback-Leibler divergence (1.43)\\nKL(π|ξ)def.=∫\\nX×Ylog(dπ\\ndξ(x,y))\\ndπ(x,y)+\\n∫\\nX×Y(dξ(x,y)−dπ(x,y)),(1.46)\\n19'\n",
            "Chunk 40: page_content='and by convention KL( π|ξ) = +∞ifπdoes not have a densitydπ\\ndξwith respect to ξ. It is important to\\nrealize that the reference measure α⊗βchosen in (1.45) to deﬁne the entropic regularizing term KL( ·|α⊗β)\\nplays no speciﬁc role, only its support matters. Formula (1.45) can be re-factored as a projection problem\\nmin\\nπ∈U(α,β)KL(π|K) (1.47)\\nwhereKis the Gibbs distributions d K(x,y)def.=e−c(x,y)\\nεdµ(x)dν(y). This problem is often referred to as the\\n“static Schr¨ odinger problem” [ ?,?], since it was initially considered by Schr¨ odinger in statistical physics [ ?]. Asε→0, the unique solution to (1.47) converges to the maximum entropy solution to (1.14), see [ ?,?].§?? details an alternate “dynamic” formulation of the Schr¨ odinger problem over the space of paths connecting\\nthe points of two measures. Sinkhorn’s Algorithm The following proposition shows that the solution of (1.39) has a speciﬁc form,\\nwhich can be parameterized using n+mvariables. That parameterization is therefore essentially dual, in\\nthe sense that a coupling PinU(a,b) hasnmvariables but n+mconstraints. Proposition 7. The solution to (1.39) is unique and has the form\\n∀(i,j)∈JnK×JmK,Pi,j=uiKi,jvj (1.48)\\nfor two (unknown) scaling variable (u,v)∈Rn\\n+×Rm\\n+. Proof. Introducing two dual variables f∈Rn,g∈Rmfor each marginal constraint, the Lagrangian of (1.39)\\nreads\\nE(P,f,g) =⟨P,C⟩−εH(P)−⟨f,P1m−a⟩−⟨g,PT1n−b⟩. Considering ﬁrst order conditions, we have\\n∂E(P,f,g)\\n∂Pi,j=Ci,j−εlog(Pi,j)−fi−gj. which results, for an optimal Pcoupling to the regularized problem, in the expression Pi,j=efi/εe−Ci,j/εegj/ε\\nwhich can be rewritten in the form provided in the proposition using non-negative vectors uandv. The factorization of the optimal solution exhibited in Equation (1.48) can be conveniently rewritten in\\nmatrix form as P= diag( u)Kdiag(v).u,vmust therefore satisfy the following non-linear equations which\\ncorrespond to the mass conservation constraints inherent to U(a,b),\\ndiag(u)Kdiag(v)1m=a,and diag( v)K⊤diag(u)1n=b, (1.49)\\nThese two equations can be further simpliﬁed, since diag( v)1mis simply v, and the multiplication of diag( u)\\ntimes Kvis\\nu⊙(Kv) =aand v⊙(KTu) =b (1.50)\\nwhere⊙corresponds to entry-wise multiplication of vectors. That problem is known in the numerical analysis\\ncommunity as the matrix scaling problem (see [ ?] and references therein). An intuitive way to try to solve\\nthese equations is to solve them iteratively, by modifying ﬁrst uso that it satisﬁes the left-hand side of\\nEquation (1.50) and then vto satisfy its right-hand side. These two updates deﬁne Sinkhorn’s algorithm:\\nu(ℓ+1)def.=a\\nKv(ℓ)and v(ℓ+1)def.=b\\nKTu(ℓ+1), (1.51)\\ninitialized with an arbitrary positive vector v(0)=1m. The division operator used above between two\\nvectors is to be understood entry-wise.'\n",
            "Chunk 41: page_content='Note that a diﬀerent initialization will likely lead to a diﬀerent\\n20'\n",
            "Chunk 42: page_content='`⇡(`)\"\\n1000 2000 3000 4000 5000-2-1.5-1-0.50`Figure 1.14: Left: evolution of the coupling πℓ\\nε= diag( U(ℓ))Kdiag(V(ℓ)) computed at iteration ℓof\\nSinkhorn’s iterations, for 1-D densities. Right: impact of εthe convergence rate of Sinkhorn, as measured\\nin term of marginal constraint violation log( ||πℓ\\nε1m−b||1). solution for u,v, since u,vare only deﬁned up to a multiplicative constant (if u,vsatisfy (1.49) then\\nso doλu,v/λfor anyλ > 0). It turns out however that these iterations converge (see Remark 11 for\\na justiﬁcation using iterative projections, and Remark 13 for a strict contraction result) and all result in\\nthe same optimal coupling diag( u)Kdiag(v). Figure 1.14, top row, shows the evolution of the coupling\\ndiag(U(ℓ))Kdiag(V(ℓ)) computed by Sinkhorn iterations. It evolves from the Gibbs kernel Ktowards the\\noptimal coupling solving (1.39) by progressively shifting the mass away from the diagonal. Remark 11 (Relation with iterative projections) .Denoting\\nC1\\nadef.={P;P1m=a}andC2\\nbdef.={\\nP;PT1m=b}\\nthe rows and columns constraints, one has U(a,b) =C1\\na∩C2\\nb. One can use Bregman iterative projections [ ?]\\nP(ℓ+1) def.= ProjKL\\nC1a(P(ℓ)) and P(ℓ+2) def.= ProjKL\\nC2\\nb(P(ℓ+1)). (1.52)\\nSince the setsC1\\naandC2\\nbare aﬃne, these iterations are known to converge to the solution of (1.44), see [ ?]. These iterate are equivalent to Sinkhorn iterations (1.51) since deﬁning\\nP(2ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)),\\none has\\nP(2ℓ+1) def.= diag( u(ℓ+1))Kdiag(v(ℓ))\\nand P(2ℓ+2) def.= diag( u(ℓ+1))Kdiag(v(ℓ+1))\\nIn practice however one should prefer using (1.51) which only requires manipulating scaling vectors and\\nmultiplication against a Gibbs kernel, which can often be accelerated (see below Remarks ??and??). Remark 12 (Hilbert metric) .As initially explained by [ ?], the global convergence analysis of Sinkhorn is\\ngreatly simpliﬁed using Hilbert projective metric on Rn\\n+,∗(positive vectors), deﬁned as\\n∀(u,u′)∈(Rn\\n+,∗)2, dH(u,u′)def.= log max\\ni,i′uiu′\\ni′\\nui′u′\\ni. This can be shows to be a distance on the projective cone Rn\\n+,∗/∼, where u∼u′means that∃s>0,u=su′\\n(the vector are equal up to rescaling, hence the naming “projective”). This means that dHsatisﬁes the\\ntriangular inequality and dH(u,u′) = 0 if and only if u∼u′. This is a projective version of Hilbert’s original\\ndistance on bounded open convex sets [ ?].'\n",
            "Chunk 43: page_content='The projective cone Rn\\n+,∗/∼is a complete metric space for this\\ndistance. It was introduced independently by [ ?] and [ ?] to provide a quantitative proof of Perron-Frobenius\\ntheorem, which, as explained in Remark ??is linked to a local linearization of Sinkhorn’s iterates. They\\nproved the following fundamental theorem, which shows that a positive matrix is a strict contraction on the\\ncone of positive vectors. 21'\n",
            "Chunk 44: page_content='Theorem 2. Let K∈Rn×m\\n+,∗, then for (v,v′)∈(Rm\\n+,∗)2\\ndH(Kv,Kv′)⩽λ(K)dH(v,v′)where\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3λ(K)def.=√\\nη(K)−1√\\nη(K)+1<1\\nη(K)def.= max\\ni,j,k,ℓKi,kKj,ℓ\\nKj,kKi,ℓ. Remark 13 (Global convergence) .The following theorem, proved by [ ?], makes use of this Theorem 2 to\\nshow the linear convergence of Sinkhorn’s iterations. Theorem 3. One has (u(ℓ),v(ℓ))→(u⋆,v⋆)and\\ndH(u(ℓ),u⋆) =O(λ(K)2ℓ), dH(v(ℓ),v⋆) =O(λ(K)2ℓ). (1.53)\\nOne also has\\ndH(u(ℓ),u⋆)⩽dH(P(ℓ)1m,a)\\n1−λ(K)\\ndH(v(ℓ),v⋆)⩽dH(P(ℓ),⊤1n,b)\\n1−λ(K)(1.54)\\nwhere we denoted P(ℓ)def.= diag( u(ℓ))Kdiag(v(ℓ)). Lastly, one has\\n∥log(P(ℓ))−log(P⋆)∥∞⩽dH(u(ℓ),u⋆) +dH(v(ℓ),v⋆) (1.55)\\nwhere P⋆is the unique solution of (1.39) . Proof. One notice that for any ( v,v′)∈(Rm\\n+,∗)2, one has\\ndH(v,v′) =dH(v/v′,1m) =dH(1m/v,1m/v′). This shows that\\ndH(u(ℓ+1),u⋆) =dH(a\\nKv(ℓ),a\\nKv⋆)\\n=dH(Kv(ℓ),Kv⋆)⩽λ(K)dH(v(ℓ),v⋆). where we used Theorem 2. This shows (1.53). One also has, using the triangular inequality\\ndH(u(ℓ),u⋆)⩽dH(u(ℓ+1),u(ℓ)) +dH(u(ℓ+1),u⋆)\\n⩽dH(a\\nKv(ℓ),u(ℓ))\\n+λ(K)dH(u(ℓ),u⋆)\\n=dH(\\na,u(ℓ)⊙(Kv(ℓ)))\\n+λ(K)dH(u(ℓ),u⋆),\\nwhich gives the ﬁrst part of (1.54) since u(ℓ)⊙(Kv(ℓ)) =P(ℓ)1m(the second one being similar). The proof\\nof (1.55) follows from [ ?, Lemma 3]\\nThe bound (1.54) shows that some error measures on the marginal constraints violation, for instance\\n∥P(ℓ)1m−a∥1and∥P(ℓ)T1n−b∥1, are useful stopping criteria to monitor the convergence.'\n",
            "Chunk 45: page_content='Figure 1.14, bottom row, highlights this linear rate on the constraint violation, and shows how this rate\\ndegrades as ε→0. These results are proved in [ ?] and are tightly connected to nonlinear Perron-Frobenius\\nTheory [ ?]. Perron-Frobenius theory corresponds to the linearization of the iterations, see ( ??). This\\nconvergence analysis is extended in [ ?], who shows that each iteration of Sinkhorn increases the permanent\\nof the scaled coupling matrix. 22'\n",
            "Chunk 46: page_content='Regularized Dual and Log-domain Computations The following proposition details the dual problem\\nassociated to (1.39). Proposition 8.'\n",
            "Chunk 47: page_content='One has\\nLε\\nC(a,b) = max\\nf∈Rn,g∈Rm⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩. (1.56)\\nThe optimal (f,g)are linked to scalings (u,v)appearing in (1.48) through\\n(u,v) = (ef/ε,eg/ε). (1.57)\\nProof. We start from the end of the proof of Proposition 7, which links the optimal primal solution P\\nand dual multipliers fandgfor the marginal constraints as Pi,j=efi/εe−Ci,j/εegj/ε. Substituting in the\\nLagrangianE(P,f,g) of Equation (1.5) the optimal Pas a function of fandg, we obtain that the Lagrange\\ndual function equals\\nf,g↦→⟨ef/ε,(K⊙C)eg/ε⟩−εH(diag(ef/ε)Kdiag(eg/ε)). (1.58)\\nThe entropy of Pscaled byε, namelyε⟨P,logP−1n×m⟩can be stated explicitly as a function of f,g,C\\n⟨diag(ef/ε)Kdiag(eg/ε),f1mT+1ngT−C−ε1n×m⟩\\n=−⟨ef/ε,(K⊙C)eg/ε⟩+⟨f,a⟩+⟨g,b⟩−ε⟨ef/ε,Keg/ε⟩\\ntherefore, the ﬁrst term in (1.58) cancels out with the ﬁrst term in the entropy above. The remaining times\\nare those displayed in (1.56). Remark 14.Dual for generic measures For generic (non-necessarily discrete) input measures ( α,β), the dual\\nproblem (1.56) reads\\nsup\\nf,g∈C(X)×C(Y)∫\\nXf(x)dα(x) +∫\\nYg(x)dβ(x)−ε∫\\nX×Ye−c(x,y)+f(x)+g(y)\\nε dα(x)dβ(y)\\nThis corresponds to a smoothing of the constraint R(c) appearing in the original problem (1.21), which\\nis retrieved in the limit ε→0. Proving existence ( i.e. the sup is actually a max) of these Kantorovich\\npotentials ( f,g) in the case of entropic transport is less easy than for classical OT (because one cannot\\nusec-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the\\nconvergence of Sinkhorn iterations, see [ ?] for more details. Remark 15 (Sinkhorn as a Block Coordinate Ascent on the Dual Problem) .A simple approach to solve the\\nunconstrained maximization problem (1.56) is to use an exact block coordinate ascent strategy, namely to\\nupdate alternatively fandgto cancel their gradients with respect to the objective of (1.56). Indeed, one\\ncan easily notice that, writing Q(f,g) for the objective of (1.56) that\\n∇|fQ(f,g) =a−ef/ε⊙(\\nKeg/ε)\\n, (1.59)\\n∇|gQ(f,g) =b−eg/ε⊙(\\nKTef/ε)\\n. (1.60)\\nBlock coordinate ascent can therefore be implemented in a closed form by applying successively the following\\nupdates, starting from any arbitrary g(0), forl⩾0,\\nf(ℓ+1)=εloga−εlog(\\nKeg(ℓ)/ε)\\n, (1.61)\\ng(ℓ+1)=εlogb−εlog(\\nKTef(ℓ+1)/ε)\\n. (1.62)\\nSuch iterations are mathematically equivalent to the Sinkhorn iterations (1.51) when considering the primal-\\ndual relations highlighted in (1.57). Indeed, we recover that at any iteration\\n(f(ℓ),g(ℓ)) =ε(log(u(ℓ)),log(v(ℓ))). 23'\n",
            "Chunk 48: page_content='Remark 16 (Soft-min rewriting) .Iterations (1.61) and (1.62) can be given an alternative interpretation,\\nusing the following notation. Given a vector zof real numbers we write min εzfor the soft-minimum of its\\ncoordinates, namely\\nminεz=−εlog∑\\nie−zi/ε. Note that min ε(z) converges to min zfor any vector zasε→0. Indeed, min εcan be interpreted as a\\ndiﬀerentiable approximation of the min function. Using these notations, Equations (1.61) and (1.62) can be\\nrewritten\\n(f(ℓ+1))i= minε(Cij−g(ℓ)\\nj)j+εlogai, (1.63)\\n(g(ℓ+1))j= minε(Cij−f(ℓ)\\ni)i+εlogbj. (1.64)\\nHere the term min ε(Cij−g(ℓ)\\nj)jdenotes the soft-minimum of all values of the j-th column of matrix\\n(C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that takes a matrix as input and outputs\\nnow a column vector of the soft-minimum values of its columns or rows. Namely, for any matrix A∈Rn×m,\\nwe deﬁne\\nMinrow\\nε(A)def.=(\\nminε(Ai,j)j)\\ni∈Rn,\\nMincol\\nε(A)def.=(\\nminε(Ai,j)i)\\nj∈Rm. Note that these operations are equivalent to the entropic c-transform introduced in §??(see in particu-\\nlar (??)). Using these notations, Sinkhorn’s iterates read\\nf(ℓ+1)= Minrow\\nε(C−1ng(ℓ)T) +εloga, (1.65)\\ng(ℓ+1)= Mincol\\nε(C−f(ℓ)1mT) +εlogb. (1.66)\\nNote that as ε→0, minεconverges to min, but the iterations do not converge anymore in the limit ε= 0,\\nbecause alternate minimization does not converge for constrained problems (which is the case for the un-\\nregularized dual (1.17)). Remark 17 (Log-domain Sinkhorn) .While mathematically equivalent to the Sinkhorn updates (1.51), itera-\\ntions (1.63) and (1.64) suggest to use the log-sum-exp stabilization trick to avoid underﬂow for small values\\nofε. Writing z = min z, that trick suggests to evaluate min εzas\\nminεz= z−εlog∑\\nie−(zi−z)/ε. (1.67)\\nInstead of substracting z to stabilize the log domain iterations as in (1.67), one can actually substract the\\npreviously computed scalings. This leads to the following stabilized iteration\\nf(ℓ+1)= Minrow\\nε(S(f(ℓ),g(ℓ)))−f(ℓ)+εlog(a) (1.68)\\ng(ℓ+1)= Mincol\\nε(S(f(ℓ+1),g(ℓ)))−g(ℓ)+εlog(b), (1.69)\\nwhere we deﬁned\\nS(f,g) =(\\nCi,j−fi−gj)\\ni,j. In contrast to the original iterations (1.51), these log-domain iterations (1.68) and (1.69) are stable for\\narbitraryε >0, because the quantity S(f,g) stays bounded during the iterations. The downside is that it\\nrequiresnmcomputations of exp at each step. Computing a Minrow\\nεor Mincol\\nεis typically substantially\\nslower than matrix multiplications, and requires computing line by line soft-minima of matrices S. There is\\ntherefore no eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously. In Euclidean domain of small dimension, it is possible to develop eﬃcient multiscale solvers with a decaying\\nεstrategy to signiﬁcantly speed up the computation using sparse grids [ ?].'\n",
            "Chunk 49: page_content='24'\n",
            "Chunk 50: page_content='1.6 Extensions\\nWasserstein Barycenters. Given input histogram {bs}S\\ns=1, wherebs∈Σns, and weights λ∈ΣS, a\\nWasserstein barycenter is computed by minimizing\\nmin\\na∈ΣnS∑\\ns=1λsLCs(a,bs) (1.70)\\nwhere the cost matrices Cs∈Rn×nsneed to be speciﬁed. A typical setup is “Eulerian”, so that all the\\nbarycenters are deﬁned on the same grid, ns=n,Cs=C=Dpis set to be a distance matrix, so that one\\nsolves\\nmin\\na∈ΣnS∑\\ns=1λsWp\\np(a,bs). This barycenter problem (1.70) was originally introduced by [ ?] following earlier ideas of [ ?]. They proved\\nin particular uniqueness of the barycenter for c(x,y) =||x−y||2overX=Rd, if one of the input measure\\nhas a density with respect to the Lebesgue measure (and more generally under the same hypothesis as the\\none guaranteeing the existence of a Monge map, see Remark ??). The barycenter problem for histograms (1.70) is in fact a linear program, since one can look for the S\\ncouplings ( Ps)sbetween each input and the barycenter itself\\nmin\\na∈Σn,(Ps∈Rn×ns)s{S∑\\ns=1λs⟨Ps,Cs⟩;∀s,P⊤\\ns1ns=a,P⊤\\ns1n=bs}\\n. Although this problem is an LP, its scale forbids the use generic solvers for medium scale problems. One\\ncan therefore resort to using ﬁrst order methods such as subgradient descent on the dual [ ?]. Remark 18.Barycenter of arbitrary measures Given a set of input measure ( βs)sdeﬁned on some space X,\\nthe barycenter problem becomes\\nmin\\nα∈M1\\n+(X)S∑\\ns=1λsLc(α,βs). (1.71)\\nIn the case where X=Rdandc(x,y) =||x−y||2, [?] shows that if one of the input measures has a density,\\nthen this barycenter is unique. Problem (1.71) can be viewed as a generalization of the problem of computing\\nbarycenters of points ( xs)S\\ns=1∈XSto arbitrary measures. Indeed, if βs=δxsis a single Dirac mass, then a\\nsolution to (1.71) is δx⋆wherex⋆is a Fr´ echet mean solving ( ??). Note that for c(x,y) =||x−y||2, the mean\\nof the barycenter α⋆is necessarily the barycenter of the mean, i.e. ∫\\nXxdα⋆(x) =∑\\nsλs∫\\nXxdαs(x),\\nand the support of α⋆is located in the convex hull of the supports of the ( αs)s.'\n",
            "Chunk 51: page_content='The consistency of the\\napproximation of the inﬁnite dimensional optimization (1.71) when approximating the input distribution\\nusing discrete ones (and thus solving (1.70) in place) is studied in [ ?]. Let us also note that it is possible to\\nre-cast (1.71) as a multi-marginal OT problem, see Remark ??. One can use entropic smoothing and approximate the solution of (1.70) using\\nmin\\na∈ΣnS∑\\ns=1λsLε\\nCs(a,bs) (1.72)\\nfor someε > 0. This is a smooth convex minimization problem, which can be tackled using gradient\\ndescent [ ?]. An alternative is to use descent method (typically quasi-Newton) on the semi-dual [ ?], which is\\n25'\n",
            "Chunk 52: page_content='useful to integrate additional regularizations on the barycenter (e.g. to impose some smoothness). A simple\\nbut eﬀective approach, as remarked in [ ?] is to rewrite (1.72) as a (weighted) KL projection problem\\nmin\\n(Ps)s{∑\\nsλsKL(Ps|Ks) ;∀s,PsT1m=bs,P111=...=PS1S}\\n(1.73)\\nwhere we denoted Ksdef.=e−Cs/ε. Here, the barycenter ais implicitly encoded in the row marginals of all\\nthe couplings Ps∈Rn×nsasa=P111=...=PS1S. As detailed in [ ?], one can generalize Sinkhorn to\\nthis problem, which also corresponds to iterative projection.'\n",
            "Chunk 53: page_content='This can also be seen as a special case of the\\ngeneralized Sinkhorn detailed in §??. The optimal couplings ( Ps)ssolving (1.73) are computed in scaling\\nform as\\nPs= diag( us)Kdiag(vs), (1.74)\\nand the scalings are sequentially updated as\\n∀s∈J1,SK,v(ℓ+1)\\nsdef.=bs\\nKT\\nsu(ℓ)\\ns, (1.75)\\n∀s∈J1,SK,u(ℓ+1)\\nsdef.=a(ℓ+1)\\nKsv(ℓ+1)\\ns, (1.76)\\nwhere a(ℓ+1)def.=∏\\ns(Ksv(ℓ+1)\\ns)λs. (1.77)\\nAn alternative way to derive these iterations is to perform alternate minimization on the variables of a dual\\nproblem, which detailed in the following proposition. Proposition 9. The optimal (us,vs)appearing in (1.74) can be written as (us,vs) = (efs/ε,egs/ε)where\\n(fs,gs)sare the solutions of the following program (whose value matches the one of (1.72) )\\nmax\\n(fs,gs)s{∑\\nsλs(\\n⟨gs,bs⟩−ε⟨Ksegs/ε, efs/ε⟩)\\n;∑\\nsλsfs= 0}\\n. (1.78)\\nProof. Introducing Lagrange multipliers in (1.73) leads to\\nmin\\n(Ps)s,amax\\n(fs,gs)s∑\\nsλs(\\nεKL(Ps|Ks) +⟨a−Ps1m,fs⟩\\n+⟨bs−PsT1m,gs⟩)\\n. Strong duality holds, so that one can exchange the min and the max, and gets\\nmax\\n(fs,gs)s∑\\nsλs(\\n⟨gs,bs⟩+ min\\nPsεKL(Ps|Ks)−⟨Ps,fs⊕gs⟩)\\n+ min\\na⟨∑\\nsλsfs,a⟩. The explicit minimization on agives the constraint∑\\nsλsfs= 0 together with\\nmax\\n(fs,gs)s∑\\nsλs⟨gs,bs⟩−εKL∗(fs⊕gs\\nε|Ks)\\nwhere KL∗(·|Ks) is the Legendre transform ( ??) of the function KL∗(·|Ks). This Legendre transform reads\\nKL∗(U|K) =∑\\ni,jKi,j(eUi,j−1), (1.79)\\n26'\n",
            "Chunk 54: page_content='Figure 1.15: Barycenters between 4 input 3-D shapes using entropic regularization (1.72). The weights\\n(λs)sare bilinear with respect to the four corners of the square. Shapes are represented as measures that\\nare uniform within the boundaries of the shape and null outside.'\n",
            "Chunk 55: page_content='which shows the desired formula. To show (1.79), since this function is separable, one needs to compute\\n∀(u,k)∈R2\\n+,KL∗(u|k)def.= max\\nrur−(rlog(r/k)−r+k)\\nwhose optimality condition reads u= log(r/k), i.e.r=keu, hence the result. Minimizing (1.78) with respect to each gs, while keeping all the other variable ﬁxed, is obtained in closed\\nform by (1.75). Minimizing (1.78) with respect to all the ( fs)srequires to solve for ausing (1.77) and leads\\nto the expression (1.76). Figures ??and??show applications to 2-D and 3-D shapes interpolation. Figure ??shows a computation\\nof barycenters on a surface, where the ground cost is the square of the geodesic distance. For this ﬁgure,\\nthe computations are performed using the geodesic in heat approximation detailed in Remark ??. We refer\\nto [?] for more details and other applications to computer graphics and imaging sciences. Wasserstein Loss. In statistics, text processing or imaging, one must usually compare a probability\\ndistribution βarising from measurements to a model, namely a parameterized family of distributions {αθ,θ∈\\nΘ}where Θ is a subset of an Euclidean space. Such a comparison is done through a “loss” or a “ﬁdelity”\\nterm, which, in this section, is the Wasserstein distance. In the simplest scenario, the computation of a\\nsuitable parameter θis obtained by minimizing directly\\nmin\\nθ∈ΘE(θ)def.=Lc(αθ,β). (1.80)\\nOf course, one can consider more complicated problems: for instance, the barycenter problem described\\nin§??consists in a sum of such terms. However, most of these more advanced problems can be usually\\nsolved by adapting tools deﬁned for basic case: either using the chain rule to compute explicitly derivatives,\\nor using automatic diﬀerentiation. The Wasserstein distance between two histograms or two densities is convex with respect to these inputs,\\nas shown by (1.17) and (1.21) respectively. Therefore, when the parameter θis itself a histogram, namely Θ =\\nΣnandαθ=θ, or more generally when θdescribesKweights in the simplex, Θ = Σ K, andαθ=∑K\\ni=1θiαi\\nis a convex combination of known atoms α1,...,αKin ΣN, Problem (1.80) remains convex (the ﬁrst case\\ncorresponds to the barycenter problem, the second to one iteration of the dictionary learning problem with\\na Wasserstein loss [ ?]). However, for more general parameterizations θ↦→αθ, Problem (1.80) is in general\\nnot convex. 27'\n",
            "Chunk 56: page_content='g✓XZ⇣xz\\x00↵✓Figure 1.16: Schematic display of the density ﬁtting problem 1.81. A practical problem of paramount importance in statistic and machine learning is density ﬁtting. Given\\nsome discrete samples ( xi)n\\ni=1⊂X from some unknown distribution, the goal is to ﬁt a parametric model\\nθ↦→αθ∈M (X) to the observed empirical input measure β\\nmin\\nθ∈ΘL(αθ,β) where β=1\\nn∑\\niδxi, (1.81)\\nwhereLis some “loss” function between a discrete and a “continuous” (arbitrary) distribution (see Fig-\\nure 1.16). In the case where αθas a densify ρθdef.=ραθwith respect to the Lebesgue measure (or any other ﬁxed\\nreference measure), the maximum likelihood estimator (MLE) is obtained by solving\\nmin\\nθLMLE(αθ,β)def.=−∑\\nilog(ρθ(xi)). This corresponds to using an empirical counterpart of a Kullback-Leibler loss since, assuming the xiare i.i.d. samples of some ¯β, then\\nLMLE(α,β)n→+∞−→ KL(α|¯β)\\nThis MLE approach is known to lead to optimal estimation procedures in many cases (see for instance [ ?]). However, it fails to work when estimating singular distributions, typically when the αθdoes not has a density\\n(so thatLMLE(αθ,β) = +∞) or when ( xi)iare samples from some singular ¯β(so that the αθshould share\\nthe same support as βfor KL(α|¯β) to be ﬁnite, but this support is usually unknown). Another issue is that\\nin several cases of practical interest, the density ρθis inaccessible (or too hard to compute). A typical setup where both problems (singular and unknown densities) occur is for so-called generative\\nmodels, where the parametric measure is written as a push-forward of a ﬁxed reference measure ζ∈M (Z)\\nαθ=hθ,♯ζwherehθ:Z→X\\nwhere the push-forward operator is introduced in Deﬁnition 1. The space Zis usually low-dimensional, so\\nthat the support of αθis localized along a low-dimensional “manifold” and the resulting density is highly\\nsingular (it does not have a density with respect to Lebesgue measure). Furthermore, computing this density\\nis usually intractable, while generating i.i.d. samples from αθis achieved by computing xi=hθ(zi) where\\n(zi)iare i.i.d. samples from ζ. In order to cope with such diﬃcult scenario, one has to use weak metrics in place of the MLE functional\\nLMLE, which needs to be written in dual form as\\nL(α,β)def.= max\\n(f,g)∈C(X)2{∫\\nXf(x)dα(x) +∫\\nXg(x)dβ(x) ; (f,g)∈R}\\n. (1.82)\\nDual norms exposed in §??correspond to imposing R={(f,−f) ;f∈B}, while optimal transport (1.21)\\nsetsR=R(c) as deﬁned in (1.22).'\n",
            "Chunk 57: page_content='28'\n",
            "Chunk 58: page_content='For a ﬁxed θ, evaluating the energy to be minimized in (1.81) using such a loss function corresponds to\\nsolving a semi-discrete optimal transport, which is the focus of Chapter ??. Minimizing the energy with\\nrespect toθis much more involved, and is typically highly non-convex. The class of estimators obtained using L=Lc, often called “Minimum Kantorovitch Estimators” (MKE),\\nwas initially introduced in [ ?], see also [ ?].'\n",
            "Chunk 59: page_content='Gromov-Wasserstein. Optimal transport needs a ground cost Cto compare histograms ( a,b), it can\\nthus not be used if the histograms are not deﬁned on the same underlying space, or if one cannot pre-register\\nthese spaces to deﬁne a ground cost. To address this issue, one can instead only assume a weaker assumption,\\nnamely that one has at its disposal two matrices D∈Rn×nandD′∈Rm×mthat represent some relationship\\nbetween the points on which the histograms are deﬁned.'\n",
            "Chunk 60: page_content='A typical scenario is when these matrices are (power\\nof) distance matrices. The Gromov-Wasserstein problem reads\\nGW(( a,D),(b,D′))2def.= min\\nP∈U(a,b)ED,D′(P)def.=∑\\ni,j,i′,j′|Di,i′−D′\\nj,j′|2Pi,jPi′,j′. (1.83)\\nThis is a non-convex problem, which can be recast as a Quadratic Assignment Problem (QAP) [ ?] and is in\\nfull generality NP-hard to solve for arbitrary inputs. It is in fact equivalent to a graph matching problem [ ?]\\nfor a particular cost. One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a distance between\\nmetric spaces equipped with a probability distribution (here assumed to be discrete in deﬁnition (1.83))\\nup to isometries preserving the measures. This distance was introduced and studied in details by Memoli\\nin [?]. An in-depth mathematical exposition (in particular, its geodesic structure and gradient ﬂows) is given\\nin [?]. See also [ ?] for applications in computer vision. This distance is also tightly connected with the\\nGromov-Hausdorﬀ distance [ ?] between metric spaces, which have been used for shape matching [ ?,?]. Remark 19.Gromov-Wasserstein distance The general setting corresponds to computing couplings between\\nmetric measure spaces ( X,dX,αX) and (Y,dY,αY) where (dX,dY) are distances and ( αX,αY) are measures\\non their respective spaces. One deﬁnes\\nGW((αX,dX),(αY,dY))2def.= min\\nπ∈U(αX,αY)∫\\nX2×Y2|dX(x,x′)−dY(y,y′)|2dπ(x,y)dπ(x′,y′). (1.84)\\nGW deﬁnes a distance between metric measure spaces up to isometries, where one says that ( αX,dX) and\\n(αY,dY) are isometric if there exists ϕ:X→Y such thatϕ♯αX=αYanddY(ϕ(x),ϕ(x′)) =dX(x,x′). Remark 20.Gromov-Wasserstein geodesics The space of metric spaces (up to isometries) endowed with\\nthisGW distance (1.84) has a geodesic structure. [ ?] shows that the geodesic between ( X0,dX0,α0) and\\n(X1,dX1,α1) can be chosen to be t∈[0,1]↦→(X0×X 1,dt,π⋆) whereπ⋆is a solution of (1.84) and for all\\n((x0,x1),(x′\\n0,x′\\n1))∈(X0×X 1)2,\\ndt((x0,x1),(x′\\n0,x′\\n1))def.= (1−t)dX0(x0,x′\\n0) +tdX1(x1,x′\\n1). This formula allows one to deﬁne and analyze gradient ﬂows which minimize functionals involving metric\\nspaces, see [ ?]. It is however diﬃcult to handle numerically, because it involves computations over the product\\nspaceX0×X 1. A heuristic approach is used in [ ?] to deﬁne geodesics and barycenters of metric measure\\nspaces while imposing the cardinality of the involved spaces and making use of the entropic smoothing (1.85)\\ndetailed below. To approximate the computation of GW, and to help convergence of minimization schemes to better\\nminima, one can consider the entropic regularized variant\\nmin\\nP∈U(a,b)ED,D′(P)−εH(P). (1.85)\\n29'\n",
            "Chunk 61: page_content='Figure 1.17: Example of fuzzy correspondences computed by solving GW problem (1.85) with Sinkhorn\\niterations (1.86). Extracted from [ ?]. As proposed initially in [ ?,?], and later revisited in [ ?] for applications in graphics, one can use iteratively\\nSinkhorn’s algorithm to progressively compute a stationary point of (1.85). Indeed, successive linearizations\\nof the objective function lead to consider the succession of updates\\nP(ℓ+1) def.= min\\nP∈U(a,b)⟨P,C(ℓ)⟩−εH(P) where (1.86)\\nC(ℓ)def.=∇ED,D′(P(ℓ)) =−D′TP(ℓ)D,\\nwhich can be interpreted as a mirror-descent scheme [ ?].'\n",
            "Chunk 62: page_content='Each update can thus be solved using Sinkhorn\\niterations (1.51) with cost C(ℓ). Figure (1.17) illustrates the use of this entropic Gromov-Wasserstein to\\ncompute soft maps between domains. 30'\n",
            "Chunk 63: page_content='Bibliography\\n[1] Amir Beck. Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MAT-\\nLAB. SIAM, 2014. [2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization\\nand statistical learning via the alternating direction method of multipliers. Foundations and Trends R⃝\\nin Machine Learning , 3(1):1–122, 2011. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004. [4] E. Cand` es and D. Donoho. New tight frames of curvelets and optimal representations of objects with\\npiecewise C2singularities. Commun. on Pure and Appl. Math. , 57(2):219–266, 2004. [5] E. J. Cand` es, L. Demanet, D. L. Donoho, and L. Ying. Fast discrete curvelet transforms. SIAM\\nMultiscale Modeling and Simulation , 5:861–899, 2005. [6] A. Chambolle. An algorithm for total variation minimization and applications. J.'\n",
            "Chunk 64: page_content='Math. Imaging Vis. ,\\n20:89–97, 2004. [7] Antonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An intro-\\nduction to total variation for image analysis. Theoretical foundations and numerical methods for sparse\\nrecovery , 9(263-340):227, 2010. [8] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization for imaging. Acta\\nNumerica , 25:161–319, 2016. [9] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal\\non Scientiﬁc Computing , 20(1):33–61, 1999. [10] Philippe G Ciarlet. Introduction ` a l’analyse num´ erique matricielle et ` a l’optimisation. 1982. [11] P. L. Combettes and V. R.'\n",
            "Chunk 65: page_content='Wajs. Signal recovery by proximal forward-backward splitting. SIAM\\nMultiscale Modeling and Simulation , 4(4), 2005. [12] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\\nwith a sparsity constraint. Commun. on Pure and Appl. Math. , 57:1413–1541, 2004. [13] D. Donoho and I. Johnstone. Ideal spatial adaptation via wavelet shrinkage. Biometrika , 81:425–455,\\nDec 1994. [14] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume\\n375. Springer Science & Business Media, 1996. [15] M.'\n",
            "Chunk 66: page_content='Figueiredo and R.'\n",
            "Chunk 67: page_content='Nowak. An EM Algorithm for Wavelet-Based Image Restoration. IEEE Trans. Image Proc. , 12(8):906–916, 2003. [16] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing , volume 1. Birkh¨ auser Basel, 2013.'\n",
            "Chunk 68: page_content='31'\n",
            "Chunk 69: page_content='[17] Stephane Mallat. A wavelet tour of signal processing: the sparse way . Academic press, 2008. [18] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated varia-\\ntional problems. Commun. on Pure and Appl. Math. , 42:577–685, 1989. [19] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R⃝in Optimization ,\\n1(3):127–239, 2014. [20] Gabriel Peyr´ e. L’alg` ebre discr` ete de la transform´ ee de Fourier . Ellipses, 2004. [21] J. Portilla, V. Strela, M.J. Wainwright, and Simoncelli E.P. Image denoising using scale mixtures of\\nGaussians in the wavelet domain. IEEE Trans. Image Proc. , 12(11):1338–1351, November 2003. [22] L. I. Rudin, S. Osher, and E.'\n",
            "Chunk 70: page_content='Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4):259–268, 1992. [23] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, Frank Lenzen, and L Sirovich. Variational methods in imaging . Springer, 2009.'\n",
            "Chunk 71: page_content='[24] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal ,\\n27(3):379–423, 1948. [25] Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili. Sparse image and signal processing: Wavelets and\\nrelated geometric multiscale analysis . Cambridge university press, 2015.'\n",
            "Chunk 72: page_content='32'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQaI0sSjWia4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqDGyx2TWie_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LofSLudmYBIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YgCyMftb32_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdwXXyzVb36K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArUE-fNXb39a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nKUPNrSwldv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}